# Training Job Template
apiVersion: batch/v1
kind: Job
metadata:
  name: ml-pipeline-training-job
  namespace: ml-pipeline
  labels:
    app: ml-pipeline-framework
    component: training
    job-type: training
spec:
  parallelism: 1
  completions: 1
  backoffLimit: 3
  template:
    metadata:
      labels:
        app: ml-pipeline-framework
        component: training
        job-type: training
    spec:
      serviceAccountName: ml-pipeline-app
      restartPolicy: Never
      containers:
      - name: training
        image: ml-pipeline-framework:latest
        command:
        - python
        - /app/scripts/train.py
        env:
        - name: CONFIG_PATH
          value: "/app/configs/pipeline_config.yaml"
        - name: EXPERIMENT_NAME
          value: "kubernetes_training"
        - name: RUN_NAME
          value: "training-job-$(date +%Y%m%d-%H%M%S)"
        - name: DB_HOST
          value: "postgres-service"
        - name: DB_PORT
          value: "5432"
        - name: DB_NAME
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: postgres_db
        - name: DB_USER
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: postgres_user
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: postgres_password
        - name: MLFLOW_TRACKING_URI
          value: "http://mlflow-service:5000"
        volumeMounts:
        - name: config-volume
          mountPath: /app/configs
          readOnly: true
        - name: scripts-volume
          mountPath: /app/scripts
          readOnly: true
        - name: data-volume
          mountPath: /app/data
        - name: models-volume
          mountPath: /app/models
        - name: artifacts-volume
          mountPath: /app/artifacts
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
          limits:
            memory: "8Gi"
            cpu: "4"
      volumes:
      - name: config-volume
        configMap:
          name: ml-pipeline-config
      - name: scripts-volume
        configMap:
          name: ml-pipeline-scripts
          defaultMode: 0755
      - name: data-volume
        persistentVolumeClaim:
          claimName: ml-pipeline-data-pvc
      - name: models-volume
        persistentVolumeClaim:
          claimName: ml-pipeline-models-pvc
      - name: artifacts-volume
        persistentVolumeClaim:
          claimName: ml-pipeline-artifacts-pvc
      nodeSelector:
        node-type: compute

---
# Prediction Job Template
apiVersion: batch/v1
kind: Job
metadata:
  name: ml-pipeline-prediction-job
  namespace: ml-pipeline
  labels:
    app: ml-pipeline-framework
    component: prediction
    job-type: prediction
spec:
  parallelism: 1
  completions: 1
  backoffLimit: 3
  template:
    metadata:
      labels:
        app: ml-pipeline-framework
        component: prediction
        job-type: prediction
    spec:
      serviceAccountName: ml-pipeline-app
      restartPolicy: Never
      containers:
      - name: prediction
        image: ml-pipeline-framework:latest
        command:
        - python
        - /app/scripts/predict.py
        env:
        - name: MODEL_PATH
          value: "/app/models/latest"
        - name: INPUT_PATH
          value: "/app/data/input.csv"
        - name: OUTPUT_PATH
          value: "/app/data/predictions.csv"
        - name: DB_HOST
          value: "postgres-service"
        - name: DB_PORT
          value: "5432"
        - name: DB_NAME
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: postgres_db
        - name: DB_USER
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: postgres_user
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: postgres_password
        volumeMounts:
        - name: scripts-volume
          mountPath: /app/scripts
          readOnly: true
        - name: data-volume
          mountPath: /app/data
        - name: models-volume
          mountPath: /app/models
          readOnly: true
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
      volumes:
      - name: scripts-volume
        configMap:
          name: ml-pipeline-scripts
          defaultMode: 0755
      - name: data-volume
        persistentVolumeClaim:
          claimName: ml-pipeline-data-pvc
      - name: models-volume
        persistentVolumeClaim:
          claimName: ml-pipeline-models-pvc

---
# Batch Prediction CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: ml-pipeline-batch-predictions
  namespace: ml-pipeline
  labels:
    app: ml-pipeline-framework
    component: batch-prediction
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: ml-pipeline-framework
            component: batch-prediction
        spec:
          serviceAccountName: ml-pipeline-app
          restartPolicy: Never
          containers:
          - name: batch-prediction
            image: ml-pipeline-framework:latest
            command:
            - /bin/bash
            - -c
            - |
              set -e
              echo "Starting batch prediction job..."
              
              # Check if model exists
              if [ ! -d "/app/models/latest" ]; then
                echo "No model found at /app/models/latest"
                exit 1
              fi
              
              # Check if input data exists
              if [ ! -f "/app/data/batch_input.csv" ]; then
                echo "No input data found at /app/data/batch_input.csv"
                exit 1
              fi
              
              # Run prediction
              python /app/scripts/predict.py
              
              echo "Batch prediction completed successfully"
            env:
            - name: MODEL_PATH
              value: "/app/models/latest"
            - name: INPUT_PATH
              value: "/app/data/batch_input.csv"
            - name: OUTPUT_PATH
              value: "/app/data/batch_predictions_$(date +%Y%m%d).csv"
            volumeMounts:
            - name: scripts-volume
              mountPath: /app/scripts
              readOnly: true
            - name: data-volume
              mountPath: /app/data
            - name: models-volume
              mountPath: /app/models
              readOnly: true
            resources:
              requests:
                memory: "2Gi"
                cpu: "1"
              limits:
                memory: "4Gi"
                cpu: "2"
          volumes:
          - name: scripts-volume
            configMap:
              name: ml-pipeline-scripts
              defaultMode: 0755
          - name: data-volume
            persistentVolumeClaim:
              claimName: ml-pipeline-data-pvc
          - name: models-volume
            persistentVolumeClaim:
              claimName: ml-pipeline-models-pvc

---
# Model Retraining CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: ml-pipeline-model-retrain
  namespace: ml-pipeline
  labels:
    app: ml-pipeline-framework
    component: retrain
spec:
  schedule: "0 1 * * 0"  # Weekly on Sunday at 1 AM
  successfulJobsHistoryLimit: 2
  failedJobsHistoryLimit: 1
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: ml-pipeline-framework
            component: retrain
        spec:
          serviceAccountName: ml-pipeline-app
          restartPolicy: Never
          containers:
          - name: retrain
            image: ml-pipeline-framework:latest
            command:
            - /bin/bash
            - -c
            - |
              set -e
              echo "Starting model retraining job..."
              
              # Set environment variables
              export EXPERIMENT_NAME="weekly_retrain"
              export RUN_NAME="retrain_$(date +%Y%m%d_%H%M%S)"
              
              # Run training
              python /app/scripts/train.py
              
              # Backup current model
              if [ -d "/app/models/latest" ]; then
                mv /app/models/latest "/app/models/backup_$(date +%Y%m%d_%H%M%S)"
              fi
              
              # Deploy new model
              cp -r "/app/models/${EXPERIMENT_NAME}_${RUN_NAME}" "/app/models/latest"
              
              echo "Model retraining completed successfully"
            env:
            - name: CONFIG_PATH
              value: "/app/configs/pipeline_config.yaml"
            - name: DB_HOST
              value: "postgres-service"
            - name: DB_PORT
              value: "5432"
            - name: DB_NAME
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: postgres_db
            - name: DB_USER
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: postgres_user
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: postgres_password
            - name: MLFLOW_TRACKING_URI
              value: "http://mlflow-service:5000"
            volumeMounts:
            - name: config-volume
              mountPath: /app/configs
              readOnly: true
            - name: scripts-volume
              mountPath: /app/scripts
              readOnly: true
            - name: data-volume
              mountPath: /app/data
            - name: models-volume
              mountPath: /app/models
            - name: artifacts-volume
              mountPath: /app/artifacts
            resources:
              requests:
                memory: "6Gi"
                cpu: "3"
              limits:
                memory: "12Gi"
                cpu: "6"
          volumes:
          - name: config-volume
            configMap:
              name: ml-pipeline-config
          - name: scripts-volume
            configMap:
              name: ml-pipeline-scripts
              defaultMode: 0755
          - name: data-volume
            persistentVolumeClaim:
              claimName: ml-pipeline-data-pvc
          - name: models-volume
            persistentVolumeClaim:
              claimName: ml-pipeline-models-pvc
          - name: artifacts-volume
            persistentVolumeClaim:
              claimName: ml-pipeline-artifacts-pvc
          nodeSelector:
            node-type: compute

---
# Data Quality Check Job
apiVersion: batch/v1
kind: CronJob
metadata:
  name: ml-pipeline-data-quality
  namespace: ml-pipeline
  labels:
    app: ml-pipeline-framework
    component: data-quality
spec:
  schedule: "0 */6 * * *"  # Every 6 hours
  successfulJobsHistoryLimit: 5
  failedJobsHistoryLimit: 2
  concurrencyPolicy: Allow
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: ml-pipeline-framework
            component: data-quality
        spec:
          serviceAccountName: ml-pipeline-app
          restartPolicy: Never
          containers:
          - name: data-quality
            image: ml-pipeline-framework:latest
            command:
            - python
            - -c
            - |
              import sys
              sys.path.insert(0, '/app')
              
              from ml_pipeline_framework.preprocessing import DataValidator
              from ml_pipeline_framework.data_access import ConnectorFactory
              import os
              import logging
              
              logging.basicConfig(level=logging.INFO)
              logger = logging.getLogger(__name__)
              
              # Initialize data connector
              config = {
                  'type': 'postgres',
                  'connection_string': f"postgresql://{os.environ['DB_USER']}:{os.environ['DB_PASSWORD']}@{os.environ['DB_HOST']}:{os.environ['DB_PORT']}/{os.environ['DB_NAME']}"
              }
              
              connector = ConnectorFactory.create_connector(config)
              
              # Load data
              data = connector.query("SELECT * FROM ml_data.customers LIMIT 10000")
              logger.info(f"Loaded {len(data)} rows for quality check")
              
              # Initialize validator
              validator = DataValidator(
                  expectation_suite='customer_data_quality',
                  fail_on_error=False
              )
              
              # Add expectations
              validator.add_expectation('expect_column_to_exist', column='customer_id')
              validator.add_expectation('expect_column_values_to_not_be_null', column='customer_id')
              validator.add_expectation('expect_column_values_to_be_between', column='age', min_value=18, max_value=120)
              validator.add_expectation('expect_column_values_to_be_between', column='credit_score', min_value=300, max_value=850)
              
              # Run validation
              results = validator.validate(data)
              
              if results['success']:
                  logger.info("Data quality check passed!")
              else:
                  logger.warning("Data quality issues detected")
                  
              # Generate report
              validator.generate_validation_report('/app/data/quality_reports/')
              
              logger.info("Data quality check completed")
            env:
            - name: DB_HOST
              value: "postgres-service"
            - name: DB_PORT
              value: "5432"
            - name: DB_NAME
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: postgres_db
            - name: DB_USER
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: postgres_user
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: postgres_password
            volumeMounts:
            - name: data-volume
              mountPath: /app/data
            resources:
              requests:
                memory: "1Gi"
                cpu: "500m"
              limits:
                memory: "2Gi"
                cpu: "1"
          volumes:
          - name: data-volume
            persistentVolumeClaim:
              claimName: ml-pipeline-data-pvc