# Production Docker Compose Configuration for ML Pipeline Framework
# This configuration provides a complete production-ready deployment with
# monitoring, logging, security, and high availability features.

version: '3.8'

services:
  # ML Pipeline Application
  ml-pipeline-app:
    image: ml-pipeline-framework:${VERSION:-latest}
    container_name: ml-pipeline-app
    restart: unless-stopped
    depends_on:
      - postgres
      - redis
      - mlflow
    environment:
      # Database Configuration
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_NAME=${DB_NAME:-ml_pipeline}
      - DB_USERNAME=${DB_USERNAME:-ml_user}
      - DB_PASSWORD=${DB_PASSWORD}
      
      # Redis Configuration
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      
      # MLflow Configuration
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MLFLOW_S3_ENDPOINT_URL=${S3_ENDPOINT_URL}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      
      # Application Configuration
      - ENVIRONMENT=production
      - LOG_LEVEL=INFO
      - WORKERS=4
      - TIMEOUT=300
      
      # Security Configuration
      - ENCRYPTION_KEY=${ENCRYPTION_KEY}
      - JWT_SECRET=${JWT_SECRET}
      - SSL_ENABLED=true
      
      # Monitoring Configuration
      - PROMETHEUS_ENABLED=true
      - METRICS_PORT=8080
      - HEALTH_CHECK_PORT=8090
    ports:
      - "8000:8000"    # API port
      - "8080:8080"    # Metrics port
      - "8090:8090"    # Health check port
    volumes:
      - ml-data:/app/data
      - ml-models:/app/models
      - ml-artifacts:/app/artifacts
      - ml-logs:/app/logs
      - ./configs:/app/configs:ro
      - ./certs:/app/certs:ro
    networks:
      - ml-pipeline-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8090/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4.0'
        reservations:
          memory: 4G
          cpus: '2.0'
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.ml-pipeline.rule=Host(`ml-pipeline.${DOMAIN}`)"
      - "traefik.http.routers.ml-pipeline.tls=true"
      - "traefik.http.routers.ml-pipeline.tls.certresolver=letsencrypt"

  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: ml-pipeline-postgres
    restart: unless-stopped
    environment:
      - POSTGRES_DB=${DB_NAME:-ml_pipeline}
      - POSTGRES_USER=${DB_USERNAME:-ml_user}
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - POSTGRES_INITDB_ARGS=--auth-host=scram-sha-256
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d:ro
      - ./postgres-conf:/etc/postgresql:ro
    networks:
      - ml-pipeline-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USERNAME:-ml_user} -d ${DB_NAME:-ml_pipeline}"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: ml-pipeline-redis
    restart: unless-stopped
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    command: redis-server --requirepass ${REDIS_PASSWORD} --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
      - ./redis.conf:/usr/local/etc/redis/redis.conf:ro
    networks:
      - ml-pipeline-network
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'

  # MLflow Tracking Server
  mlflow:
    image: mlflow/mlflow:2.8.1
    container_name: ml-pipeline-mlflow
    restart: unless-stopped
    depends_on:
      - postgres
    environment:
      - MLFLOW_BACKEND_STORE_URI=postgresql://${DB_USERNAME:-ml_user}:${DB_PASSWORD}@postgres:5432/${DB_NAME:-ml_pipeline}_mlflow
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=s3://${S3_BUCKET}/mlflow-artifacts
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - MLFLOW_S3_ENDPOINT_URL=${S3_ENDPOINT_URL}
    ports:
      - "5000:5000"
    command: >
      mlflow server
      --backend-store-uri postgresql://${DB_USERNAME:-ml_user}:${DB_PASSWORD}@postgres:5432/${DB_NAME:-ml_pipeline}_mlflow
      --default-artifact-root s3://${S3_BUCKET}/mlflow-artifacts
      --host 0.0.0.0
      --port 5000
      --workers 2
    networks:
      - ml-pipeline-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'

  # Prometheus Monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: ml-pipeline-prometheus
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    ports:
      - "9090:9090"
    volumes:
      - prometheus-data:/prometheus
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/alert-rules.yml:/etc/prometheus/alert-rules.yml:ro
    networks:
      - ml-pipeline-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'

  # Grafana Dashboard
  grafana:
    image: grafana/grafana:latest
    container_name: ml-pipeline-grafana
    restart: unless-stopped
    depends_on:
      - prometheus
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-worldmap-panel
      - GF_SERVER_ROOT_URL=https://grafana.${DOMAIN}
      - GF_SECURITY_COOKIE_SECURE=true
      - GF_SECURITY_COOKIE_SAMESITE=strict
    ports:
      - "3000:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    networks:
      - ml-pipeline-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'

  # Alertmanager for Alerts
  alertmanager:
    image: prom/alertmanager:latest
    container_name: ml-pipeline-alertmanager
    restart: unless-stopped
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://alertmanager.${DOMAIN}'
    ports:
      - "9093:9093"
    volumes:
      - alertmanager-data:/alertmanager
      - ./monitoring/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
    networks:
      - ml-pipeline-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.25'
        reservations:
          memory: 256M
          cpus: '0.125'

  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    container_name: ml-pipeline-nginx
    restart: unless-stopped
    depends_on:
      - ml-pipeline-app
      - grafana
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./certs:/etc/nginx/certs:ro
      - nginx-logs:/var/log/nginx
    networks:
      - ml-pipeline-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  # Log aggregator (ELK Stack alternative - Loki)
  loki:
    image: grafana/loki:latest
    container_name: ml-pipeline-loki
    restart: unless-stopped
    command: -config.file=/etc/loki/local-config.yaml
    ports:
      - "3100:3100"
    volumes:
      - loki-data:/loki
      - ./monitoring/loki-config.yml:/etc/loki/local-config.yaml:ro
    networks:
      - ml-pipeline-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'

  # Log collector
  promtail:
    image: grafana/promtail:latest
    container_name: ml-pipeline-promtail
    restart: unless-stopped
    depends_on:
      - loki
    volumes:
      - ml-logs:/var/log/ml-pipeline:ro
      - nginx-logs:/var/log/nginx:ro
      - ./monitoring/promtail-config.yml:/etc/promtail/config.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/config.yml
    networks:
      - ml-pipeline-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.25'
        reservations:
          memory: 256M
          cpus: '0.125'

  # Node Exporter for System Metrics
  node-exporter:
    image: prom/node-exporter:latest
    container_name: ml-pipeline-node-exporter
    restart: unless-stopped
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    networks:
      - ml-pipeline-network
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.125'

# Network Configuration
networks:
  ml-pipeline-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

# Volume Configuration
volumes:
  # Application Data
  ml-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}
  
  ml-models:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${MODELS_PATH:-./models}
  
  ml-artifacts:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${ARTIFACTS_PATH:-./artifacts}
  
  ml-logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${LOGS_PATH:-./logs}
  
  # Database Data
  postgres-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${POSTGRES_DATA_PATH:-./postgres-data}
  
  redis-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${REDIS_DATA_PATH:-./redis-data}
  
  # Monitoring Data
  prometheus-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PROMETHEUS_DATA_PATH:-./prometheus-data}
  
  grafana-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${GRAFANA_DATA_PATH:-./grafana-data}
  
  alertmanager-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${ALERTMANAGER_DATA_PATH:-./alertmanager-data}
  
  loki-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${LOKI_DATA_PATH:-./loki-data}
  
  nginx-logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${NGINX_LOGS_PATH:-./nginx-logs}