# Spark Configuration for ML Pipeline Framework
# This file contains default Spark configuration optimized for ML workloads

# Application Properties
spark.app.name=MLPipelineFramework
spark.master=local[*]

# Driver Configuration
spark.driver.memory=2g
spark.driver.maxResultSize=1g
spark.driver.cores=2

# Executor Configuration  
spark.executor.memory=2g
spark.executor.cores=2
spark.executor.instances=2

# Memory Management
spark.sql.execution.arrow.pyspark.enabled=true
spark.sql.execution.arrow.maxRecordsPerBatch=10000
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.advisoryPartitionSizeInBytes=64MB

# Performance Optimizations
spark.sql.adaptive.skewJoin.enabled=true
spark.sql.adaptive.localShuffleReader.enabled=true
spark.sql.adaptive.coalescePartitions.parallelismFirst=false
spark.sql.adaptive.coalescePartitions.minPartitionNum=1

# Shuffle Configuration
spark.sql.shuffle.partitions=200
spark.shuffle.compress=true
spark.shuffle.spill.compress=true
spark.io.compression.codec=snappy

# Checkpointing
spark.sql.streaming.checkpointLocation=/tmp/spark-checkpoints

# Dynamic Allocation (disabled by default for containers)
spark.dynamicAllocation.enabled=false
spark.dynamicAllocation.minExecutors=1
spark.dynamicAllocation.maxExecutors=4
spark.dynamicAllocation.initialExecutors=2

# Network Configuration
spark.network.timeout=600s
spark.executor.heartbeatInterval=60s
spark.sql.broadcastTimeout=300

# History Server
spark.history.ui.port=18080
spark.history.fs.logDirectory=/tmp/spark-events

# Event Log
spark.eventLog.enabled=true
spark.eventLog.dir=/tmp/spark-events
spark.eventLog.compress=true

# UI Configuration
spark.ui.enabled=true
spark.ui.port=4040
spark.ui.retainedJobs=1000
spark.ui.retainedStages=1000

# ML Specific Optimizations
spark.sql.execution.arrow.pyspark.enabled=true
spark.sql.execution.arrow.pyspark.fallback.enabled=true

# Delta Lake Configuration (if using Delta)
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog

# Kubernetes specific settings (commented out by default)
# spark.kubernetes.container.image=ml-pipeline-framework:latest
# spark.kubernetes.authenticate.driver.serviceAccountName=spark
# spark.kubernetes.executor.deleteOnTermination=true

# Resource allocation for different workload types
# Uncomment appropriate section based on workload

# For CPU-intensive ML workloads
# spark.task.cpus=1
# spark.executor.cores=4
# spark.executor.memory=4g

# For memory-intensive data processing
# spark.executor.memory=8g
# spark.executor.memoryFraction=0.8
# spark.storage.memoryFraction=0.3

# For I/O intensive workloads
# spark.sql.adaptive.coalescePartitions.enabled=true
# spark.sql.files.maxPartitionBytes=134217728
# spark.sql.files.openCostInBytes=4194304