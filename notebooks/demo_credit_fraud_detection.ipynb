{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Credit Card Fraud Detection - Complete ML Pipeline Demo\n\nThis notebook demonstrates the complete ML Pipeline Framework capabilities using a credit card fraud detection use case with CSV data processing only.\n\n## Features Demonstrated:\n- üìä **Data Loading & Schema Analysis** - CSV-based data processing with efficient libraries\n- üîç **Comprehensive EDA** - Fraud-specific visualizations and pattern analysis\n- üõ†Ô∏è **Advanced Feature Engineering** - Time-based, frequency, amount-based, and merchant risk features\n- ü§ñ **AutoML Pipeline** - Automated model selection with all supported algorithms\n- üìà **Model Interpretability** - SHAP, LIME, ALE, Anchors, and Counterfactuals\n- üìã **Admissible ML** - Model cards, fairness analysis, and regulatory compliance\n- üí∞ **Business Impact Analysis** - Cost-benefit analysis and optimal threshold selection\n- üöÄ **Production Readiness** - Monitoring setup, A/B testing, and deployment preparation\n\n## Key Requirements:\n- ‚úÖ CSV data only (no PySpark)\n- ‚úÖ Single-machine processing with efficient libraries\n- ‚úÖ 0.17% fraud rate simulation\n- ‚úÖ Comprehensive AutoML execution\n- ‚úÖ Full interpretability suite\n- ‚úÖ Business metrics focus"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Environment Setup and Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Essential imports for fraud detection pipeline\nimport os\nimport sys\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, Markdown, HTML\nimport yaml\nimport json\nfrom pathlib import Path\n\n# Configure environment\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8-darkgrid')\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\n\n# Add project root to path\nsys.path.insert(0, os.path.abspath('..'))\n\n# ML Pipeline Framework imports (using existing modules)\nfrom src.utils.config_parser import ConfigParser\nfrom src.utils.logging_config import setup_logging\n\n# Core ML libraries\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n    confusion_matrix, classification_report, roc_curve, precision_recall_curve\n)\n\n# Advanced ML libraries\ntry:\n    import xgboost as xgb\n    XGBOOST_AVAILABLE = True\nexcept ImportError:\n    XGBOOST_AVAILABLE = False\n\ntry:\n    import lightgbm as lgb\n    LIGHTGBM_AVAILABLE = True\nexcept ImportError:\n    LIGHTGBM_AVAILABLE = False\n\ntry:\n    import catboost as cb\n    CATBOOST_AVAILABLE = True\nexcept ImportError:\n    CATBOOST_AVAILABLE = False\n\n# Interpretability libraries\ntry:\n    import shap\n    SHAP_AVAILABLE = True\nexcept ImportError:\n    SHAP_AVAILABLE = False\n\ntry:\n    from lime.lime_tabular import LimeTabularExplainer\n    LIME_AVAILABLE = True\nexcept ImportError:\n    LIME_AVAILABLE = False\n\n# Imbalanced learning\ntry:\n    from imblearn.over_sampling import SMOTE, ADASYN\n    from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours\n    from imblearn.combine import SMOTETomek\n    IMBLEARN_AVAILABLE = True\nexcept ImportError:\n    IMBLEARN_AVAILABLE = False\n\n# Hyperparameter optimization\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\ntry:\n    import optuna\n    OPTUNA_AVAILABLE = True\nexcept ImportError:\n    OPTUNA_AVAILABLE = False\n\n# Business metrics and costs\nfrom sklearn.metrics import make_scorer\n\n# Setup logging\nlogger = setup_logging(component='fraud_detection_demo')\n\nprint(\"‚úÖ Environment setup complete!\")\nprint(f\"üìä Data processing libraries: pandas {pd.__version__}, numpy {np.__version__}\")\nprint(f\"ü§ñ ML libraries: scikit-learn\")\nif XGBOOST_AVAILABLE:\n    print(f\"   XGBoost: {xgb.__version__}\")\nif LIGHTGBM_AVAILABLE:\n    print(f\"   LightGBM: {lgb.__version__}\")\nif CATBOOST_AVAILABLE:\n    print(f\"   CatBoost: {cb.__version__}\")\nprint(f\"üîç Interpretability: {'SHAP, ' if SHAP_AVAILABLE else ''}{'LIME' if LIME_AVAILABLE else ''}\")\nprint(f\"‚öñÔ∏è Imbalanced learning: {'‚úÖ' if IMBLEARN_AVAILABLE else '‚ùå'}\")\nprint(f\"üéØ Hyperparameter optimization: {'optuna, ' if OPTUNA_AVAILABLE else ''}scikit-learn\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Data Loading - CSV Files Only"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate realistic credit card fraud dataset with 0.17% fraud rate\nnp.random.seed(42)\n\ndef generate_realistic_fraud_dataset(n_samples=100000, fraud_rate=0.0017):\n    \"\"\"Generate realistic credit card fraud dataset matching industry patterns.\"\"\"\n    \n    n_frauds = int(n_samples * fraud_rate)\n    n_normal = n_samples - n_frauds\n    \n    print(f\"Generating {n_samples:,} transactions ({n_frauds:,} fraudulent, {n_normal:,} normal)\")\n    \n    # Base customer profiles\n    customer_ids = np.arange(1, 25001)  # 25,000 unique customers\n    merchant_ids = np.arange(1, 5001)   # 5,000 unique merchants\n    \n    # Normal transactions - realistic patterns\n    normal_data = {\n        'customer_id': np.random.choice(customer_ids, n_normal),\n        'merchant_id': np.random.choice(merchant_ids, n_normal),\n        'transaction_amount': np.random.lognormal(mean=3.2, sigma=1.1, size=n_normal),  # Median ~$25\n        'transaction_hour': np.random.choice(24, n_normal, p=[\n            0.01, 0.01, 0.01, 0.01, 0.01, 0.02,  # Midnight - 5am (low activity)\n            0.03, 0.05, 0.07, 0.08, 0.08, 0.09,  # 6am - 11am (morning)\n            0.10, 0.11, 0.10, 0.09, 0.08, 0.07,  # Noon - 5pm (afternoon peak)\n            0.06, 0.05, 0.04, 0.03, 0.02, 0.01   # 6pm - 11pm (evening decline)\n        ]),\n        'day_of_week': np.random.choice(7, n_normal, p=[0.15, 0.14, 0.14, 0.14, 0.14, 0.15, 0.14]),\n        'merchant_category': np.random.choice([\n            'grocery', 'gas_station', 'restaurant', 'retail', 'online', 'pharmacy', 'entertainment'\n        ], n_normal, p=[0.25, 0.15, 0.20, 0.15, 0.10, 0.08, 0.07]),\n        'customer_age': np.random.normal(42, 15, n_normal).clip(18, 85),\n        'account_age_months': np.random.exponential(24, n_normal).clip(1, 300),\n        'credit_limit': np.random.lognormal(9.2, 0.7, n_normal),  # Median ~$10K\n        'previous_transactions_today': np.random.poisson(2.5, n_normal),\n        'days_since_last_transaction': np.random.exponential(3.5, n_normal),\n        'weekend_flag': np.random.choice([0, 1], n_normal, p=[0.71, 0.29]),  # ~29% weekend\n        'location_risk_score': np.random.beta(2, 8, n_normal),  # Low risk locations\n        'merchant_risk_score': np.random.beta(2, 6, n_normal),  # Mostly low-risk merchants\n        'is_fraud': np.zeros(n_normal, dtype=int)\n    }\n    \n    # Fraudulent transactions - different patterns\n    fraud_data = {\n        'customer_id': np.random.choice(customer_ids, n_frauds),\n        'merchant_id': np.random.choice(merchant_ids, n_frauds),\n        'transaction_amount': np.random.lognormal(mean=4.8, sigma=1.4, size=n_frauds),  # Higher amounts\n        'transaction_hour': np.random.choice(24, n_frauds, p=[\n            0.08, 0.07, 0.06, 0.05, 0.04, 0.03,  # Late night/early morning spike\n            0.02, 0.02, 0.03, 0.04, 0.05, 0.06,  # Morning\n            0.06, 0.07, 0.06, 0.05, 0.04, 0.04,  # Afternoon\n            0.04, 0.05, 0.06, 0.08, 0.09, 0.09   # Evening/night spike\n        ]),\n        'day_of_week': np.random.choice(7, n_frauds, p=[0.12, 0.13, 0.14, 0.15, 0.16, 0.16, 0.14]),\n        'merchant_category': np.random.choice([\n            'online', 'entertainment', 'retail', 'gas_station', 'restaurant', 'grocery', 'pharmacy'\n        ], n_frauds, p=[0.35, 0.20, 0.18, 0.12, 0.08, 0.05, 0.02]),  # Higher online fraud\n        'customer_age': np.random.normal(38, 12, n_frauds).clip(18, 85),  # Slightly younger\n        'account_age_months': np.random.exponential(18, n_frauds).clip(1, 300),  # Newer accounts\n        'credit_limit': np.random.lognormal(9.0, 0.8, n_frauds),  # Similar limits\n        'previous_transactions_today': np.random.poisson(6.5, n_frauds),  # More activity\n        'days_since_last_transaction': np.random.exponential(1.2, n_frauds),  # More frequent\n        'weekend_flag': np.random.choice([0, 1], n_frauds, p=[0.65, 0.35]),  # Slightly more weekend\n        'location_risk_score': np.random.beta(5, 5, n_frauds),  # Higher risk locations\n        'merchant_risk_score': np.random.beta(4, 3, n_frauds),  # Higher risk merchants\n        'is_fraud': np.ones(n_frauds, dtype=int)\n    }\n    \n    # Combine datasets\n    combined_data = {}\n    for key in normal_data.keys():\n        combined_data[key] = np.concatenate([normal_data[key], fraud_data[key]])\n    \n    # Create DataFrame\n    df = pd.DataFrame(combined_data)\n    \n    # Add transaction timestamp (sorted chronologically)\n    start_date = datetime(2024, 1, 1)\n    end_date = datetime(2024, 12, 31)\n    date_range = pd.date_range(start_date, end_date, freq='3min')[:n_samples]\n    df['transaction_datetime'] = np.random.choice(date_range, n_samples, replace=False)\n    df = df.sort_values('transaction_datetime').reset_index(drop=True)\n    \n    # Add transaction IDs\n    df['transaction_id'] = [f'TXN_{i:08d}' for i in range(1, len(df) + 1)]\n    \n    # Calculate derived features\n    df['amount_to_limit_ratio'] = df['transaction_amount'] / df['credit_limit']\n    df['high_amount_flag'] = (df['transaction_amount'] > df['transaction_amount'].quantile(0.95)).astype(int)\n    df['velocity_score'] = df['previous_transactions_today'] * df['amount_to_limit_ratio']\n    df['composite_risk_score'] = df['location_risk_score'] * df['merchant_risk_score']\n    df['unusual_time_flag'] = ((df['transaction_hour'] <= 5) | (df['transaction_hour'] >= 23)).astype(int)\n    \n    # Shuffle final dataset\n    df = df.sample(frac=1).reset_index(drop=True)\n    \n    return df\n\n# Generate the dataset\nprint(\"üîÑ Generating realistic credit card fraud dataset...\")\nfraud_df = generate_realistic_fraud_dataset(n_samples=100000, fraud_rate=0.0017)\n\n# Create data directory and save\ndata_dir = Path('../data')\ndata_dir.mkdir(exist_ok=True)\ncsv_path = data_dir / 'credit_card_fraud_data.csv'\n\n# Save with efficient data types\nfraud_df.to_csv(csv_path, index=False)\n\nprint(f\"‚úÖ Dataset generated and saved to {csv_path}\")\nprint(f\"üìä Dataset shape: {fraud_df.shape}\")\nprint(f\"üéØ Actual fraud rate: {fraud_df['is_fraud'].mean():.4f} ({fraud_df['is_fraud'].sum():,} fraudulent transactions)\")\nprint(f\"üíæ File size: {csv_path.stat().st_size / 1024 / 1024:.1f} MB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Load and examine the data schema\nprint(\"üìä Loading CSV data and analyzing schema...\")\n\n# Load with pandas (efficient for this size)\ndf = pd.read_csv(csv_path, parse_dates=['transaction_datetime'])\n\nprint(f\"‚úÖ Data loaded successfully\")\nprint(f\"üìè Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\nprint(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\nprint(f\"üïí Time range: {df['transaction_datetime'].min()} to {df['transaction_datetime'].max()}\")\n\n# Display schema information\nprint(\"\\nüìã Data Schema:\")\nschema_info = pd.DataFrame({\n    'Column': df.columns,\n    'Data Type': df.dtypes,\n    'Non-Null Count': df.count(),\n    'Null Count': df.isnull().sum(),\n    'Unique Values': df.nunique(),\n    'Memory Usage (KB)': df.memory_usage(deep=True) / 1024\n})\ndisplay(schema_info)\n\n# Basic statistics\nprint(\"\\nüìà Basic Dataset Statistics:\")\nprint(f\"‚Ä¢ Total transactions: {len(df):,}\")\nprint(f\"‚Ä¢ Fraudulent transactions: {df['is_fraud'].sum():,} ({df['is_fraud'].mean():.4%})\")\nprint(f\"‚Ä¢ Normal transactions: {(df['is_fraud'] == 0).sum():,}\")\nprint(f\"‚Ä¢ Unique customers: {df['customer_id'].nunique():,}\")\nprint(f\"‚Ä¢ Unique merchants: {df['merchant_id'].nunique():,}\")\nprint(f\"‚Ä¢ Date range: {(df['transaction_datetime'].max() - df['transaction_datetime'].min()).days} days\")\n\n# Class imbalance verification\nfraud_counts = df['is_fraud'].value_counts()\nprint(f\"\\n‚öñÔ∏è Class Distribution:\")\nprint(f\"‚Ä¢ Normal (0): {fraud_counts[0]:,} ({fraud_counts[0]/len(df):.4%})\")\nprint(f\"‚Ä¢ Fraud (1): {fraud_counts[1]:,} ({fraud_counts[1]/len(df):.4%})\")\nprint(f\"‚Ä¢ Imbalance ratio: 1:{fraud_counts[0]//fraud_counts[1]:,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 3. Exploratory Data Analysis with Fraud Focus"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Comprehensive fraud detection EDA\nprint(\"üîç Conducting Fraud-Focused Exploratory Data Analysis...\")\n\n# Create visualizations using our fraud visualization framework\nfraud_viz = AnimatedFraudPatternVisualizer(df)\ninteractive_3d = Interactive3DFeatureSpace(df)\nbusiness_dashboard = BusinessMetricsDashboard(df)\n\n# Basic fraud analysis\nfig, axes = plt.subplots(3, 3, figsize=(20, 15))\nfig.suptitle('Credit Card Fraud Detection - Comprehensive EDA', fontsize=16, y=0.98)\n\n# 1. Transaction amount distribution by fraud status\nax1 = axes[0, 0]\nnormal_amounts = df[df['is_fraud'] == 0]['transaction_amount']\nfraud_amounts = df[df['is_fraud'] == 1]['transaction_amount']\nax1.hist([normal_amounts, fraud_amounts], bins=50, alpha=0.7, \n         label=['Normal', 'Fraud'], color=['blue', 'red'], density=True)\nax1.set_xlabel('Transaction Amount ($)')\nax1.set_ylabel('Density')\nax1.set_title('Transaction Amount Distribution')\nax1.legend()\nax1.set_xlim(0, 1000)  # Focus on main range\n\n# 2. Fraud rate by hour of day\nax2 = axes[0, 1]\nfraud_by_hour = df.groupby('transaction_hour').agg({\n    'is_fraud': ['sum', 'count']\n}).round(4)\nfraud_by_hour.columns = ['fraud_count', 'total_count']\nfraud_by_hour['fraud_rate'] = fraud_by_hour['fraud_count'] / fraud_by_hour['total_count']\nax2.bar(fraud_by_hour.index, fraud_by_hour['fraud_rate'] * 100, color='crimson', alpha=0.7)\nax2.set_xlabel('Hour of Day')\nax2.set_ylabel('Fraud Rate (%)')\nax2.set_title('Fraud Rate by Hour of Day')\nax2.set_xticks(range(0, 24, 4))\n\n# 3. Fraud rate by merchant category\nax3 = axes[0, 2]\nfraud_by_category = df.groupby('merchant_category').agg({\n    'is_fraud': ['sum', 'count']\n}).round(4)\nfraud_by_category.columns = ['fraud_count', 'total_count']\nfraud_by_category['fraud_rate'] = fraud_by_category['fraud_count'] / fraud_by_category['total_count']\nfraud_by_category = fraud_by_category.sort_values('fraud_rate', ascending=True)\nax3.barh(range(len(fraud_by_category)), fraud_by_category['fraud_rate'] * 100, color='orange')\nax3.set_yticks(range(len(fraud_by_category)))\nax3.set_yticklabels(fraud_by_category.index)\nax3.set_xlabel('Fraud Rate (%)')\nax3.set_title('Fraud Rate by Merchant Category')\n\n# 4. Customer age distribution\nax4 = axes[1, 0]\nax4.hist([df[df['is_fraud'] == 0]['customer_age'],\n          df[df['is_fraud'] == 1]['customer_age']], \n         bins=30, alpha=0.7, label=['Normal', 'Fraud'], color=['blue', 'red'], density=True)\nax4.set_xlabel('Customer Age')\nax4.set_ylabel('Density')\nax4.set_title('Customer Age Distribution')\nax4.legend()\n\n# 5. Transaction velocity analysis\nax5 = axes[1, 1]\nax5.hist([df[df['is_fraud'] == 0]['previous_transactions_today'],\n          df[df['is_fraud'] == 1]['previous_transactions_today']], \n         bins=20, alpha=0.7, label=['Normal', 'Fraud'], color=['blue', 'red'], density=True)\nax5.set_xlabel('Previous Transactions Today')\nax5.set_ylabel('Density')\nax5.set_title('Daily Transaction Velocity')\nax5.legend()\n\n# 6. Risk score distribution\nax6 = axes[1, 2]\nax6.hist([df[df['is_fraud'] == 0]['composite_risk_score'],\n          df[df['is_fraud'] == 1]['composite_risk_score']], \n         bins=30, alpha=0.7, label=['Normal', 'Fraud'], color=['blue', 'red'], density=True)\nax6.set_xlabel('Composite Risk Score')\nax6.set_ylabel('Density')\nax6.set_title('Risk Score Distribution')\nax6.legend()\n\n# 7. Weekend vs weekday fraud\nax7 = axes[2, 0]\nweekend_fraud = df.groupby('weekend_flag').agg({\n    'is_fraud': ['sum', 'count']\n}).round(4)\nweekend_fraud.columns = ['fraud_count', 'total_count']\nweekend_fraud['fraud_rate'] = weekend_fraud['fraud_count'] / weekend_fraud['total_count']\nlabels = ['Weekday', 'Weekend']\nax7.bar(labels, weekend_fraud['fraud_rate'] * 100, color=['steelblue', 'darkorange'])\nax7.set_ylabel('Fraud Rate (%)')\nax7.set_title('Weekend vs Weekday Fraud Rate')\n\n# 8. Account age vs fraud\nax8 = axes[2, 1]\naccount_age_bins = pd.cut(df['account_age_months'], bins=10)\nfraud_by_age = df.groupby(account_age_bins).agg({\n    'is_fraud': ['sum', 'count']\n}).round(4)\nfraud_by_age.columns = ['fraud_count', 'total_count']\nfraud_by_age['fraud_rate'] = fraud_by_age['fraud_count'] / fraud_by_age['total_count']\nax8.plot(range(len(fraud_by_age)), fraud_by_age['fraud_rate'] * 100, \n         marker='o', color='green', linewidth=2)\nax8.set_xlabel('Account Age Bins')\nax8.set_ylabel('Fraud Rate (%)')\nax8.set_title('Fraud Rate by Account Age')\nax8.set_xticks(range(0, len(fraud_by_age), 2))\n\n# 9. Amount to limit ratio\nax9 = axes[2, 2]\nax9.hist([df[df['is_fraud'] == 0]['amount_to_limit_ratio'],\n          df[df['is_fraud'] == 1]['amount_to_limit_ratio']], \n         bins=30, alpha=0.7, label=['Normal', 'Fraud'], color=['blue', 'red'], density=True)\nax9.set_xlabel('Amount to Credit Limit Ratio')\nax9.set_ylabel('Density')\nax9.set_title('Amount/Limit Ratio Distribution')\nax9.legend()\nax9.set_xlim(0, 0.5)  # Focus on main range\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics by fraud status\nprint(\"\\nüìä Statistical Summary by Fraud Status:\")\nnumeric_cols = df.select_dtypes(include=[np.number]).columns\nsummary_stats = df.groupby('is_fraud')[numeric_cols].agg(['mean', 'median', 'std']).round(4)\n\n# Flatten column names\nsummary_stats.columns = [f'{col}_{stat}' for col, stat in summary_stats.columns]\n\n# Calculate differences\nnormal_stats = summary_stats.loc[0]\nfraud_stats = summary_stats.loc[1]\n\n# Show key differences\nkey_features = ['transaction_amount', 'transaction_hour', 'customer_age', \n                'previous_transactions_today', 'composite_risk_score', 'amount_to_limit_ratio']\n\ncomparison_df = pd.DataFrame()\nfor feature in key_features:\n    if f'{feature}_mean' in normal_stats.index:\n        comparison_df[feature] = {\n            'Normal_Mean': normal_stats[f'{feature}_mean'],\n            'Fraud_Mean': fraud_stats[f'{feature}_mean'],\n            'Difference': fraud_stats[f'{feature}_mean'] - normal_stats[f'{feature}_mean'],\n            'Ratio': fraud_stats[f'{feature}_mean'] / normal_stats[f'{feature}_mean'] if normal_stats[f'{feature}_mean'] != 0 else 0\n        }\n\ndisplay(comparison_df.T.round(4))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 4. Feature Engineering for Fraud Detection"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Advanced feature engineering for fraud detection\nprint(\"üõ†Ô∏è Engineering Features for Fraud Detection...\")\n\n# Create a copy for feature engineering\ndf_features = df.copy()\n\n# 1. Time-based features\nprint(\"\\n‚è∞ Creating time-based features...\")\ndf_features['hour_sin'] = np.sin(2 * np.pi * df_features['transaction_hour'] / 24)\ndf_features['hour_cos'] = np.cos(2 * np.pi * df_features['transaction_hour'] / 24)\ndf_features['day_sin'] = np.sin(2 * np.pi * df_features['day_of_week'] / 7)\ndf_features['day_cos'] = np.cos(2 * np.pi * df_features['day_of_week'] / 7)\n\n# Extract additional datetime features\ndf_features['month'] = df_features['transaction_datetime'].dt.month\ndf_features['quarter'] = df_features['transaction_datetime'].dt.quarter\ndf_features['day_of_month'] = df_features['transaction_datetime'].dt.day\ndf_features['is_month_end'] = (df_features['transaction_datetime'].dt.day > 25).astype(int)\ndf_features['is_quarter_end'] = (df_features['month'].isin([3, 6, 9, 12]) & \n                                (df_features['day_of_month'] > 25)).astype(int)\n\n# 2. Customer frequency features\nprint(\"üìä Creating customer frequency features...\")\n# Customer transaction statistics (using expanding window for realistic simulation)\ncustomer_stats = df_features.groupby('customer_id').expanding().agg({\n    'transaction_amount': ['count', 'mean', 'std', 'sum'],\n    'is_fraud': 'sum'\n}).shift(1)  # Shift to avoid data leakage\n\n# Flatten column names\ncustomer_stats.columns = ['_'.join(col).strip() for col in customer_stats.columns]\ncustomer_stats = customer_stats.reset_index()\n\n# Merge back with main dataset\ndf_features = df_features.reset_index().merge(\n    customer_stats, \n    on=['customer_id', 'level_1'], \n    how='left'\n).drop('level_1', axis=1).set_index('index')\n\n# Fill initial NaN values for new customers\nnumeric_cols = df_features.select_dtypes(include=[np.number]).columns\ndf_features[numeric_cols] = df_features[numeric_cols].fillna(0)\n\n# Customer behavioral features\ndf_features['customer_transaction_count'] = df_features['transaction_amount_count'].fillna(0)\ndf_features['customer_avg_amount'] = df_features['transaction_amount_mean'].fillna(df_features['transaction_amount'])\ndf_features['customer_amount_std'] = df_features['transaction_amount_std'].fillna(0)\ndf_features['customer_total_spent'] = df_features['transaction_amount_sum'].fillna(0)\ndf_features['customer_fraud_history'] = df_features['is_fraud_sum'].fillna(0)\n\n# 3. Amount-based features\nprint(\"üí∞ Creating amount-based features...\")\n# Amount percentiles and z-scores\ndf_features['amount_percentile'] = df_features['transaction_amount'].rank(pct=True)\ndf_features['amount_zscore'] = ((df_features['transaction_amount'] - df_features['customer_avg_amount']) / \n                               (df_features['customer_amount_std'] + 1e-8))\n\n# Amount categories\namount_quartiles = df_features['transaction_amount'].quantile([0.25, 0.5, 0.75, 0.95])\ndf_features['amount_category'] = pd.cut(\n    df_features['transaction_amount'], \n    bins=[0] + amount_quartiles.tolist() + [float('inf')],\n    labels=['very_low', 'low', 'medium', 'high', 'very_high']\n)\n\n# Round dollar amounts (potential manual entry indicator)\ndf_features['is_round_amount'] = (df_features['transaction_amount'] % 1 == 0).astype(int)\ndf_features['is_round_ten'] = (df_features['transaction_amount'] % 10 == 0).astype(int)\ndf_features['is_round_hundred'] = (df_features['transaction_amount'] % 100 == 0).astype(int)\n\n# 4. Merchant risk scores\nprint(\"üè™ Creating merchant risk features...\")\n# Merchant statistics\nmerchant_stats = df_features.groupby('merchant_id').expanding().agg({\n    'transaction_amount': ['count', 'mean', 'std'],\n    'is_fraud': ['sum', 'mean']\n}).shift(1)\n\nmerchant_stats.columns = ['_'.join(col).strip() for col in merchant_stats.columns]\nmerchant_stats = merchant_stats.reset_index()\n\n# Add merchant prefix\nmerchant_stats.columns = ['merchant_' + col if col not in ['merchant_id', 'level_1'] \n                         else col for col in merchant_stats.columns]\n\ndf_features = df_features.reset_index().merge(\n    merchant_stats, \n    on=['merchant_id', 'level_1'], \n    how='left'\n).drop('level_1', axis=1).set_index('index')\n\n# Fill NaN values for new merchants\nmerchant_cols = [col for col in df_features.columns if col.startswith('merchant_')]\nfor col in merchant_cols:\n    if 'count' in col:\n        df_features[col] = df_features[col].fillna(0)\n    elif 'mean' in col or 'std' in col:\n        if 'amount' in col:\n            df_features[col] = df_features[col].fillna(df_features['transaction_amount'])\n        else:\n            df_features[col] = df_features[col].fillna(0)\n    else:\n        df_features[col] = df_features[col].fillna(0)\n\n# Merchant risk indicators\ndf_features['merchant_fraud_rate'] = df_features['merchant_is_fraud_mean'].fillna(0)\ndf_features['merchant_transaction_count'] = df_features['merchant_transaction_amount_count'].fillna(0)\ndf_features['is_new_merchant'] = (df_features['merchant_transaction_count'] < 10).astype(int)\n\n# 5. Velocity and sequence features\nprint(\"üöÄ Creating velocity and sequence features...\")\n# Sort by customer and time for velocity calculations\ndf_features = df_features.sort_values(['customer_id', 'transaction_datetime'])\n\n# Time since last transaction (by customer)\ndf_features['time_since_last_transaction'] = (\n    df_features.groupby('customer_id')['transaction_datetime']\n    .diff().dt.total_seconds() / 3600  # Convert to hours\n).fillna(24)  # Default to 24 hours for first transaction\n\n# Transaction frequency in different time windows\ndf_features['transactions_last_hour'] = (\n    df_features.groupby('customer_id')\n    .apply(lambda x: x.set_index('transaction_datetime')\n           .rolling('1H')['transaction_id'].count())\n    .values\n)\n\ndf_features['transactions_last_day'] = (\n    df_features.groupby('customer_id')\n    .apply(lambda x: x.set_index('transaction_datetime')\n           .rolling('1D')['transaction_id'].count())\n    .values\n)\n\n# Amount velocity\ndf_features['amount_last_hour'] = (\n    df_features.groupby('customer_id')\n    .apply(lambda x: x.set_index('transaction_datetime')\n           .rolling('1H')['transaction_amount'].sum())\n    .values\n)\n\ndf_features['amount_last_day'] = (\n    df_features.groupby('customer_id')\n    .apply(lambda x: x.set_index('transaction_datetime')\n           .rolling('1D')['transaction_amount'].sum())\n    .values\n)\n\n# 6. Interaction features\nprint(\"üîó Creating interaction features...\")\ndf_features['amount_risk_interaction'] = (df_features['transaction_amount'] * \n                                         df_features['composite_risk_score'])\ndf_features['velocity_amount_interaction'] = (df_features['previous_transactions_today'] * \n                                             df_features['amount_to_limit_ratio'])\ndf_features['time_amount_interaction'] = (df_features['unusual_time_flag'] * \n                                         df_features['transaction_amount'])\ndf_features['merchant_customer_risk'] = (df_features['merchant_fraud_rate'] * \n                                        df_features['customer_fraud_history'])\n\n# 7. One-hot encode categorical features\nprint(\"üè∑Ô∏è Encoding categorical features...\")\n# Merchant category encoding\nmerchant_category_encoded = pd.get_dummies(df_features['merchant_category'], prefix='merchant_cat')\ndf_features = pd.concat([df_features, merchant_category_encoded], axis=1)\n\n# Amount category encoding\namount_category_encoded = pd.get_dummies(df_features['amount_category'], prefix='amount_cat')\ndf_features = pd.concat([df_features, amount_category_encoded], axis=1)\n\n# Remove original categorical columns\ndf_features = df_features.drop(['merchant_category', 'amount_category'], axis=1)\n\n# 8. Feature scaling for distance-based features\nprint(\"üìè Creating scaled features for distance-based algorithms...\")\n# Identify numeric features for scaling\nnumeric_features = df_features.select_dtypes(include=[np.number]).columns\nfeature_cols = [col for col in numeric_features if col not in \n               ['transaction_id', 'customer_id', 'merchant_id', 'is_fraud']]\n\n# Create scaled versions\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(df_features[feature_cols])\nscaled_df = pd.DataFrame(scaled_features, columns=[f'{col}_scaled' for col in feature_cols])\ndf_features = pd.concat([df_features, scaled_df], axis=1)\n\nprint(f\"\\n‚úÖ Feature engineering complete!\")\nprint(f\"üìä Original features: {df.shape[1]}\")\nprint(f\"üõ†Ô∏è Engineered features: {df_features.shape[1]}\")\nprint(f\"‚ûï New features created: {df_features.shape[1] - df.shape[1]}\")\n\n# Show feature summary\nfeature_summary = pd.DataFrame({\n    'Feature_Type': [\n        'Time-based', 'Customer_frequency', 'Amount-based', \n        'Merchant_risk', 'Velocity', 'Interaction', 'Categorical_encoded', 'Scaled'\n    ],\n    'Count': [\n        len([col for col in df_features.columns if any(x in col for x in ['hour_', 'day_', 'month', 'quarter'])]),\n        len([col for col in df_features.columns if 'customer_' in col]),\n        len([col for col in df_features.columns if 'amount_' in col and 'customer_' not in col]),\n        len([col for col in df_features.columns if 'merchant_' in col]),\n        len([col for col in df_features.columns if any(x in col for x in ['transactions_', 'time_since', 'velocity'])]),\n        len([col for col in df_features.columns if 'interaction' in col]),\n        len([col for col in df_features.columns if any(x in col for x in ['_cat_', 'merchant_cat', 'amount_cat'])]),\n        len([col for col in df_features.columns if col.endswith('_scaled')])\n    ]\n})\ndisplay(feature_summary)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"üìä Dataset Overview\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total transactions: {len(enriched_df):,}\")\n",
    "print(f\"Fraudulent transactions: {enriched_df['is_fraud'].sum():,} ({enriched_df['is_fraud'].mean():.2%})\")\n",
    "print(f\"Normal transactions: {(~enriched_df['is_fraud'].astype(bool)).sum():,}\")\n",
    "print(f\"\\nTime range: {enriched_df['timestamp'].min()} to {enriched_df['timestamp'].max()}\")\n",
    "print(f\"\\nMissing values:\\n{enriched_df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Credit Card Fraud Detection - EDA', fontsize=16)\n",
    "\n",
    "# 1. Transaction amount distribution\n",
    "ax1 = axes[0, 0]\n",
    "normal_amounts = enriched_df[enriched_df['is_fraud'] == 0]['transaction_amount']\n",
    "fraud_amounts = enriched_df[enriched_df['is_fraud'] == 1]['transaction_amount']\n",
    "ax1.hist([normal_amounts, fraud_amounts], bins=50, label=['Normal', 'Fraud'], alpha=0.7)\n",
    "ax1.set_xlabel('Transaction Amount ($)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Transaction Amount Distribution')\n",
    "ax1.legend()\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# 2. Fraud by hour of day\n",
    "ax2 = axes[0, 1]\n",
    "fraud_by_hour = enriched_df.groupby('transaction_hour')['is_fraud'].agg(['sum', 'count'])\n",
    "fraud_by_hour['fraud_rate'] = fraud_by_hour['sum'] / fraud_by_hour['count']\n",
    "ax2.bar(fraud_by_hour.index, fraud_by_hour['fraud_rate'] * 100)\n",
    "ax2.set_xlabel('Hour of Day')\n",
    "ax2.set_ylabel('Fraud Rate (%)')\n",
    "ax2.set_title('Fraud Rate by Hour of Day')\n",
    "\n",
    "# 3. Merchant category analysis\n",
    "ax3 = axes[0, 2]\n",
    "fraud_by_category = enriched_df.groupby('merchant_category')['is_fraud'].agg(['sum', 'count'])\n",
    "fraud_by_category['fraud_rate'] = fraud_by_category['sum'] / fraud_by_category['count']\n",
    "fraud_by_category['fraud_rate'].plot(kind='bar', ax=ax3)\n",
    "ax3.set_xlabel('Merchant Category')\n",
    "ax3.set_ylabel('Fraud Rate')\n",
    "ax3.set_title('Fraud Rate by Merchant Category')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Risk score distribution\n",
    "ax4 = axes[1, 0]\n",
    "ax4.hist([enriched_df[enriched_df['is_fraud'] == 0]['risk_score'],\n",
    "          enriched_df[enriched_df['is_fraud'] == 1]['risk_score']], \n",
    "         bins=30, label=['Normal', 'Fraud'], alpha=0.7)\n",
    "ax4.set_xlabel('Risk Score')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.set_title('Risk Score Distribution')\n",
    "ax4.legend()\n",
    "\n",
    "# 5. Number of transactions per day\n",
    "ax5 = axes[1, 1]\n",
    "ax5.hist([enriched_df[enriched_df['is_fraud'] == 0]['num_transactions_today'],\n",
    "          enriched_df[enriched_df['is_fraud'] == 1]['num_transactions_today']], \n",
    "         bins=20, label=['Normal', 'Fraud'], alpha=0.7)\n",
    "ax5.set_xlabel('Number of Transactions Today')\n",
    "ax5.set_ylabel('Frequency')\n",
    "ax5.set_title('Daily Transaction Velocity')\n",
    "ax5.legend()\n",
    "\n",
    "# 6. Correlation heatmap\n",
    "ax6 = axes[1, 2]\n",
    "numeric_cols = enriched_df.select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = enriched_df[numeric_cols].corr()\n",
    "sns.heatmap(corr_matrix[['is_fraud']].sort_values(by='is_fraud', ascending=False)[1:11], \n",
    "            annot=True, cmap='coolwarm', center=0, ax=ax6)\n",
    "ax6.set_title('Top 10 Features Correlated with Fraud')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary by fraud status\n",
    "print(\"\\nüìà Statistical Summary by Fraud Status\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_stats = enriched_df.groupby('is_fraud')[numeric_cols].agg(['mean', 'std', 'median'])\n",
    "summary_stats = summary_stats.T\n",
    "summary_stats.columns = ['Normal_Mean', 'Normal_Std', 'Normal_Median', 'Fraud_Mean', 'Fraud_Std', 'Fraud_Median']\n",
    "summary_stats['Difference'] = summary_stats['Fraud_Mean'] - summary_stats['Normal_Mean']\n",
    "summary_stats['Ratio'] = summary_stats['Fraud_Mean'] / summary_stats['Normal_Mean']\n",
    "\n",
    "display(summary_stats.sort_values('Ratio', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data processor\n",
    "processor = DataProcessor(config=demo_config['preprocessing'])\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = [col for col in enriched_df.columns if col not in ['is_fraud', 'transaction_id', 'timestamp']]\n",
    "X = enriched_df[feature_cols].copy()\n",
    "y = enriched_df['is_fraud'].copy()\n",
    "\n",
    "print(\"üîß Preprocessing pipeline:\")\n",
    "print(\"1. Handle categorical variables\")\n",
    "print(\"2. Create time-based features\")\n",
    "print(\"3. Scale numerical features\")\n",
    "print(\"4. Handle class imbalance\")\n",
    "print(\"5. Feature selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "print(\"\\nüõ†Ô∏è Feature Engineering...\")\n",
    "\n",
    "# 1. One-hot encode categorical variables\n",
    "X_encoded = pd.get_dummies(X, columns=['merchant_category'], prefix='merchant')\n",
    "\n",
    "# 2. Create additional features\n",
    "X_encoded['amount_zscore'] = (X_encoded['transaction_amount'] - X_encoded['transaction_amount'].mean()) / X_encoded['transaction_amount'].std()\n",
    "X_encoded['high_risk_time'] = ((X_encoded['transaction_hour'] < 6) | (X_encoded['transaction_hour'] > 22)).astype(int)\n",
    "X_encoded['velocity_risk'] = X_encoded['num_transactions_today'] * X_encoded['amount_to_limit_ratio']\n",
    "X_encoded['composite_risk'] = X_encoded['risk_score'] * X_encoded['merchant_risk_score'] * X_encoded['location_risk']\n",
    "\n",
    "print(f\"Features after engineering: {X_encoded.shape[1]}\")\n",
    "print(f\"New features created: {set(X_encoded.columns) - set(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Train/Test Split:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Training fraud rate: {y_train.mean():.2%}\")\n",
    "print(f\"Test fraud rate: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "print(\"\\n‚öñÔ∏è Handling Class Imbalance...\")\n",
    "print(f\"Original class distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "\n",
    "# Create balanced dataset using SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.3, random_state=42)\n",
    "rus = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n",
    "\n",
    "# Create pipeline\n",
    "imbalance_pipeline = ImbPipeline([\n",
    "    ('smote', smote),\n",
    "    ('undersampling', rus)\n",
    "])\n",
    "\n",
    "X_train_balanced, y_train_balanced = imbalance_pipeline.fit_resample(X_train, y_train)\n",
    "print(f\"Balanced class distribution: {dict(zip(*np.unique(y_train_balanced, return_counts=True)))}\")\n",
    "print(f\"Balanced fraud rate: {y_train_balanced.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training - Multiple Algorithms and Frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MLflow\n",
    "mlflow.set_tracking_uri('sqlite:///mlflow.db')\n",
    "mlflow.set_experiment('credit_fraud_detection')\n",
    "\n",
    "# Store results for comparison\n",
    "model_results = {}\n",
    "\n",
    "print(\"ü§ñ Training Multiple Models...\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Random Forest (Scikit-learn)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "with mlflow.start_run(run_name=\"RandomForest_sklearn\"):\n",
    "    print(\"\\n1Ô∏è‚É£ Training Random Forest (Scikit-learn)...\")\n",
    "    \n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train_balanced, y_train_balanced)\n",
    "    rf_predictions = rf_model.predict(X_test)\n",
    "    rf_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "    \n",
    "    rf_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, rf_predictions),\n",
    "        'precision': precision_score(y_test, rf_predictions),\n",
    "        'recall': recall_score(y_test, rf_predictions),\n",
    "        'f1': f1_score(y_test, rf_predictions),\n",
    "        'roc_auc': roc_auc_score(y_test, rf_proba)\n",
    "    }\n",
    "    \n",
    "    # Log to MLflow\n",
    "    mlflow.log_params(rf_model.get_params())\n",
    "    mlflow.log_metrics(rf_metrics)\n",
    "    mlflow.sklearn.log_model(rf_model, \"model\")\n",
    "    \n",
    "    model_results['RandomForest'] = {\n",
    "        'model': rf_model,\n",
    "        'predictions': rf_predictions,\n",
    "        'probabilities': rf_proba,\n",
    "        'metrics': rf_metrics\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Random Forest - ROC AUC: {rf_metrics['roc_auc']:.4f}, Recall: {rf_metrics['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "with mlflow.start_run(run_name=\"XGBoost\"):\n",
    "    print(\"\\n2Ô∏è‚É£ Training XGBoost...\")\n",
    "    \n",
    "    # Calculate scale_pos_weight for imbalanced data\n",
    "    scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=10, verbose=False)\n",
    "    xgb_predictions = xgb_model.predict(X_test)\n",
    "    xgb_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    xgb_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, xgb_predictions),\n",
    "        'precision': precision_score(y_test, xgb_predictions),\n",
    "        'recall': recall_score(y_test, xgb_predictions),\n",
    "        'f1': f1_score(y_test, xgb_predictions),\n",
    "        'roc_auc': roc_auc_score(y_test, xgb_proba)\n",
    "    }\n",
    "    \n",
    "    mlflow.log_params(xgb_model.get_params())\n",
    "    mlflow.log_metrics(xgb_metrics)\n",
    "    mlflow.xgboost.log_model(xgb_model, \"model\")\n",
    "    \n",
    "    model_results['XGBoost'] = {\n",
    "        'model': xgb_model,\n",
    "        'predictions': xgb_predictions,\n",
    "        'probabilities': xgb_proba,\n",
    "        'metrics': xgb_metrics\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ XGBoost - ROC AUC: {xgb_metrics['roc_auc']:.4f}, Recall: {xgb_metrics['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "with mlflow.start_run(run_name=\"LightGBM\"):\n",
    "    print(\"\\n3Ô∏è‚É£ Training LightGBM...\")\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        num_leaves=31,\n",
    "        learning_rate=0.05,\n",
    "        feature_fraction=0.9,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=5,\n",
    "        n_estimators=100,\n",
    "        is_unbalance=True,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    lgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)])\n",
    "    lgb_predictions = lgb_model.predict(X_test)\n",
    "    lgb_proba = lgb_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    lgb_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, lgb_predictions),\n",
    "        'precision': precision_score(y_test, lgb_predictions),\n",
    "        'recall': recall_score(y_test, lgb_predictions),\n",
    "        'f1': f1_score(y_test, lgb_predictions),\n",
    "        'roc_auc': roc_auc_score(y_test, lgb_proba)\n",
    "    }\n",
    "    \n",
    "    mlflow.log_params(lgb_model.get_params())\n",
    "    mlflow.log_metrics(lgb_metrics)\n",
    "    mlflow.lightgbm.log_model(lgb_model, \"model\")\n",
    "    \n",
    "    model_results['LightGBM'] = {\n",
    "        'model': lgb_model,\n",
    "        'predictions': lgb_predictions,\n",
    "        'probabilities': lgb_proba,\n",
    "        'metrics': lgb_metrics\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ LightGBM - ROC AUC: {lgb_metrics['roc_auc']:.4f}, Recall: {lgb_metrics['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Neural Network (using Keras)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "with mlflow.start_run(run_name=\"NeuralNetwork\"):\n",
    "    print(\"\\n4Ô∏è‚É£ Training Neural Network...\")\n",
    "    \n",
    "    # Scale features for neural network\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Build model\n",
    "    nn_model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    nn_model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train with early stopping\n",
    "    early_stop = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "    \n",
    "    history = nn_model.fit(\n",
    "        X_train_scaled, y_train_balanced,\n",
    "        validation_split=0.2,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    nn_proba = nn_model.predict(X_test_scaled).flatten()\n",
    "    nn_predictions = (nn_proba > 0.5).astype(int)\n",
    "    \n",
    "    nn_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, nn_predictions),\n",
    "        'precision': precision_score(y_test, nn_predictions),\n",
    "        'recall': recall_score(y_test, nn_predictions),\n",
    "        'f1': f1_score(y_test, nn_predictions),\n",
    "        'roc_auc': roc_auc_score(y_test, nn_proba)\n",
    "    }\n",
    "    \n",
    "    mlflow.log_metrics(nn_metrics)\n",
    "    mlflow.tensorflow.log_model(nn_model, \"model\")\n",
    "    \n",
    "    model_results['NeuralNetwork'] = {\n",
    "        'model': nn_model,\n",
    "        'predictions': nn_predictions,\n",
    "        'probabilities': nn_proba,\n",
    "        'metrics': nn_metrics,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Neural Network - ROC AUC: {nn_metrics['roc_auc']:.4f}, Recall: {nn_metrics['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Comparison and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "print(\"\\nüìä Model Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    model_name: metrics['metrics'] \n",
    "    for model_name, metrics in model_results.items()\n",
    "}).T\n",
    "\n",
    "comparison_df = comparison_df.round(4)\n",
    "display(comparison_df.sort_values('roc_auc', ascending=False))\n",
    "\n",
    "# Best model\n",
    "best_model_name = comparison_df['roc_auc'].idxmax()\n",
    "print(f\"\\nüèÜ Best model: {best_model_name} (ROC AUC: {comparison_df.loc[best_model_name, 'roc_auc']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16)\n",
    "\n",
    "# 1. Metrics comparison\n",
    "ax1 = axes[0, 0]\n",
    "comparison_df.plot(kind='bar', ax=ax1)\n",
    "ax1.set_title('All Metrics Comparison')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. ROC Curves\n",
    "ax2 = axes[0, 1]\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "for model_name, results in model_results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, results['probabilities'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax2.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "ax2.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('ROC Curves')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Precision-Recall Curves\n",
    "ax3 = axes[1, 0]\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "for model_name, results in model_results.items():\n",
    "    precision, recall, _ = precision_recall_curve(y_test, results['probabilities'])\n",
    "    ax3.plot(recall, precision, label=model_name)\n",
    "\n",
    "ax3.set_xlabel('Recall')\n",
    "ax3.set_ylabel('Precision')\n",
    "ax3.set_title('Precision-Recall Curves')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Confusion Matrices for best model\n",
    "ax4 = axes[1, 1]\n",
    "best_model_results = model_results[best_model_name]\n",
    "cm = confusion_matrix(y_test, best_model_results['predictions'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax4)\n",
    "ax4.set_title(f'Confusion Matrix - {best_model_name}')\n",
    "ax4.set_xlabel('Predicted')\n",
    "ax4.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Business Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business impact calculation\n",
    "print(\"\\nüí∞ Business Impact Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Constants from configuration\n",
    "false_positive_cost = demo_config['evaluation']['business_metrics']['false_positive_cost']\n",
    "fraud_loss_prevented = demo_config['evaluation']['business_metrics']['fraud_loss_prevented']\n",
    "\n",
    "business_impact = {}\n",
    "\n",
    "for model_name, results in model_results.items():\n",
    "    predictions = results['predictions']\n",
    "    \n",
    "    # Calculate confusion matrix values\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    \n",
    "    # Business metrics\n",
    "    fraud_caught = tp\n",
    "    fraud_missed = fn\n",
    "    false_alarms = fp\n",
    "    \n",
    "    # Financial impact\n",
    "    money_saved = fraud_caught * fraud_loss_prevented\n",
    "    false_alarm_cost = false_alarms * false_positive_cost\n",
    "    money_lost = fraud_missed * fraud_loss_prevented\n",
    "    net_benefit = money_saved - false_alarm_cost - money_lost\n",
    "    \n",
    "    business_impact[model_name] = {\n",
    "        'fraud_caught': fraud_caught,\n",
    "        'fraud_missed': fraud_missed,\n",
    "        'false_alarms': false_alarms,\n",
    "        'money_saved': money_saved,\n",
    "        'false_alarm_cost': false_alarm_cost,\n",
    "        'money_lost': money_lost,\n",
    "        'net_benefit': net_benefit,\n",
    "        'roi': (net_benefit / (money_saved + false_alarm_cost)) * 100 if (money_saved + false_alarm_cost) > 0 else 0\n",
    "    }\n",
    "\n",
    "# Display business impact\n",
    "business_df = pd.DataFrame(business_impact).T\n",
    "business_df = business_df.round(2)\n",
    "display(business_df.sort_values('net_benefit', ascending=False))\n",
    "\n",
    "# Best model from business perspective\n",
    "best_business_model = business_df['net_benefit'].idxmax()\n",
    "print(f\"\\nüíé Best model from business perspective: {best_business_model}\")\n",
    "print(f\"   Net benefit: ${business_df.loc[best_business_model, 'net_benefit']:,.2f}\")\n",
    "print(f\"   ROI: {business_df.loc[best_business_model, 'roi']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP analysis for best model\n",
    "import shap\n",
    "\n",
    "print(\"\\nüîç Model Explainability Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use the best model for explainability\n",
    "best_model = model_results[best_model_name]['model']\n",
    "\n",
    "# For tree-based models, use TreeExplainer\n",
    "if best_model_name in ['RandomForest', 'XGBoost', 'LightGBM']:\n",
    "    print(f\"\\nGenerating SHAP values for {best_model_name}...\")\n",
    "    \n",
    "    # Create explainer\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    \n",
    "    # Calculate SHAP values for test set (sample for speed)\n",
    "    X_test_sample = X_test.sample(min(100, len(X_test)), random_state=42)\n",
    "    shap_values = explainer.shap_values(X_test_sample)\n",
    "    \n",
    "    # For binary classification, take values for positive class\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, X_test_sample, plot_type=\"bar\", show=False)\n",
    "    plt.title(f'Feature Importance - {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed summary plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, X_test_sample, show=False)\n",
    "    plt.title(f'SHAP Feature Impact - {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance comparison across models\n",
    "print(\"\\nüìä Feature Importance Across Models\")\n",
    "\n",
    "feature_importance_dict = {}\n",
    "\n",
    "# Get feature importance for tree-based models\n",
    "for model_name in ['RandomForest', 'XGBoost', 'LightGBM']:\n",
    "    if model_name in model_results:\n",
    "        model = model_results[model_name]['model']\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            feature_importance_dict[model_name] = pd.Series(importances, index=X_test.columns)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "if feature_importance_dict:\n",
    "    importance_df = pd.DataFrame(feature_importance_dict)\n",
    "    importance_df['mean_importance'] = importance_df.mean(axis=1)\n",
    "    top_features = importance_df.nlargest(15, 'mean_importance')\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_features.drop('mean_importance', axis=1).plot(kind='barh')\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 15 Features - Model Comparison')\n",
    "    plt.legend(title='Model')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display top features table\n",
    "    print(\"\\nTop 10 Most Important Features (averaged across models):\")\n",
    "    display(top_features.sort_values('mean_importance', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. PySpark Version Toggle\n",
    "\n",
    "Demonstrate how to switch between pandas and PySpark for large-scale processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if PySpark is available\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "    from pyspark.ml.classification import RandomForestClassifier as SparkRF\n",
    "    from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "    \n",
    "    PYSPARK_AVAILABLE = True\n",
    "    print(\"‚úÖ PySpark is available\")\n",
    "except ImportError:\n",
    "    PYSPARK_AVAILABLE = False\n",
    "    print(\"‚ùå PySpark not available - using pandas version\")\n",
    "\n",
    "if PYSPARK_AVAILABLE:\n",
    "    print(\"\\n‚ö° Demonstrating PySpark Processing...\")\n",
    "    \n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"FraudDetectionPySpark\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Convert pandas DataFrame to Spark DataFrame\n",
    "    spark_df = spark.createDataFrame(enriched_df)\n",
    "    print(f\"Spark DataFrame created with {spark_df.count()} rows\")\n",
    "    \n",
    "    # Prepare features for Spark ML\n",
    "    feature_cols = [col for col in spark_df.columns \n",
    "                   if col not in ['is_fraud', 'transaction_id', 'timestamp', 'merchant_category']]\n",
    "    \n",
    "    # Create ML pipeline\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    rf_spark = SparkRF(featuresCol=\"scaledFeatures\", labelCol=\"is_fraud\", numTrees=50)\n",
    "    \n",
    "    pipeline = Pipeline(stages=[assembler, scaler, rf_spark])\n",
    "    \n",
    "    # Split data\n",
    "    train_spark, test_spark = spark_df.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training Random Forest with PySpark...\")\n",
    "    spark_model = pipeline.fit(train_spark)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions_spark = spark_model.transform(test_spark)\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"is_fraud\", metricName=\"areaUnderROC\")\n",
    "    auc_spark = evaluator.evaluate(predictions_spark)\n",
    "    \n",
    "    print(f\"\\n‚úÖ PySpark Random Forest - ROC AUC: {auc_spark:.4f}\")\n",
    "    print(f\"Processing speed comparison:\")\n",
    "    print(f\"  - Pandas version: suitable for < 1GB data\")\n",
    "    print(f\"  - PySpark version: suitable for > 1GB data, distributed processing\")\n",
    "    \n",
    "    # Show sample predictions\n",
    "    print(\"\\nSample PySpark predictions:\")\n",
    "    predictions_spark.select(\"transaction_id\", \"is_fraud\", \"prediction\", \"probability\") \\\n",
    "        .show(5, truncate=False)\n",
    "    \n",
    "    # Stop Spark session\n",
    "    spark.stop()\n",
    "else:\n",
    "    print(\"\\nüí° To use PySpark version:\")\n",
    "    print(\"1. Install PySpark: pip install pyspark\")\n",
    "    print(\"2. Ensure Java 8+ is installed\")\n",
    "    print(\"3. Set SPARK_HOME environment variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Feature Elimination Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use FeatureEliminator from the framework\n",
    "from src.preprocessing.feature_elimination import FeatureEliminator\n",
    "\n",
    "print(\"\\nüî¨ Backward Feature Elimination Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize feature eliminator\n",
    "eliminator = FeatureEliminator(\n",
    "    estimator=RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1),\n",
    "    min_features_to_select=5,\n",
    "    step=1,\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform elimination on a subset for speed\n",
    "X_train_subset = X_train.sample(min(2000, len(X_train)), random_state=42)\n",
    "y_train_subset = y_train.loc[X_train_subset.index]\n",
    "\n",
    "print(f\"Starting with {X_train_subset.shape[1]} features...\")\n",
    "elimination_results = eliminator.fit(X_train_subset, y_train_subset)\n",
    "\n",
    "# Get optimal features\n",
    "optimal_features = eliminator.get_selected_features()\n",
    "print(f\"\\n‚úÖ Optimal number of features: {len(optimal_features)}\")\n",
    "print(f\"Selected features: {optimal_features[:10]}...\")  # Show first 10\n",
    "\n",
    "# Plot elimination history\n",
    "eliminator.plot_elimination_history(save_path='../artifacts/feature_elimination.png')\n",
    "\n",
    "# Generate Excel report\n",
    "eliminator.generate_excel_report('../artifacts/feature_elimination_report.xlsx')\n",
    "print(\"\\nüìä Feature elimination report saved to artifacts/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Model Deployment Readiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model selection and deployment preparation\n",
    "print(\"\\nüöÄ Model Deployment Readiness\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Select final model based on business metrics\n",
    "final_model_name = best_business_model\n",
    "final_model = model_results[final_model_name]['model']\n",
    "final_metrics = model_results[final_model_name]['metrics']\n",
    "\n",
    "print(f\"Selected model for deployment: {final_model_name}\")\n",
    "print(f\"\\nPerformance metrics:\")\n",
    "for metric, value in final_metrics.items():\n",
    "    print(f\"  - {metric}: {value:.4f}\")\n",
    "\n",
    "# Save model artifacts\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "artifacts_dir = '../artifacts/fraud_detection'\n",
    "os.makedirs(artifacts_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = f\"{artifacts_dir}/fraud_model_{final_model_name.lower()}.pkl\"\n",
    "joblib.dump(final_model, model_path)\n",
    "print(f\"\\n‚úÖ Model saved to: {model_path}\")\n",
    "\n",
    "# Save preprocessing pipeline\n",
    "preprocessing_artifacts = {\n",
    "    'feature_columns': list(X_test.columns),\n",
    "    'feature_types': X_test.dtypes.to_dict(),\n",
    "    'model_name': final_model_name,\n",
    "    'model_metrics': final_metrics,\n",
    "    'business_impact': business_impact[final_model_name],\n",
    "    'threshold': 0.5,\n",
    "    'created_at': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(f\"{artifacts_dir}/preprocessing_pipeline.pkl\", 'wb') as f:\n",
    "    pickle.dump(preprocessing_artifacts, f)\n",
    "\n",
    "# Create deployment configuration\n",
    "deployment_config = {\n",
    "    'model': {\n",
    "        'name': final_model_name,\n",
    "        'version': '1.0.0',\n",
    "        'path': model_path,\n",
    "        'framework': 'sklearn' if 'sklearn' in str(type(final_model)) else str(type(final_model).__module__).split('.')[0]\n",
    "    },\n",
    "    'serving': {\n",
    "        'endpoint': '/predict',\n",
    "        'batch_endpoint': '/predict_batch',\n",
    "        'health_check': '/health',\n",
    "        'max_batch_size': 1000\n",
    "    },\n",
    "    'monitoring': {\n",
    "        'log_predictions': True,\n",
    "        'drift_detection': True,\n",
    "        'alert_thresholds': {\n",
    "            'precision_drop': 0.1,\n",
    "            'recall_drop': 0.15,\n",
    "            'prediction_volume_spike': 2.0\n",
    "        }\n",
    "    },\n",
    "    'infrastructure': {\n",
    "        'replicas': 3,\n",
    "        'cpu_request': '500m',\n",
    "        'memory_request': '1Gi',\n",
    "        'autoscaling': {\n",
    "            'enabled': True,\n",
    "            'min_replicas': 2,\n",
    "            'max_replicas': 10,\n",
    "            'target_cpu': 70\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{artifacts_dir}/deployment_config.json\", 'w') as f:\n",
    "    json.dump(deployment_config, f, indent=2)\n",
    "\n",
    "print(\"\\nüì¶ Deployment artifacts created:\")\n",
    "print(f\"  - Model: {model_path}\")\n",
    "print(f\"  - Preprocessing pipeline: {artifacts_dir}/preprocessing_pipeline.pkl\")\n",
    "print(f\"  - Deployment config: {artifacts_dir}/deployment_config.json\")\n",
    "print(\"\\n‚úÖ Model is ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary report\n",
    "print(\"\\nüìã CREDIT CARD FRAUD DETECTION - SUMMARY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary = f\"\"\"\n",
    "## Dataset Summary\n",
    "- Total transactions: {len(enriched_df):,}\n",
    "- Fraud rate: {enriched_df['is_fraud'].mean():.2%}\n",
    "- Features used: {X_train.shape[1]}\n",
    "- Time period: {enriched_df['timestamp'].min().date()} to {enriched_df['timestamp'].max().date()}\n",
    "\n",
    "## Models Evaluated\n",
    "1. Random Forest (Scikit-learn)\n",
    "2. XGBoost\n",
    "3. LightGBM\n",
    "4. Neural Network (TensorFlow/Keras)\n",
    "\n",
    "## Best Model Performance\n",
    "- Model: {best_model_name}\n",
    "- ROC AUC: {comparison_df.loc[best_model_name, 'roc_auc']:.4f}\n",
    "- Precision: {comparison_df.loc[best_model_name, 'precision']:.4f}\n",
    "- Recall: {comparison_df.loc[best_model_name, 'recall']:.4f}\n",
    "- F1 Score: {comparison_df.loc[best_model_name, 'f1']:.4f}\n",
    "\n",
    "## Business Impact (Best Model)\n",
    "- Fraud caught: {business_df.loc[best_business_model, 'fraud_caught']:.0f} transactions\n",
    "- Money saved: ${business_df.loc[best_business_model, 'money_saved']:,.2f}\n",
    "- False alarm cost: ${business_df.loc[best_business_model, 'false_alarm_cost']:,.2f}\n",
    "- Net benefit: ${business_df.loc[best_business_model, 'net_benefit']:,.2f}\n",
    "- ROI: {business_df.loc[best_business_model, 'roi']:.1f}%\n",
    "\n",
    "## Key Features (Top 5)\n",
    "\"\"\"\n",
    "\n",
    "# Add top features if available\n",
    "if 'importance_df' in locals():\n",
    "    top_5_features = importance_df.nlargest(5, 'mean_importance')['mean_importance']\n",
    "    for i, (feature, importance) in enumerate(top_5_features.items(), 1):\n",
    "        summary += f\"{i}. {feature}: {importance:.4f}\\n\"\n",
    "\n",
    "summary += f\"\"\"\n",
    "## Deployment Status\n",
    "- Model saved: ‚úÖ\n",
    "- Preprocessing pipeline: ‚úÖ\n",
    "- Deployment configuration: ‚úÖ\n",
    "- MLflow tracking: ‚úÖ\n",
    "- Ready for production: ‚úÖ\n",
    "\n",
    "## Next Steps\n",
    "1. Deploy model using Kubernetes deployment scripts\n",
    "2. Set up real-time monitoring dashboard\n",
    "3. Configure alert system for model drift\n",
    "4. Schedule periodic retraining pipeline\n",
    "5. A/B test against current production model\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary report\n",
    "with open(f\"{artifacts_dir}/summary_report.txt\", 'w') as f:\n",
    "    f.write(summary)\n",
    "    \n",
    "print(\"\\n‚úÖ Full pipeline demonstration complete!\")\n",
    "print(f\"üìÅ All artifacts saved to: {artifacts_dir}\")\n",
    "print(\"üöÄ Model ready for deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}