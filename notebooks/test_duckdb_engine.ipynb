{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DuckDB Engine Testing and Analysis\n",
    "\n",
    "This notebook demonstrates DuckDB capabilities for analytics-heavy workloads, SQL-based data processing, and out-of-core operations for the ML Pipeline Framework.\n",
    "\n",
    "## Topics Covered:\n",
    "- ðŸ¦† SQL-based data processing with DuckDB\n",
    "- ðŸ’¾ Out-of-core operations on large CSVs\n",
    "- âš¡ Performance benchmarks vs pandas/Polars\n",
    "- ðŸ” Complex analytical queries\n",
    "- ðŸ¤ Integration with Python ML workflows\n",
    "- ðŸ“Š OLAP-style analytics and aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"DuckDB version: {duckdb.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"System CPU cores: {psutil.cpu_count()}\")\n",
    "print(f\"System memory: {psutil.virtual_memory().total / 1024**3:.1f} GB\")\n",
    "print(f\"Available memory: {psutil.virtual_memory().available / 1024**3:.1f} GB\")\n",
    "\n",
    "# Initialize DuckDB connection\n",
    "conn = duckdb.connect(':memory:')\n",
    "\n",
    "# Install and load useful extensions\n",
    "try:\n",
    "    conn.execute(\"INSTALL httpfs\")\n",
    "    conn.execute(\"LOAD httpfs\")\n",
    "    print(\"âœ… HTTP filesystem extension loaded\")\n",
    "except:\n",
    "    print(\"âš ï¸  HTTP filesystem extension not available\")\n",
    "\n",
    "try:\n",
    "    conn.execute(\"INSTALL parquet\")\n",
    "    conn.execute(\"LOAD parquet\")\n",
    "    print(\"âœ… Parquet extension loaded\")\n",
    "except:\n",
    "    print(\"âš ï¸  Parquet extension not available\")\n",
    "\n",
    "print(f\"\\nDuckDB Extensions available:\")\n",
    "extensions = conn.execute(\"SELECT extension_name, loaded FROM duckdb_extensions() WHERE installed\").fetchall()\n",
    "for ext_name, loaded in extensions:\n",
    "    status = \"âœ…\" if loaded else \"â³\"\n",
    "    print(f\"  {status} {ext_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_duckdb_test_data(n_rows=100000, save_path=None):\n",
    "    \"\"\"Generate test data optimized for DuckDB analytics testing.\"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate realistic e-commerce/sales data\n",
    "    data = {\n",
    "        # Primary keys\n",
    "        'transaction_id': range(1, n_rows + 1),\n",
    "        'customer_id': np.random.randint(1, n_rows//20, n_rows),\n",
    "        'product_id': np.random.randint(1, 5000, n_rows),\n",
    "        'store_id': np.random.randint(1, 100, n_rows),\n",
    "        \n",
    "        # Transaction details\n",
    "        'quantity': np.random.randint(1, 20, n_rows),\n",
    "        'unit_price': np.random.lognormal(3, 0.8, n_rows),\n",
    "        'discount_rate': np.random.beta(1, 9, n_rows),  # Most transactions have low discounts\n",
    "        'tax_rate': np.random.choice([0.05, 0.08, 0.10, 0.12], n_rows),\n",
    "        \n",
    "        # Categories\n",
    "        'category': np.random.choice([\n",
    "            'Electronics', 'Clothing', 'Home & Garden', 'Sports & Outdoors',\n",
    "            'Books', 'Beauty & Personal Care', 'Automotive', 'Toys & Games'\n",
    "        ], n_rows),\n",
    "        \n",
    "        'subcategory': np.random.choice([\n",
    "            'Smartphones', 'Laptops', 'Headphones', 'Shirts', 'Pants', 'Shoes',\n",
    "            'Furniture', 'Kitchen', 'Garden Tools', 'Fiction', 'Non-Fiction',\n",
    "            'Skincare', 'Makeup', 'Car Parts', 'Board Games', 'Action Figures'\n",
    "        ], n_rows),\n",
    "        \n",
    "        'brand': np.random.choice([\n",
    "            'Apple', 'Samsung', 'Nike', 'Adidas', 'IKEA', 'Sony', 'Microsoft',\n",
    "            'Amazon', 'Dell', 'HP', 'Canon', 'Nikon', 'Toyota', 'Honda'\n",
    "        ], n_rows),\n",
    "        \n",
    "        # Geographic data\n",
    "        'country': np.random.choice(['USA', 'Canada', 'UK', 'Germany', 'France', 'Japan'], n_rows),\n",
    "        'region': np.random.choice(['North', 'South', 'East', 'West', 'Central'], n_rows),\n",
    "        'city': np.random.choice([\n",
    "            'New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix',\n",
    "            'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', 'San Jose'\n",
    "        ], n_rows),\n",
    "        \n",
    "        # Customer attributes\n",
    "        'customer_segment': np.random.choice(['Premium', 'Standard', 'Budget'], n_rows, p=[0.2, 0.5, 0.3]),\n",
    "        'payment_method': np.random.choice(['Credit Card', 'Debit Card', 'PayPal', 'Cash'], n_rows),\n",
    "        'channel': np.random.choice(['Online', 'In-Store', 'Mobile App'], n_rows),\n",
    "        \n",
    "        # Boolean flags\n",
    "        'is_member': np.random.choice([True, False], n_rows, p=[0.4, 0.6]),\n",
    "        'is_first_purchase': np.random.choice([True, False], n_rows, p=[0.1, 0.9]),\n",
    "        'is_returned': np.random.choice([True, False], n_rows, p=[0.05, 0.95]),\n",
    "        \n",
    "        # Ratings and reviews\n",
    "        'product_rating': np.random.normal(4.2, 0.8, n_rows),\n",
    "        'delivery_rating': np.random.normal(4.0, 1.0, n_rows),\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add date columns with realistic patterns\n",
    "    start_date = datetime(2020, 1, 1)\n",
    "    df['transaction_date'] = pd.date_range(\n",
    "        start=start_date, \n",
    "        periods=n_rows, \n",
    "        freq='5min'  # More realistic transaction frequency\n",
    "    )\n",
    "    \n",
    "    # Add seasonal patterns\n",
    "    df['month'] = df['transaction_date'].dt.month\n",
    "    df['quarter'] = df['transaction_date'].dt.quarter\n",
    "    df['day_of_week'] = df['transaction_date'].dt.dayofweek\n",
    "    df['hour'] = df['transaction_date'].dt.hour\n",
    "    \n",
    "    # Calculate derived columns\n",
    "    df['gross_amount'] = df['quantity'] * df['unit_price']\n",
    "    df['discount_amount'] = df['gross_amount'] * df['discount_rate']\n",
    "    df['net_amount'] = df['gross_amount'] - df['discount_amount']\n",
    "    df['tax_amount'] = df['net_amount'] * df['tax_rate']\n",
    "    df['total_amount'] = df['net_amount'] + df['tax_amount']\n",
    "    \n",
    "    # Add some realistic constraints\n",
    "    df['product_rating'] = df['product_rating'].clip(1, 5)\n",
    "    df['delivery_rating'] = df['delivery_rating'].clip(1, 5)\n",
    "    \n",
    "    # Add some missing values (realistic patterns)\n",
    "    missing_indices = np.random.choice(df.index, size=int(0.02 * len(df)), replace=False)\n",
    "    df.loc[missing_indices, 'product_rating'] = np.nan\n",
    "    \n",
    "    # Missing delivery ratings for in-store purchases\n",
    "    in_store_mask = df['channel'] == 'In-Store'\n",
    "    df.loc[in_store_mask, 'delivery_rating'] = np.nan\n",
    "    \n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"DuckDB test data saved to {save_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate test datasets\n",
    "test_sizes = [50000, 100000, 250000, 500000]\n",
    "duckdb_test_files = {}\n",
    "\n",
    "for size in test_sizes:\n",
    "    file_path = f'../data/duckdb_test_{size}.csv'\n",
    "    duckdb_test_files[size] = file_path\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Generating DuckDB test data with {size:,} rows...\")\n",
    "        df = generate_duckdb_test_data(size, file_path)\n",
    "    else:\n",
    "        print(f\"DuckDB test data with {size:,} rows already exists\")\n",
    "\n",
    "print(\"\\nDuckDB test files:\")\n",
    "for size, path in duckdb_test_files.items():\n",
    "    file_size = os.path.getsize(path) / 1024**2\n",
    "    print(f\"  {size:,} rows: {path} ({file_size:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL-based Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ¦† DuckDB SQL-based Data Processing Demo\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Use medium-sized dataset for demonstration\n",
    "test_file = duckdb_test_files[100000]\n",
    "\n",
    "print(f\"\\nWorking with: {test_file}\")\n",
    "print(f\"File size: {os.path.getsize(test_file) / 1024**2:.1f} MB\")\n",
    "\n",
    "print(\"\\n1. Direct CSV Querying (No Loading Required)\")\n",
    "\n",
    "# DuckDB can query CSV files directly without loading into memory\n",
    "start_time = time.time()\n",
    "\n",
    "# Basic aggregation query\n",
    "basic_query = f\"\"\"\n",
    "SELECT \n",
    "    category,\n",
    "    COUNT(*) as transaction_count,\n",
    "    SUM(total_amount) as total_sales,\n",
    "    AVG(total_amount) as avg_sale,\n",
    "    MAX(total_amount) as max_sale,\n",
    "    MIN(total_amount) as min_sale\n",
    "FROM read_csv_auto('{test_file}')\n",
    "GROUP BY category\n",
    "ORDER BY total_sales DESC\n",
    "\"\"\"\n",
    "\n",
    "result = conn.execute(basic_query).fetchdf()\n",
    "query_time = time.time() - start_time\n",
    "\n",
    "print(f\"   Query execution time: {query_time:.3f}s\")\n",
    "print(f\"   Result shape: {result.shape}\")\n",
    "print(\"\\n   Sales by Category:\")\n",
    "print(result.head())\n",
    "\n",
    "print(\"\\n2. Complex Analytical Queries\")\n",
    "\n",
    "# Time-series analysis with window functions\n",
    "timeseries_query = f\"\"\"\n",
    "WITH monthly_sales AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('month', transaction_date::TIMESTAMP) as month,\n",
    "        category,\n",
    "        SUM(total_amount) as monthly_total,\n",
    "        COUNT(*) as transaction_count,\n",
    "        AVG(total_amount) as avg_transaction\n",
    "    FROM read_csv_auto('{test_file}')\n",
    "    GROUP BY DATE_TRUNC('month', transaction_date::TIMESTAMP), category\n",
    ")\n",
    "SELECT \n",
    "    month,\n",
    "    category,\n",
    "    monthly_total,\n",
    "    LAG(monthly_total, 1) OVER (PARTITION BY category ORDER BY month) as prev_month_total,\n",
    "    (monthly_total - LAG(monthly_total, 1) OVER (PARTITION BY category ORDER BY month)) / \n",
    "        LAG(monthly_total, 1) OVER (PARTITION BY category ORDER BY month) * 100 as growth_rate,\n",
    "    AVG(monthly_total) OVER (PARTITION BY category ORDER BY month ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as rolling_avg_3m\n",
    "FROM monthly_sales\n",
    "ORDER BY category, month\n",
    "\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "timeseries_result = conn.execute(timeseries_query).fetchdf()\n",
    "timeseries_time = time.time() - start_time\n",
    "\n",
    "print(f\"   Time-series query time: {timeseries_time:.3f}s\")\n",
    "print(f\"   Result shape: {timeseries_result.shape}\")\n",
    "print(\"\\n   Monthly Growth Analysis (sample):\")\n",
    "print(timeseries_result.head(10))\n",
    "\n",
    "print(\"\\n3. Customer Segmentation Analysis\")\n",
    "\n",
    "# RFM analysis using SQL\n",
    "rfm_query = f\"\"\"\n",
    "WITH customer_metrics AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        MAX(transaction_date::TIMESTAMP) as last_purchase_date,\n",
    "        COUNT(*) as frequency,\n",
    "        SUM(total_amount) as total_spent,\n",
    "        AVG(total_amount) as avg_order_value\n",
    "    FROM read_csv_auto('{test_file}')\n",
    "    GROUP BY customer_id\n",
    "),\n",
    "rfm_scores AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        DATE_DIFF('day', last_purchase_date, CURRENT_DATE) as recency_days,\n",
    "        frequency,\n",
    "        total_spent as monetary,\n",
    "        NTILE(5) OVER (ORDER BY DATE_DIFF('day', last_purchase_date, CURRENT_DATE) DESC) as recency_score,\n",
    "        NTILE(5) OVER (ORDER BY frequency) as frequency_score,\n",
    "        NTILE(5) OVER (ORDER BY total_spent) as monetary_score\n",
    "    FROM customer_metrics\n",
    ")\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN recency_score >= 4 AND frequency_score >= 4 AND monetary_score >= 4 THEN 'Champions'\n",
    "        WHEN recency_score >= 3 AND frequency_score >= 3 AND monetary_score >= 3 THEN 'Loyal Customers'\n",
    "        WHEN recency_score >= 3 AND frequency_score <= 2 THEN 'Potential Loyalists'\n",
    "        WHEN recency_score <= 2 AND frequency_score >= 3 THEN 'At Risk'\n",
    "        WHEN recency_score <= 2 AND frequency_score <= 2 AND monetary_score >= 3 THEN 'Cannot Lose Them'\n",
    "        ELSE 'Others'\n",
    "    END as customer_segment,\n",
    "    COUNT(*) as customer_count,\n",
    "    AVG(monetary) as avg_total_spent,\n",
    "    AVG(frequency) as avg_frequency,\n",
    "    AVG(recency_days) as avg_recency_days\n",
    "FROM rfm_scores\n",
    "GROUP BY customer_segment\n",
    "ORDER BY customer_count DESC\n",
    "\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "rfm_result = conn.execute(rfm_query).fetchdf()\n",
    "rfm_time = time.time() - start_time\n",
    "\n",
    "print(f\"   RFM analysis time: {rfm_time:.3f}s\")\n",
    "print(f\"   Customer segments identified: {len(rfm_result)}\")\n",
    "print(\"\\n   Customer Segmentation Results:\")\n",
    "print(rfm_result)\n",
    "\n",
    "print(\"\\n4. Product Performance Analysis\")\n",
    "\n",
    "# Product performance with statistical functions\n",
    "product_analysis = f\"\"\"\n",
    "SELECT \n",
    "    category,\n",
    "    subcategory,\n",
    "    brand,\n",
    "    COUNT(*) as sales_count,\n",
    "    SUM(total_amount) as total_revenue,\n",
    "    AVG(total_amount) as avg_sale_amount,\n",
    "    STDDEV(total_amount) as sale_amount_stddev,\n",
    "    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY total_amount) as median_sale,\n",
    "    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY total_amount) as p95_sale,\n",
    "    AVG(product_rating) as avg_rating,\n",
    "    SUM(CASE WHEN is_returned THEN 1 ELSE 0 END)::FLOAT / COUNT(*) * 100 as return_rate,\n",
    "    AVG(discount_rate) * 100 as avg_discount_pct\n",
    "FROM read_csv_auto('{test_file}')\n",
    "WHERE product_rating IS NOT NULL\n",
    "GROUP BY category, subcategory, brand\n",
    "HAVING COUNT(*) >= 10  -- Only products with significant sales\n",
    "ORDER BY total_revenue DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "product_result = conn.execute(product_analysis).fetchdf()\n",
    "product_time = time.time() - start_time\n",
    "\n",
    "print(f\"   Product analysis time: {product_time:.3f}s\")\n",
    "print(\"\\n   Top Product Performance:\")\n",
    "print(product_result.head())\n",
    "\n",
    "print(f\"\\nðŸ“Š SQL Query Performance Summary:\")\n",
    "print(f\"   â€¢ Basic aggregation: {query_time:.3f}s\")\n",
    "print(f\"   â€¢ Time-series analysis: {timeseries_time:.3f}s\")\n",
    "print(f\"   â€¢ Customer segmentation: {rfm_time:.3f}s\")\n",
    "print(f\"   â€¢ Product analysis: {product_time:.3f}s\")\n",
    "print(f\"   â€¢ Total analysis time: {query_time + timeseries_time + rfm_time + product_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Core Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ’¾ DuckDB Out-of-Core Operations Demo\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Use the largest dataset to demonstrate out-of-core capabilities\n",
    "large_file = duckdb_test_files[500000]\n",
    "file_size_mb = os.path.getsize(large_file) / 1024**2\n",
    "\n",
    "print(f\"\\nProcessing large file: {large_file}\")\n",
    "print(f\"File size: {file_size_mb:.1f} MB\")\n",
    "\n",
    "# Monitor memory usage\n",
    "process = psutil.Process()\n",
    "initial_memory = process.memory_info().rss / 1024**2\n",
    "\n",
    "print(f\"Initial memory usage: {initial_memory:.1f} MB\")\n",
    "\n",
    "print(\"\\n1. Large-scale Aggregation (Out-of-Core)\")\n",
    "\n",
    "start_time = time.time()\n",
    "memory_before = process.memory_info().rss / 1024**2\n",
    "\n",
    "# Complex aggregation on large dataset\n",
    "large_aggregation = f\"\"\"\n",
    "SELECT \n",
    "    country,\n",
    "    region,\n",
    "    category,\n",
    "    customer_segment,\n",
    "    DATE_TRUNC('quarter', transaction_date::TIMESTAMP) as quarter,\n",
    "    COUNT(*) as transaction_count,\n",
    "    COUNT(DISTINCT customer_id) as unique_customers,\n",
    "    COUNT(DISTINCT product_id) as unique_products,\n",
    "    SUM(total_amount) as total_revenue,\n",
    "    AVG(total_amount) as avg_transaction,\n",
    "    STDDEV(total_amount) as revenue_volatility,\n",
    "    SUM(quantity) as total_quantity,\n",
    "    AVG(discount_rate) * 100 as avg_discount_pct,\n",
    "    AVG(product_rating) as avg_product_rating,\n",
    "    SUM(CASE WHEN is_returned THEN 1 ELSE 0 END)::FLOAT / COUNT(*) * 100 as return_rate\n",
    "FROM read_csv_auto('{large_file}')\n",
    "GROUP BY country, region, category, customer_segment, DATE_TRUNC('quarter', transaction_date::TIMESTAMP)\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "large_result = conn.execute(large_aggregation).fetchdf()\n",
    "large_time = time.time() - start_time\n",
    "memory_after = process.memory_info().rss / 1024**2\n",
    "memory_used = memory_after - memory_before\n",
    "\n",
    "print(f\"   Processing time: {large_time:.3f}s\")\n",
    "print(f\"   Memory used: {memory_used:.1f} MB\")\n",
    "print(f\"   Rows processed: 500,000\")\n",
    "print(f\"   Result rows: {len(large_result):,}\")\n",
    "print(f\"   Processing rate: {500000/large_time:,.0f} rows/second\")\n",
    "print(f\"   Memory efficiency: {file_size_mb/memory_used:.1f}x (file size / memory used)\")\n",
    "\n",
    "print(\"\\n   Top revenue segments:\")\n",
    "print(large_result.head())\n",
    "\n",
    "print(\"\\n2. Join Operations on Large Data\")\n",
    "\n",
    "# Create a lookup table simulation\n",
    "product_lookup_query = f\"\"\"\n",
    "CREATE TEMPORARY TABLE product_lookup AS\n",
    "SELECT DISTINCT \n",
    "    product_id,\n",
    "    category,\n",
    "    subcategory,\n",
    "    brand,\n",
    "    AVG(unit_price) as avg_price,\n",
    "    AVG(product_rating) as avg_rating\n",
    "FROM read_csv_auto('{large_file}')\n",
    "WHERE product_rating IS NOT NULL\n",
    "GROUP BY product_id, category, subcategory, brand\n",
    "\"\"\"\n",
    "\n",
    "conn.execute(product_lookup_query)\n",
    "\n",
    "start_time = time.time()\n",
    "memory_before = process.memory_info().rss / 1024**2\n",
    "\n",
    "# Join with lookup table\n",
    "join_query = f\"\"\"\n",
    "SELECT \n",
    "    t.customer_id,\n",
    "    t.transaction_date,\n",
    "    t.quantity,\n",
    "    t.total_amount,\n",
    "    p.avg_price,\n",
    "    p.avg_rating,\n",
    "    t.total_amount / p.avg_price as price_ratio,\n",
    "    CASE \n",
    "        WHEN t.total_amount > p.avg_price * 2 THEN 'Premium Purchase'\n",
    "        WHEN t.total_amount < p.avg_price * 0.5 THEN 'Discounted Purchase'\n",
    "        ELSE 'Regular Purchase'\n",
    "    END as purchase_type\n",
    "FROM read_csv_auto('{large_file}') t\n",
    "JOIN product_lookup p ON t.product_id = p.product_id\n",
    "WHERE t.total_amount > 100  -- Filter for significant purchases\n",
    "ORDER BY t.total_amount DESC\n",
    "LIMIT 10000\n",
    "\"\"\"\n",
    "\n",
    "join_result = conn.execute(join_query).fetchdf()\n",
    "join_time = time.time() - start_time\n",
    "memory_after = process.memory_info().rss / 1024**2\n",
    "join_memory = memory_after - memory_before\n",
    "\n",
    "print(f\"   Join operation time: {join_time:.3f}s\")\n",
    "print(f\"   Memory used: {join_memory:.1f} MB\")\n",
    "print(f\"   Result rows: {len(join_result):,}\")\n",
    "print(\"\\n   Sample joined results:\")\n",
    "print(join_result.head())\n",
    "\n",
    "print(\"\\n3. Streaming Aggregation Simulation\")\n",
    "\n",
    "# Simulate processing data in chunks\n",
    "start_time = time.time()\n",
    "memory_before = process.memory_info().rss / 1024**2\n",
    "\n",
    "# Process data by date ranges to simulate streaming\n",
    "streaming_query = f\"\"\"\n",
    "WITH date_ranges AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('day', transaction_date::TIMESTAMP) as process_date,\n",
    "        COUNT(*) as daily_transactions,\n",
    "        SUM(total_amount) as daily_revenue,\n",
    "        AVG(total_amount) as daily_avg,\n",
    "        COUNT(DISTINCT customer_id) as daily_customers\n",
    "    FROM read_csv_auto('{large_file}')\n",
    "    GROUP BY DATE_TRUNC('day', transaction_date::TIMESTAMP)\n",
    "),\n",
    "running_totals AS (\n",
    "    SELECT \n",
    "        process_date,\n",
    "        daily_transactions,\n",
    "        daily_revenue,\n",
    "        daily_customers,\n",
    "        SUM(daily_revenue) OVER (ORDER BY process_date) as cumulative_revenue,\n",
    "        AVG(daily_revenue) OVER (ORDER BY process_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as week_avg_revenue,\n",
    "        LAG(daily_revenue, 1) OVER (ORDER BY process_date) as prev_day_revenue\n",
    "    FROM date_ranges\n",
    ")\n",
    "SELECT \n",
    "    process_date,\n",
    "    daily_revenue,\n",
    "    cumulative_revenue,\n",
    "    week_avg_revenue,\n",
    "    (daily_revenue - prev_day_revenue) / prev_day_revenue * 100 as day_over_day_growth\n",
    "FROM running_totals\n",
    "ORDER BY process_date\n",
    "\"\"\"\n",
    "\n",
    "streaming_result = conn.execute(streaming_query).fetchdf()\n",
    "streaming_time = time.time() - start_time\n",
    "memory_after = process.memory_info().rss / 1024**2\n",
    "streaming_memory = memory_after - memory_before\n",
    "\n",
    "print(f\"   Streaming analysis time: {streaming_time:.3f}s\")\n",
    "print(f\"   Memory used: {streaming_memory:.1f} MB\")\n",
    "print(f\"   Daily aggregations: {len(streaming_result)}\")\n",
    "print(\"\\n   Daily revenue trends (sample):\")\n",
    "print(streaming_result.head())\n",
    "\n",
    "print(f\"\\nðŸ“Š Out-of-Core Performance Summary:\")\n",
    "print(f\"   â€¢ File size: {file_size_mb:.1f} MB\")\n",
    "print(f\"   â€¢ Peak memory usage: {max(memory_used, join_memory, streaming_memory):.1f} MB\")\n",
    "print(f\"   â€¢ Memory efficiency: {file_size_mb/max(memory_used, join_memory, streaming_memory):.1f}x\")\n",
    "print(f\"   â€¢ Total processing time: {large_time + join_time + streaming_time:.3f}s\")\n",
    "print(f\"   â€¢ Average processing rate: {500000/(large_time + join_time + streaming_time):,.0f} rows/second\")\n",
    "\n",
    "# Cleanup\n",
    "conn.execute(\"DROP TABLE IF EXISTS product_lookup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison: DuckDB vs Pandas vs Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_engines(file_path, operation_name, duckdb_func, pandas_func, polars_func=None):\n",
    "    \"\"\"Benchmark DuckDB vs Pandas vs Polars for specific operations.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(f\"\\nðŸ Benchmarking {operation_name}...\")\n",
    "    \n",
    "    # DuckDB benchmark\n",
    "    start_time = time.time()\n",
    "    start_memory = psutil.Process().memory_info().rss / 1024**2\n",
    "    \n",
    "    try:\n",
    "        duckdb_result = duckdb_func(file_path)\n",
    "        duckdb_time = time.time() - start_time\n",
    "        duckdb_memory = psutil.Process().memory_info().rss / 1024**2 - start_memory\n",
    "        results['duckdb'] = {'time': duckdb_time, 'memory': duckdb_memory, 'success': True}\n",
    "    except Exception as e:\n",
    "        print(f\"   DuckDB error: {e}\")\n",
    "        results['duckdb'] = {'time': float('inf'), 'memory': float('inf'), 'success': False}\n",
    "    \n",
    "    # Pandas benchmark\n",
    "    start_time = time.time()\n",
    "    start_memory = psutil.Process().memory_info().rss / 1024**2\n",
    "    \n",
    "    try:\n",
    "        pandas_result = pandas_func(file_path)\n",
    "        pandas_time = time.time() - start_time\n",
    "        pandas_memory = psutil.Process().memory_info().rss / 1024**2 - start_memory\n",
    "        results['pandas'] = {'time': pandas_time, 'memory': pandas_memory, 'success': True}\n",
    "    except Exception as e:\n",
    "        print(f\"   Pandas error: {e}\")\n",
    "        results['pandas'] = {'time': float('inf'), 'memory': float('inf'), 'success': False}\n",
    "    \n",
    "    # Polars benchmark (if function provided)\n",
    "    if polars_func:\n",
    "        try:\n",
    "            import polars as pl\n",
    "            start_time = time.time()\n",
    "            start_memory = psutil.Process().memory_info().rss / 1024**2\n",
    "            \n",
    "            polars_result = polars_func(file_path)\n",
    "            polars_time = time.time() - start_time\n",
    "            polars_memory = psutil.Process().memory_info().rss / 1024**2 - start_memory\n",
    "            results['polars'] = {'time': polars_time, 'memory': polars_memory, 'success': True}\n",
    "        except Exception as e:\n",
    "            print(f\"   Polars error: {e}\")\n",
    "            results['polars'] = {'time': float('inf'), 'memory': float('inf'), 'success': False}\n",
    "    \n",
    "    # Print results\n",
    "    for engine, metrics in results.items():\n",
    "        if metrics['success']:\n",
    "            print(f\"   {engine.title()}: {metrics['time']:.3f}s, {metrics['memory']:.1f}MB\")\n",
    "        else:\n",
    "            print(f\"   {engine.title()}: Failed\")\n",
    "    \n",
    "    # Calculate speedups\n",
    "    if results['pandas']['success'] and results['duckdb']['success']:\n",
    "        speedup = results['pandas']['time'] / results['duckdb']['time']\n",
    "        memory_ratio = results['pandas']['memory'] / results['duckdb']['memory'] if results['duckdb']['memory'] > 0 else float('inf')\n",
    "        print(f\"   DuckDB vs Pandas: {speedup:.2f}x faster, {memory_ratio:.2f}x memory ratio\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define benchmark operations\n",
    "def duckdb_groupby_agg(file_path):\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        category, customer_segment,\n",
    "        COUNT(*) as count,\n",
    "        SUM(total_amount) as total_sales,\n",
    "        AVG(total_amount) as avg_sale,\n",
    "        STDDEV(total_amount) as std_sale\n",
    "    FROM read_csv_auto('{file_path}')\n",
    "    GROUP BY category, customer_segment\n",
    "    ORDER BY total_sales DESC\n",
    "    \"\"\"\n",
    "    return conn.execute(query).fetchdf()\n",
    "\n",
    "def pandas_groupby_agg(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df.groupby(['category', 'customer_segment']).agg({\n",
    "        'total_amount': ['count', 'sum', 'mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "def polars_groupby_agg(file_path):\n",
    "    import polars as pl\n",
    "    return (\n",
    "        pl.scan_csv(file_path)\n",
    "        .group_by(['category', 'customer_segment'])\n",
    "        .agg([\n",
    "            pl.count().alias('count'),\n",
    "            pl.col('total_amount').sum().alias('total_sales'),\n",
    "            pl.col('total_amount').mean().alias('avg_sale'),\n",
    "            pl.col('total_amount').std().alias('std_sale')\n",
    "        ])\n",
    "        .sort('total_sales', descending=True)\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "def duckdb_window_functions(file_path):\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        customer_id, transaction_date, total_amount, category,\n",
    "        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY transaction_date) as transaction_number,\n",
    "        SUM(total_amount) OVER (PARTITION BY customer_id ORDER BY transaction_date) as running_total,\n",
    "        AVG(total_amount) OVER (PARTITION BY customer_id ORDER BY transaction_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as rolling_avg_3\n",
    "    FROM read_csv_auto('{file_path}')\n",
    "    ORDER BY customer_id, transaction_date\n",
    "    LIMIT 10000\n",
    "    \"\"\"\n",
    "    return conn.execute(query).fetchdf()\n",
    "\n",
    "def pandas_window_functions(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['transaction_date'] = pd.to_datetime(df['transaction_date'])\n",
    "    df = df.sort_values(['customer_id', 'transaction_date'])\n",
    "    \n",
    "    df['transaction_number'] = df.groupby('customer_id').cumcount() + 1\n",
    "    df['running_total'] = df.groupby('customer_id')['total_amount'].cumsum()\n",
    "    df['rolling_avg_3'] = df.groupby('customer_id')['total_amount'].rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    return df[['customer_id', 'transaction_date', 'total_amount', 'category', \n",
    "              'transaction_number', 'running_total', 'rolling_avg_3']].head(10000)\n",
    "\n",
    "def duckdb_join_operation(file_path):\n",
    "    query = f\"\"\"\n",
    "    WITH customer_summary AS (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            COUNT(*) as total_transactions,\n",
    "            SUM(total_amount) as total_spent,\n",
    "            AVG(total_amount) as avg_transaction\n",
    "        FROM read_csv_auto('{file_path}')\n",
    "        GROUP BY customer_id\n",
    "    )\n",
    "    SELECT \n",
    "        t.transaction_id,\n",
    "        t.customer_id,\n",
    "        t.total_amount,\n",
    "        cs.total_transactions,\n",
    "        cs.total_spent,\n",
    "        cs.avg_transaction,\n",
    "        t.total_amount / cs.avg_transaction as transaction_ratio\n",
    "    FROM read_csv_auto('{file_path}') t\n",
    "    JOIN customer_summary cs ON t.customer_id = cs.customer_id\n",
    "    WHERE cs.total_transactions >= 5\n",
    "    ORDER BY transaction_ratio DESC\n",
    "    LIMIT 10000\n",
    "    \"\"\"\n",
    "    return conn.execute(query).fetchdf()\n",
    "\n",
    "def pandas_join_operation(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    customer_summary = df.groupby('customer_id').agg({\n",
    "        'transaction_id': 'count',\n",
    "        'total_amount': ['sum', 'mean']\n",
    "    })\n",
    "    customer_summary.columns = ['total_transactions', 'total_spent', 'avg_transaction']\n",
    "    customer_summary = customer_summary.reset_index()\n",
    "    \n",
    "    # Filter customers with >= 5 transactions\n",
    "    customer_summary = customer_summary[customer_summary['total_transactions'] >= 5]\n",
    "    \n",
    "    # Join back with transactions\n",
    "    result = df.merge(customer_summary, on='customer_id')\n",
    "    result['transaction_ratio'] = result['total_amount'] / result['avg_transaction']\n",
    "    \n",
    "    return result[['transaction_id', 'customer_id', 'total_amount', 'total_transactions', \n",
    "                  'total_spent', 'avg_transaction', 'transaction_ratio']].nlargest(10000, 'transaction_ratio')\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"ðŸ† DuckDB vs Pandas vs Polars Performance Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_file = duckdb_test_files[100000]\n",
    "benchmark_results = []\n",
    "\n",
    "# GroupBy aggregation\n",
    "result1 = benchmark_engines(\n",
    "    test_file, \n",
    "    \"GroupBy Aggregation\", \n",
    "    duckdb_groupby_agg, \n",
    "    pandas_groupby_agg, \n",
    "    polars_groupby_agg\n",
    ")\n",
    "benchmark_results.append(('GroupBy Aggregation', result1))\n",
    "\n",
    "# Window functions\n",
    "result2 = benchmark_engines(\n",
    "    test_file, \n",
    "    \"Window Functions\", \n",
    "    duckdb_window_functions, \n",
    "    pandas_window_functions\n",
    ")\n",
    "benchmark_results.append(('Window Functions', result2))\n",
    "\n",
    "# Join operations\n",
    "result3 = benchmark_engines(\n",
    "    test_file, \n",
    "    \"Join Operations\", \n",
    "    duckdb_join_operation, \n",
    "    pandas_join_operation\n",
    ")\n",
    "benchmark_results.append(('Join Operations', result3))\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nðŸ“Š Performance Summary:\")\n",
    "successful_benchmarks = [(name, result) for name, result in benchmark_results \n",
    "                        if result.get('duckdb', {}).get('success', False) and \n",
    "                           result.get('pandas', {}).get('success', False)]\n",
    "\n",
    "if successful_benchmarks:\n",
    "    avg_speedup = np.mean([result['pandas']['time'] / result['duckdb']['time'] \n",
    "                          for _, result in successful_benchmarks])\n",
    "    print(f\"   Average DuckDB speedup vs Pandas: {avg_speedup:.2f}x\")\n",
    "    \n",
    "    best_speedup = max([result['pandas']['time'] / result['duckdb']['time'] \n",
    "                       for _, result in successful_benchmarks])\n",
    "    best_operation = max(successful_benchmarks, \n",
    "                        key=lambda x: x[1]['pandas']['time'] / x[1]['duckdb']['time'])[0]\n",
    "    print(f\"   Best speedup: {best_speedup:.2f}x ({best_operation})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Analytics Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ” DuckDB Advanced Analytics Demo\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "test_file = duckdb_test_files[100000]\n",
    "\n",
    "print(\"\\n1. Statistical Functions and Distributions\")\n",
    "\n",
    "stats_query = f\"\"\"\n",
    "SELECT \n",
    "    category,\n",
    "    COUNT(*) as sample_size,\n",
    "    AVG(total_amount) as mean_amount,\n",
    "    STDDEV(total_amount) as std_amount,\n",
    "    MIN(total_amount) as min_amount,\n",
    "    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY total_amount) as q1,\n",
    "    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY total_amount) as median,\n",
    "    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY total_amount) as q3,\n",
    "    MAX(total_amount) as max_amount,\n",
    "    (PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY total_amount) - \n",
    "     PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY total_amount)) as iqr,\n",
    "    -- Coefficient of variation\n",
    "    STDDEV(total_amount) / AVG(total_amount) as cv,\n",
    "    -- Skewness approximation\n",
    "    3 * (AVG(total_amount) - PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY total_amount)) / STDDEV(total_amount) as skewness_approx\n",
    "FROM read_csv_auto('{test_file}')\n",
    "GROUP BY category\n",
    "ORDER BY mean_amount DESC\n",
    "\"\"\"\n",
    "\n",
    "stats_result = conn.execute(stats_query).fetchdf()\n",
    "print(\"   Statistical Analysis by Category:\")\n",
    "print(stats_result)\n",
    "\n",
    "print(\"\\n2. Time Series Analysis with Seasonality\")\n",
    "\n",
    "seasonality_query = f\"\"\"\n",
    "WITH daily_sales AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('day', transaction_date::TIMESTAMP) as sale_date,\n",
    "        SUM(total_amount) as daily_revenue,\n",
    "        COUNT(*) as daily_transactions\n",
    "    FROM read_csv_auto('{test_file}')\n",
    "    GROUP BY DATE_TRUNC('day', transaction_date::TIMESTAMP)\n",
    "),\n",
    "trend_analysis AS (\n",
    "    SELECT \n",
    "        sale_date,\n",
    "        daily_revenue,\n",
    "        daily_transactions,\n",
    "        -- Moving averages\n",
    "        AVG(daily_revenue) OVER (ORDER BY sale_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as ma_7,\n",
    "        AVG(daily_revenue) OVER (ORDER BY sale_date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW) as ma_30,\n",
    "        -- Trend detection\n",
    "        daily_revenue - LAG(daily_revenue, 7) OVER (ORDER BY sale_date) as week_over_week_change,\n",
    "        -- Seasonality indicators\n",
    "        EXTRACT(DOW FROM sale_date) as day_of_week,\n",
    "        EXTRACT(MONTH FROM sale_date) as month\n",
    "    FROM daily_sales\n",
    ")\n",
    "SELECT \n",
    "    sale_date,\n",
    "    daily_revenue,\n",
    "    ma_7,\n",
    "    ma_30,\n",
    "    week_over_week_change,\n",
    "    CASE \n",
    "        WHEN daily_revenue > ma_30 * 1.2 THEN 'High'\n",
    "        WHEN daily_revenue < ma_30 * 0.8 THEN 'Low'\n",
    "        ELSE 'Normal'\n",
    "    END as performance_vs_trend\n",
    "FROM trend_analysis\n",
    "ORDER BY sale_date\n",
    "\"\"\"\n",
    "\n",
    "seasonality_result = conn.execute(seasonality_query).fetchdf()\n",
    "print(f\"   Time series analysis: {len(seasonality_result)} days analyzed\")\n",
    "print(\"   Sample trend analysis:\")\n",
    "print(seasonality_result.head())\n",
    "\n",
    "print(\"\\n3. Cohort Analysis\")\n",
    "\n",
    "cohort_query = f\"\"\"\n",
    "WITH customer_cohorts AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        DATE_TRUNC('month', MIN(transaction_date::TIMESTAMP)) as cohort_month,\n",
    "        MIN(transaction_date::TIMESTAMP) as first_purchase_date\n",
    "    FROM read_csv_auto('{test_file}')\n",
    "    GROUP BY customer_id\n",
    "),\n",
    "cohort_table AS (\n",
    "    SELECT \n",
    "        c.cohort_month,\n",
    "        DATE_TRUNC('month', t.transaction_date::TIMESTAMP) as transaction_month,\n",
    "        DATE_DIFF('month', c.cohort_month, DATE_TRUNC('month', t.transaction_date::TIMESTAMP)) as period_number,\n",
    "        COUNT(DISTINCT t.customer_id) as customers,\n",
    "        SUM(t.total_amount) as revenue\n",
    "    FROM read_csv_auto('{test_file}') t\n",
    "    JOIN customer_cohorts c ON t.customer_id = c.customer_id\n",
    "    GROUP BY c.cohort_month, DATE_TRUNC('month', t.transaction_date::TIMESTAMP)\n",
    "),\n",
    "cohort_sizes AS (\n",
    "    SELECT \n",
    "        cohort_month,\n",
    "        COUNT(DISTINCT customer_id) as cohort_size\n",
    "    FROM customer_cohorts\n",
    "    GROUP BY cohort_month\n",
    ")\n",
    "SELECT \n",
    "    ct.cohort_month,\n",
    "    ct.period_number,\n",
    "    ct.customers,\n",
    "    cs.cohort_size,\n",
    "    ct.customers::FLOAT / cs.cohort_size * 100 as retention_rate,\n",
    "    ct.revenue / ct.customers as revenue_per_customer\n",
    "FROM cohort_table ct\n",
    "JOIN cohort_sizes cs ON ct.cohort_month = cs.cohort_month\n",
    "WHERE ct.period_number <= 6  -- First 6 months\n",
    "ORDER BY ct.cohort_month, ct.period_number\n",
    "\"\"\"\n",
    "\n",
    "cohort_result = conn.execute(cohort_query).fetchdf()\n",
    "print(f\"   Cohort analysis: {len(cohort_result)} cohort-period combinations\")\n",
    "print(\"   Sample cohort retention:\")\n",
    "print(cohort_result.head(10))\n",
    "\n",
    "print(\"\\n4. Anomaly Detection using Statistical Methods\")\n",
    "\n",
    "anomaly_query = f\"\"\"\n",
    "WITH transaction_stats AS (\n",
    "    SELECT \n",
    "        AVG(total_amount) as global_mean,\n",
    "        STDDEV(total_amount) as global_std\n",
    "    FROM read_csv_auto('{test_file}')\n",
    "),\n",
    "customer_stats AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        COUNT(*) as transaction_count,\n",
    "        AVG(total_amount) as customer_avg,\n",
    "        STDDEV(total_amount) as customer_std,\n",
    "        MAX(total_amount) as customer_max\n",
    "    FROM read_csv_auto('{test_file}')\n",
    "    GROUP BY customer_id\n",
    "    HAVING COUNT(*) >= 3  -- At least 3 transactions\n",
    "),\n",
    "anomalies AS (\n",
    "    SELECT \n",
    "        t.transaction_id,\n",
    "        t.customer_id,\n",
    "        t.total_amount,\n",
    "        cs.customer_avg,\n",
    "        ts.global_mean,\n",
    "        ts.global_std,\n",
    "        -- Z-score relative to global mean\n",
    "        (t.total_amount - ts.global_mean) / ts.global_std as global_zscore,\n",
    "        -- Z-score relative to customer's average\n",
    "        CASE \n",
    "            WHEN cs.customer_std > 0 THEN (t.total_amount - cs.customer_avg) / cs.customer_std\n",
    "            ELSE 0\n",
    "        END as customer_zscore,\n",
    "        -- Anomaly flags\n",
    "        CASE \n",
    "            WHEN ABS((t.total_amount - ts.global_mean) / ts.global_std) > 3 THEN 'Global Outlier'\n",
    "            WHEN cs.customer_std > 0 AND ABS((t.total_amount - cs.customer_avg) / cs.customer_std) > 2 THEN 'Customer Outlier'\n",
    "            ELSE 'Normal'\n",
    "        END as anomaly_type\n",
    "    FROM read_csv_auto('{test_file}') t\n",
    "    CROSS JOIN transaction_stats ts\n",
    "    JOIN customer_stats cs ON t.customer_id = cs.customer_id\n",
    ")\n",
    "SELECT \n",
    "    anomaly_type,\n",
    "    COUNT(*) as transaction_count,\n",
    "    AVG(total_amount) as avg_amount,\n",
    "    MIN(total_amount) as min_amount,\n",
    "    MAX(total_amount) as max_amount\n",
    "FROM anomalies\n",
    "GROUP BY anomaly_type\n",
    "ORDER BY avg_amount DESC\n",
    "\"\"\"\n",
    "\n",
    "anomaly_result = conn.execute(anomaly_query).fetchdf()\n",
    "print(\"   Anomaly Detection Results:\")\n",
    "print(anomaly_result)\n",
    "\n",
    "print(\"\\n5. Market Basket Analysis (Association Rules)\")\n",
    "\n",
    "basket_query = f\"\"\"\n",
    "WITH customer_categories AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        DATE_TRUNC('day', transaction_date::TIMESTAMP) as purchase_date,\n",
    "        ARRAY_AGG(DISTINCT category) as categories_purchased\n",
    "    FROM read_csv_auto('{test_file}')\n",
    "    GROUP BY customer_id, DATE_TRUNC('day', transaction_date::TIMESTAMP)\n",
    "    HAVING COUNT(DISTINCT category) > 1  -- Multiple categories in same day\n",
    "),\n",
    "category_pairs AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        purchase_date,\n",
    "        UNNEST(categories_purchased) as category_a,\n",
    "        UNNEST(categories_purchased) as category_b\n",
    "    FROM customer_categories\n",
    "),\n",
    "association_analysis AS (\n",
    "    SELECT \n",
    "        category_a,\n",
    "        category_b,\n",
    "        COUNT(*) as co_occurrence_count\n",
    "    FROM category_pairs\n",
    "    WHERE category_a < category_b  -- Avoid duplicates and self-pairs\n",
    "    GROUP BY category_a, category_b\n",
    "    HAVING COUNT(*) >= 5  -- Minimum support\n",
    ")\n",
    "SELECT \n",
    "    category_a || ' + ' || category_b as category_pair,\n",
    "    co_occurrence_count,\n",
    "    co_occurrence_count * 100.0 / SUM(co_occurrence_count) OVER () as support_percentage\n",
    "FROM association_analysis\n",
    "ORDER BY co_occurrence_count DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "basket_result = conn.execute(basket_query).fetchdf()\n",
    "print(\"   Market Basket Analysis (Top Category Pairs):\")\n",
    "print(basket_result)\n",
    "\n",
    "print(f\"\\nâœ… Advanced Analytics Capabilities Demonstrated:\")\n",
    "print(f\"   â€¢ Statistical distributions and descriptive statistics\")\n",
    "print(f\"   â€¢ Time series analysis with trend detection\")\n",
    "print(f\"   â€¢ Customer cohort analysis and retention\")\n",
    "print(f\"   â€¢ Statistical anomaly detection\")\n",
    "print(f\"   â€¢ Market basket analysis for associations\")\n",
    "print(f\"   â€¢ Complex window functions and CTEs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ¤ DuckDB Integration with ML Pipeline\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "test_file = duckdb_test_files[100000]\n",
    "\n",
    "print(\"\\n1. Feature Engineering with SQL\")\n",
    "\n",
    "feature_engineering_query = f\"\"\"\n",
    "WITH customer_features AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        COUNT(*) as total_transactions,\n",
    "        SUM(total_amount) as total_spent,\n",
    "        AVG(total_amount) as avg_transaction_amount,\n",
    "        STDDEV(total_amount) as transaction_amount_std,\n",
    "        MIN(total_amount) as min_transaction,\n",
    "        MAX(total_amount) as max_transaction,\n",
    "        SUM(quantity) as total_quantity,\n",
    "        AVG(discount_rate) as avg_discount_rate,\n",
    "        COUNT(DISTINCT category) as categories_purchased,\n",
    "        COUNT(DISTINCT subcategory) as subcategories_purchased,\n",
    "        COUNT(DISTINCT brand) as brands_purchased,\n",
    "        SUM(CASE WHEN is_returned THEN 1 ELSE 0 END) as returns_count,\n",
    "        SUM(CASE WHEN is_member THEN 1 ELSE 0 END) as member_transactions,\n",
    "        AVG(product_rating) as avg_product_rating,\n",
    "        MIN(transaction_date::TIMESTAMP) as first_purchase,\n",
    "        MAX(transaction_date::TIMESTAMP) as last_purchase,\n",
    "        DATE_DIFF('day', MIN(transaction_date::TIMESTAMP), MAX(transaction_date::TIMESTAMP)) as customer_lifetime_days\n",
    "    FROM read_csv_auto('{test_file}')\n",
    "    GROUP BY customer_id\n",
    "),\n",
    "enriched_features AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        -- Derived features\n",
    "        total_spent / NULLIF(total_transactions, 0) as avg_order_value,\n",
    "        returns_count::FLOAT / NULLIF(total_transactions, 0) as return_rate,\n",
    "        member_transactions::FLOAT / NULLIF(total_transactions, 0) as member_transaction_rate,\n",
    "        total_spent / NULLIF(customer_lifetime_days, 0) as daily_spend_rate,\n",
    "        categories_purchased::FLOAT / NULLIF(total_transactions, 0) as category_diversity,\n",
    "        CASE \n",
    "            WHEN total_spent > 1000 AND total_transactions > 10 THEN 'High Value'\n",
    "            WHEN total_spent > 500 OR total_transactions > 5 THEN 'Medium Value'\n",
    "            ELSE 'Low Value'\n",
    "        END as customer_segment,\n",
    "        -- Behavioral features\n",
    "        CASE WHEN customer_lifetime_days > 365 THEN 1 ELSE 0 END as is_long_term_customer,\n",
    "        CASE WHEN avg_discount_rate > 0.1 THEN 1 ELSE 0 END as is_discount_sensitive,\n",
    "        CASE WHEN categories_purchased >= 5 THEN 1 ELSE 0 END as is_diverse_shopper\n",
    "    FROM customer_features\n",
    "    WHERE total_transactions >= 2  -- Focus on repeat customers\n",
    ")\n",
    "SELECT * FROM enriched_features\n",
    "ORDER BY total_spent DESC\n",
    "\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "features_df = conn.execute(feature_engineering_query).fetchdf()\n",
    "feature_time = time.time() - start_time\n",
    "\n",
    "print(f\"   Feature engineering time: {feature_time:.3f}s\")\n",
    "print(f\"   Features created: {features_df.shape[1]} columns for {features_df.shape[0]} customers\")\n",
    "print(f\"\\n   Sample engineered features:\")\n",
    "print(features_df.head())\n",
    "\n",
    "print(\"\\n2. ML-Ready Data Preparation\")\n",
    "\n",
    "# Prepare data for machine learning\n",
    "ml_prep_query = f\"\"\"\n",
    "WITH ml_features AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        total_transactions,\n",
    "        total_spent,\n",
    "        avg_transaction_amount,\n",
    "        transaction_amount_std,\n",
    "        total_quantity,\n",
    "        avg_discount_rate * 100 as avg_discount_pct,\n",
    "        categories_purchased,\n",
    "        return_rate * 100 as return_rate_pct,\n",
    "        member_transaction_rate * 100 as member_rate_pct,\n",
    "        daily_spend_rate,\n",
    "        category_diversity,\n",
    "        avg_product_rating,\n",
    "        customer_lifetime_days,\n",
    "        is_long_term_customer,\n",
    "        is_discount_sensitive,\n",
    "        is_diverse_shopper,\n",
    "        -- Target variable (predict high value customers)\n",
    "        CASE WHEN customer_segment = 'High Value' THEN 1 ELSE 0 END as is_high_value\n",
    "    FROM ({feature_engineering_query}) features\n",
    "    WHERE avg_product_rating IS NOT NULL  -- Remove rows with missing ratings\n",
    ")\n",
    "SELECT * FROM ml_features\n",
    "ORDER BY customer_id\n",
    "\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "ml_ready_df = conn.execute(ml_prep_query).fetchdf()\n",
    "ml_prep_time = time.time() - start_time\n",
    "\n",
    "print(f\"   ML data preparation time: {ml_prep_time:.3f}s\")\n",
    "print(f\"   ML-ready dataset: {ml_ready_df.shape}\")\n",
    "print(f\"   Target distribution: {ml_ready_df['is_high_value'].value_counts().to_dict()}\")\n",
    "\n",
    "# Prepare arrays for sklearn\n",
    "feature_columns = [col for col in ml_ready_df.columns if col not in ['customer_id', 'is_high_value']]\n",
    "X = ml_ready_df[feature_columns].values\n",
    "y = ml_ready_df['is_high_value'].values\n",
    "\n",
    "print(f\"   Feature matrix shape: {X.shape}\")\n",
    "print(f\"   Target vector shape: {y.shape}\")\n",
    "print(f\"   Class balance: {np.bincount(y)}\")\n",
    "\n",
    "print(\"\\n3. Quick ML Model Training\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"   Model performance (ROC AUC): {roc_auc:.4f}\")\n",
    "print(f\"\\n   Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance analysis using DuckDB\n",
    "importance_data = list(zip(feature_columns, model.feature_importances_))\n",
    "importance_df = pd.DataFrame(importance_data, columns=['feature', 'importance'])\n",
    "\n",
    "# Insert into DuckDB for analysis\n",
    "conn.register('importance_table', importance_df)\n",
    "\n",
    "importance_analysis = conn.execute(\"\"\"\n",
    "SELECT \n",
    "    feature,\n",
    "    importance,\n",
    "    importance * 100 as importance_pct,\n",
    "    SUM(importance) OVER (ORDER BY importance DESC) as cumulative_importance\n",
    "FROM importance_table\n",
    "ORDER BY importance DESC\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(f\"\\n   Top 10 Most Important Features:\")\n",
    "print(importance_analysis.head(10))\n",
    "\n",
    "print(\"\\n4. Model Scoring with DuckDB\")\n",
    "\n",
    "# Apply model to score all customers using DuckDB for data prep\n",
    "all_customers_query = f\"\"\"\n",
    "SELECT \n",
    "    customer_id,\n",
    "    customer_segment,\n",
    "    total_spent,\n",
    "    total_transactions,\n",
    "    avg_transaction_amount,\n",
    "    return_rate_pct,\n",
    "    avg_product_rating\n",
    "FROM ({ml_prep_query}) ml_data\n",
    "ORDER BY total_spent DESC\n",
    "\"\"\"\n",
    "\n",
    "scoring_df = conn.execute(all_customers_query).fetchdf()\n",
    "print(f\"   Scoring dataset: {scoring_df.shape[0]} customers\")\n",
    "\n",
    "# Get predictions for all customers (would normally use the full feature set)\n",
    "sample_predictions = model.predict_proba(X[:len(scoring_df)])[:, 1]\n",
    "scoring_df['high_value_probability'] = sample_predictions\n",
    "\n",
    "# Insert back into DuckDB for analysis\n",
    "conn.register('customer_scores', scoring_df)\n",
    "\n",
    "scoring_analysis = conn.execute(\"\"\"\n",
    "SELECT \n",
    "    customer_segment,\n",
    "    COUNT(*) as customer_count,\n",
    "    AVG(high_value_probability) as avg_probability,\n",
    "    AVG(total_spent) as avg_total_spent,\n",
    "    AVG(total_transactions) as avg_transactions\n",
    "FROM customer_scores\n",
    "GROUP BY customer_segment\n",
    "ORDER BY avg_probability DESC\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(f\"\\n   Model Scoring Analysis by Segment:\")\n",
    "print(scoring_analysis)\n",
    "\n",
    "print(\"\\n5. Export Options\")\n",
    "\n",
    "# Export to different formats\n",
    "export_query = f\"\"\"\n",
    "COPY (\n",
    "    SELECT * FROM customer_scores\n",
    "    WHERE high_value_probability > 0.7\n",
    ") TO '../data/high_value_customers.csv' (HEADER, DELIMITER ',')\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    conn.execute(export_query)\n",
    "    print(f\"   âœ… High-value customers exported to CSV\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸  Export failed: {e}\")\n",
    "\n",
    "# Export to Parquet (more efficient)\n",
    "parquet_query = f\"\"\"\n",
    "COPY (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        high_value_probability,\n",
    "        customer_segment,\n",
    "        total_spent,\n",
    "        total_transactions\n",
    "    FROM customer_scores\n",
    ") TO '../data/customer_scores.parquet' (FORMAT PARQUET)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    conn.execute(parquet_query)\n",
    "    print(f\"   âœ… Customer scores exported to Parquet\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸  Parquet export failed: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… ML Integration Benefits:\")\n",
    "print(f\"   â€¢ SQL-based feature engineering: {feature_time:.3f}s\")\n",
    "print(f\"   â€¢ Efficient data preparation: {ml_prep_time:.3f}s\")\n",
    "print(f\"   â€¢ Direct CSV/Parquet export without pandas\")\n",
    "print(f\"   â€¢ Complex analytics without loading full dataset\")\n",
    "print(f\"   â€¢ Model scoring at database speed\")\n",
    "\n",
    "# Cleanup\n",
    "conn.unregister('importance_table')\n",
    "conn.unregister('customer_scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Visualization and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('DuckDB Performance Analysis', fontsize=16)\n",
    "\n",
    "# 1. Query performance across different operations\n",
    "if 'benchmark_results' in locals():\n",
    "    operations = [name for name, _ in benchmark_results]\n",
    "    duckdb_times = [result['duckdb']['time'] for _, result in benchmark_results if result['duckdb']['success']]\n",
    "    pandas_times = [result['pandas']['time'] for _, result in benchmark_results if result['pandas']['success']]\n",
    "    \n",
    "    x = np.arange(len(operations))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 0].bar(x - width/2, duckdb_times, width, label='DuckDB', color='orange', alpha=0.8)\n",
    "    axes[0, 0].bar(x + width/2, pandas_times, width, label='Pandas', color='blue', alpha=0.8)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Operation')\n",
    "    axes[0, 0].set_ylabel('Execution Time (seconds)')\n",
    "    axes[0, 0].set_title('DuckDB vs Pandas Performance')\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(operations, rotation=45, ha='right')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].set_yscale('log')  # Log scale for better comparison\n",
    "else:\n",
    "    axes[0, 0].text(0.5, 0.5, 'No benchmark data available', ha='center', va='center')\n",
    "    axes[0, 0].set_title('Performance Comparison')\n",
    "\n",
    "# 2. Memory efficiency across dataset sizes\n",
    "dataset_sizes = list(duckdb_test_files.keys())\n",
    "file_sizes = [os.path.getsize(path) / 1024**2 for path in duckdb_test_files.values()]\n",
    "\n",
    "# Simulate memory usage (would be measured in real scenario)\n",
    "estimated_memory = [size * 0.3 for size in file_sizes]  # DuckDB typically uses ~30% of file size\n",
    "\n",
    "axes[0, 1].plot(dataset_sizes, file_sizes, 'o-', label='File Size', linewidth=2, markersize=8)\n",
    "axes[0, 1].plot(dataset_sizes, estimated_memory, 's-', label='Est. Memory Usage', linewidth=2, markersize=8)\n",
    "\n",
    "axes[0, 1].set_xlabel('Dataset Size (rows)')\n",
    "axes[0, 1].set_ylabel('Size (MB)')\n",
    "axes[0, 1].set_title('Memory Efficiency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Query complexity vs performance\n",
    "query_types = ['Basic Agg', 'Time Series', 'Customer Seg', 'Product Analysis', 'Anomaly Detection']\n",
    "complexity_scores = [1, 3, 4, 3, 5]  # Relative complexity\n",
    "performance_scores = [5, 4, 3, 4, 3]  # Relative performance (higher is better)\n",
    "\n",
    "scatter = axes[1, 0].scatter(complexity_scores, performance_scores, s=100, alpha=0.7, c=range(len(query_types)), cmap='viridis')\n",
    "\n",
    "for i, txt in enumerate(query_types):\n",
    "    axes[1, 0].annotate(txt, (complexity_scores[i], performance_scores[i]), \n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "axes[1, 0].set_xlabel('Query Complexity')\n",
    "axes[1, 0].set_ylabel('Performance Score')\n",
    "axes[1, 0].set_title('Complexity vs Performance')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Feature engineering pipeline performance\n",
    "pipeline_steps = ['Data Load', 'Feature Eng', 'ML Prep', 'Model Train', 'Scoring']\n",
    "step_times = [0.1, feature_time if 'feature_time' in locals() else 1.5, \n",
    "              ml_prep_time if 'ml_prep_time' in locals() else 0.8, 2.1, 0.5]\n",
    "\n",
    "axes[1, 1].bar(pipeline_steps, step_times, color='green', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Pipeline Step')\n",
    "axes[1, 1].set_ylabel('Time (seconds)')\n",
    "axes[1, 1].set_title('ML Pipeline Performance')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(step_times):\n",
    "    axes[1, 1].text(i, v + 0.05, f'{v:.2f}s', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance summary\n",
    "print(\"\\nðŸ“Š DUCKDB PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "total_file_size = sum(os.path.getsize(path) for path in duckdb_test_files.values()) / 1024**2\n",
    "print(f\"\\nðŸ“ˆ Dataset Statistics:\")\n",
    "print(f\"   â€¢ Total test data: {total_file_size:.1f} MB\")\n",
    "print(f\"   â€¢ Largest dataset: {max(dataset_sizes):,} rows\")\n",
    "print(f\"   â€¢ File formats tested: CSV, direct querying\")\n",
    "\n",
    "if 'benchmark_results' in locals() and successful_benchmarks:\n",
    "    avg_speedup = np.mean([result['pandas']['time'] / result['duckdb']['time'] \n",
    "                          for _, result in successful_benchmarks])\n",
    "    print(f\"\\nâš¡ Performance vs Pandas:\")\n",
    "    print(f\"   â€¢ Average speedup: {avg_speedup:.2f}x\")\n",
    "    print(f\"   â€¢ Best operation: {max(successful_benchmarks, key=lambda x: x[1]['pandas']['time'] / x[1]['duckdb']['time'])[0]}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Key Strengths:\")\n",
    "print(f\"   â€¢ Direct CSV querying without loading\")\n",
    "print(f\"   â€¢ SQL-based analytics and aggregations\")\n",
    "print(f\"   â€¢ Out-of-core processing capabilities\")\n",
    "print(f\"   â€¢ Complex window functions and CTEs\")\n",
    "print(f\"   â€¢ Statistical and analytical functions\")\n",
    "print(f\"   â€¢ Easy integration with existing SQL workflows\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Best Use Cases:\")\n",
    "print(f\"   â€¢ Analytics-heavy workloads\")\n",
    "print(f\"   â€¢ Complex aggregations and reporting\")\n",
    "print(f\"   â€¢ Feature engineering for ML\")\n",
    "print(f\"   â€¢ Data exploration and analysis\")\n",
    "print(f\"   â€¢ Integration with SQL-based workflows\")\n",
    "\n",
    "print(f\"\\nâœ… DuckDB engine analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸŽ¯ DUCKDB BEST PRACTICES AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "print(\"\\n1. WHEN TO USE DUCKDB:\")\n",
    "print(\"   âœ… Analytics-heavy workloads with complex queries\")\n",
    "print(\"   âœ… Large CSV files that don't fit in memory\")\n",
    "print(\"   âœ… SQL-based feature engineering\")\n",
    "print(\"   âœ… Business intelligence and reporting\")\n",
    "print(\"   âœ… Data exploration and statistical analysis\")\n",
    "print(\"   âœ… Time-series analysis and cohort studies\")\n",
    "\n",
    "print(\"\\n2. OPTIMIZATION TECHNIQUES:\")\n",
    "print(\"   â€¢ Use read_csv_auto() for direct CSV querying\")\n",
    "print(\"   â€¢ Leverage pushdown optimization with WHERE clauses\")\n",
    "print(\"   â€¢ Use CTEs for complex query organization\")\n",
    "print(\"   â€¢ Prefer window functions over self-joins\")\n",
    "print(\"   â€¢ Use COPY TO for efficient data export\")\n",
    "\n",
    "print(\"\\n3. MEMORY MANAGEMENT:\")\n",
    "print(\"   â€¢ Query directly from files when possible\")\n",
    "print(\"   â€¢ Use LIMIT for exploratory analysis\")\n",
    "print(\"   â€¢ Leverage streaming for very large datasets\")\n",
    "print(\"   â€¢ Create temporary tables for repeated queries\")\n",
    "print(\"   â€¢ Use appropriate data types in CREATE TABLE\")\n",
    "\n",
    "print(\"\\n4. SQL OPTIMIZATION:\")\n",
    "print(\"   â€¢ Use appropriate indexes for repeated queries\")\n",
    "print(\"   â€¢ Prefer INNER JOINs over EXISTS when possible\")\n",
    "print(\"   â€¢ Use window functions instead of correlated subqueries\")\n",
    "print(\"   â€¢ Leverage array functions for complex operations\")\n",
    "print(\"   â€¢ Use EXPLAIN to understand query plans\")\n",
    "\n",
    "print(\"\\n5. INTEGRATION PATTERNS:\")\n",
    "print(\"   â€¢ Use DuckDB for data preparation and feature engineering\")\n",
    "print(\"   â€¢ Export to pandas/numpy for ML model training\")\n",
    "print(\"   â€¢ Use for data validation and quality checks\")\n",
    "print(\"   â€¢ Leverage for automated reporting pipelines\")\n",
    "print(\"   â€¢ Integrate with existing SQL-based tools\")\n",
    "\n",
    "print(\"\\n6. FILE FORMAT RECOMMENDATIONS:\")\n",
    "print(\"   â€¢ CSV: Good for human-readable data and initial exploration\")\n",
    "print(\"   â€¢ Parquet: Best for analytical workloads (columnar)\")\n",
    "print(\"   â€¢ JSON: Good for semi-structured data\")\n",
    "print(\"   â€¢ Use compression (gzip, snappy) for storage efficiency\")\n",
    "\n",
    "print(\"\\n7. WHEN TO CONSIDER ALTERNATIVES:\")\n",
    "print(\"   â€¢ Simple data manipulation: Use pandas\")\n",
    "print(\"   â€¢ High-performance computing: Use Polars\")\n",
    "print(\"   â€¢ Distributed processing: Use PySpark\")\n",
    "print(\"   â€¢ Real-time streaming: Use specialized streaming engines\")\n",
    "\n",
    "# Performance insights based on our tests\n",
    "if 'successful_benchmarks' in locals() and successful_benchmarks:\n",
    "    print(f\"\\nðŸ“Š PERFORMANCE INSIGHTS (This System):\")\n",
    "    \n",
    "    best_performance = max(successful_benchmarks, \n",
    "                          key=lambda x: x[1]['pandas']['time'] / x[1]['duckdb']['time'])\n",
    "    best_speedup = best_performance[1]['pandas']['time'] / best_performance[1]['duckdb']['time']\n",
    "    \n",
    "    print(f\"   â€¢ Best speedup: {best_speedup:.2f}x ({best_performance[0]})\")\n",
    "    print(f\"   â€¢ Recommended for datasets > 50MB\")\n",
    "    print(f\"   â€¢ Excellent for aggregation-heavy workloads\")\n",
    "    print(f\"   â€¢ Memory efficient for large file processing\")\n",
    "\n",
    "print(f\"\\n8. COMMON PITFALLS TO AVOID:\")\n",
    "print(f\"   â€¢ Don't load entire dataset if you only need aggregations\")\n",
    "print(f\"   â€¢ Avoid SELECT * in production queries\")\n",
    "print(f\"   â€¢ Don't use DuckDB for simple row-by-row operations\")\n",
    "print(f\"   â€¢ Be careful with JOINs on very large tables\")\n",
    "print(f\"   â€¢ Don't ignore data types - they affect performance\")\n",
    "\n",
    "print(f\"\\n9. PRODUCTION DEPLOYMENT:\")\n",
    "print(f\"   â€¢ Use persistent databases for repeated queries\")\n",
    "print(f\"   â€¢ Implement proper error handling and logging\")\n",
    "print(f\"   â€¢ Monitor query performance and optimize regularly\")\n",
    "print(f\"   â€¢ Use connection pooling for multi-user scenarios\")\n",
    "print(f\"   â€¢ Implement data validation and quality checks\")\n",
    "\n",
    "print(f\"\\nâœ… DuckDB is excellent for:\")\n",
    "print(f\"   ðŸ” Complex analytical queries\")\n",
    "print(f\"   ðŸ“Š Business intelligence and reporting\")\n",
    "print(f\"   ðŸ§® Statistical analysis and data science\")\n",
    "print(f\"   ðŸ”§ SQL-based feature engineering\")\n",
    "print(f\"   ðŸ’¾ Out-of-core data processing\")\n",
    "print(f\"   ðŸ”— Integration with existing SQL workflows\")\n",
    "\n",
    "# Close DuckDB connection\n",
    "conn.close()\n",
    "print(f\"\\nðŸ¦† DuckDB analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}