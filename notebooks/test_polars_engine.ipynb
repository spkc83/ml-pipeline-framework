{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polars Engine Testing and Analysis\n",
    "\n",
    "This notebook demonstrates Polars capabilities, lazy evaluation benefits, and performance comparisons with pandas for the ML Pipeline Framework.\n",
    "\n",
    "## Topics Covered:\n",
    "- âš¡ Lazy evaluation with Polars\n",
    "- ðŸš€ Parallel processing capabilities\n",
    "- ðŸ’¾ Memory efficiency vs pandas\n",
    "- ðŸ”§ Complex aggregations and window functions\n",
    "- ðŸŒŠ Streaming large datasets\n",
    "- ðŸ¤ Integration with ML pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"System CPU cores: {psutil.cpu_count()}\")\n",
    "print(f\"System memory: {psutil.virtual_memory().total / 1024**3:.1f} GB\")\n",
    "print(f\"Available memory: {psutil.virtual_memory().available / 1024**3:.1f} GB\")\n",
    "\n",
    "# Set Polars configuration for better performance\n",
    "pl.Config.set_streaming_chunk_size(25000)\n",
    "pl.Config.set_fmt_str_lengths(50)\n",
    "\n",
    "print(f\"\\nPolars configuration:\")\n",
    "print(f\"  Streaming chunk size: {pl.Config.streaming_chunk_size()}\")\n",
    "print(f\"  String display length: {pl.Config.fmt_str_lengths()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_polars_test_data(n_rows=100000, save_path=None):\n",
    "    \"\"\"Generate test data optimized for Polars testing.\"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate data with various data types\n",
    "    data = {\n",
    "        # Integer columns\n",
    "        'id': range(n_rows),\n",
    "        'customer_id': np.random.randint(1, n_rows//10, n_rows),\n",
    "        'product_id': np.random.randint(1, 1000, n_rows),\n",
    "        'quantity': np.random.randint(1, 10, n_rows),\n",
    "        \n",
    "        # Float columns\n",
    "        'price': np.random.lognormal(3, 1, n_rows),\n",
    "        'discount': np.random.beta(2, 8, n_rows),\n",
    "        'rating': np.random.normal(4.0, 1.0, n_rows),\n",
    "        \n",
    "        # String columns\n",
    "        'category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home', 'Sports'], n_rows),\n",
    "        'region': np.random.choice(['North', 'South', 'East', 'West'], n_rows),\n",
    "        'status': np.random.choice(['Active', 'Inactive', 'Pending'], n_rows),\n",
    "        \n",
    "        # Boolean column\n",
    "        'is_premium': np.random.choice([True, False], n_rows, p=[0.3, 0.7]),\n",
    "        \n",
    "        # Date columns\n",
    "        'order_date': pd.date_range('2020-01-01', periods=n_rows, freq='15min'),\n",
    "        'ship_date': pd.date_range('2020-01-02', periods=n_rows, freq='15min'),\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add calculated columns\n",
    "    df['total_amount'] = df['price'] * df['quantity'] * (1 - df['discount'])\n",
    "    df['days_to_ship'] = (df['ship_date'] - df['order_date']).dt.days\n",
    "    \n",
    "    # Add some missing values\n",
    "    missing_indices = np.random.choice(df.index, size=int(0.02 * len(df)), replace=False)\n",
    "    df.loc[missing_indices, 'rating'] = np.nan\n",
    "    \n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Test data saved to {save_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate test datasets\n",
    "test_sizes = [50000, 100000, 250000]\n",
    "polars_test_files = {}\n",
    "\n",
    "for size in test_sizes:\n",
    "    file_path = f'../data/polars_test_{size}.csv'\n",
    "    polars_test_files[size] = file_path\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Generating Polars test data with {size:,} rows...\")\n",
    "        df = generate_polars_test_data(size, file_path)\n",
    "    else:\n",
    "        print(f\"Polars test data with {size:,} rows already exists\")\n",
    "\n",
    "print(\"\\nPolars test files:\")\n",
    "for size, path in polars_test_files.items():\n",
    "    file_size = os.path.getsize(path) / 1024**2\n",
    "    print(f\"  {size:,} rows: {path} ({file_size:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Evaluation Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ§  Polars Lazy Evaluation Demo\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Load data with Polars (lazy)\n",
    "test_file = polars_test_files[100000]\n",
    "\n",
    "print(\"\\n1. Creating Lazy Frame (no data loaded yet)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create lazy frame - this is instant\n",
    "lazy_df = pl.scan_csv(test_file)\n",
    "\n",
    "lazy_create_time = time.time() - start_time\n",
    "print(f\"   Lazy frame creation time: {lazy_create_time:.6f}s (instant!)\")\n",
    "print(f\"   Schema: {lazy_df.schema}\")\n",
    "\n",
    "print(\"\\n2. Building query plan (still no execution)...\")\n",
    "query_start = time.time()\n",
    "\n",
    "# Complex query - still no execution\n",
    "lazy_query = (\n",
    "    lazy_df\n",
    "    .filter(pl.col(\"price\") > 50)\n",
    "    .with_columns([\n",
    "        pl.col(\"total_amount\").alias(\"amount\"),\n",
    "        (pl.col(\"price\") * pl.col(\"discount\")).alias(\"discount_amount\")\n",
    "    ])\n",
    "    .group_by([\"category\", \"region\"])\n",
    "    .agg([\n",
    "        pl.col(\"amount\").sum().alias(\"total_sales\"),\n",
    "        pl.col(\"amount\").mean().alias(\"avg_sale\"),\n",
    "        pl.col(\"quantity\").sum().alias(\"total_quantity\"),\n",
    "        pl.count().alias(\"transaction_count\")\n",
    "    ])\n",
    "    .filter(pl.col(\"total_sales\") > 1000)\n",
    "    .sort(\"total_sales\", descending=True)\n",
    ")\n",
    "\n",
    "query_build_time = time.time() - query_start\n",
    "print(f\"   Query plan build time: {query_build_time:.6f}s\")\n",
    "\n",
    "# Show the query plan\n",
    "print(\"\\n3. Query Plan (optimized):\")\n",
    "print(lazy_query.explain())\n",
    "\n",
    "print(\"\\n4. Executing query (now data is actually processed)...\")\n",
    "exec_start = time.time()\n",
    "\n",
    "result = lazy_query.collect()\n",
    "\n",
    "exec_time = time.time() - exec_start\n",
    "print(f\"   Query execution time: {exec_time:.3f}s\")\n",
    "print(f\"   Result shape: {result.shape}\")\n",
    "print(f\"\\nFirst 5 results:\")\n",
    "print(result.head())\n",
    "\n",
    "print(f\"\\nðŸ“Š Lazy Evaluation Benefits:\")\n",
    "print(f\"   â€¢ Query optimization before execution\")\n",
    "print(f\"   â€¢ No intermediate memory usage\")\n",
    "print(f\"   â€¢ Predicate pushdown (filters applied early)\")\n",
    "print(f\"   â€¢ Projection pushdown (only needed columns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison: Polars vs Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_polars_vs_pandas(file_path, operation_name, polars_func, pandas_func):\n",
    "    \"\"\"Benchmark Polars vs Pandas for specific operations.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Benchmark Polars\n",
    "    print(f\"\\nðŸš€ Benchmarking {operation_name}...\")\n",
    "    \n",
    "    # Polars (lazy)\n",
    "    start_time = time.time()\n",
    "    start_memory = psutil.Process().memory_info().rss / 1024**2\n",
    "    \n",
    "    polars_result = polars_func(file_path)\n",
    "    \n",
    "    polars_time = time.time() - start_time\n",
    "    polars_memory = psutil.Process().memory_info().rss / 1024**2 - start_memory\n",
    "    \n",
    "    # Pandas\n",
    "    start_time = time.time()\n",
    "    start_memory = psutil.Process().memory_info().rss / 1024**2\n",
    "    \n",
    "    pandas_result = pandas_func(file_path)\n",
    "    \n",
    "    pandas_time = time.time() - start_time\n",
    "    pandas_memory = psutil.Process().memory_info().rss / 1024**2 - start_memory\n",
    "    \n",
    "    # Calculate speedup\n",
    "    speedup = pandas_time / polars_time if polars_time > 0 else float('inf')\n",
    "    memory_ratio = pandas_memory / polars_memory if polars_memory > 0 else float('inf')\n",
    "    \n",
    "    results = {\n",
    "        'operation': operation_name,\n",
    "        'polars_time': polars_time,\n",
    "        'pandas_time': pandas_time,\n",
    "        'speedup': speedup,\n",
    "        'polars_memory': polars_memory,\n",
    "        'pandas_memory': pandas_memory,\n",
    "        'memory_ratio': memory_ratio\n",
    "    }\n",
    "    \n",
    "    print(f\"   Polars: {polars_time:.3f}s, {polars_memory:.1f}MB\")\n",
    "    print(f\"   Pandas: {pandas_time:.3f}s, {pandas_memory:.1f}MB\")\n",
    "    print(f\"   Speedup: {speedup:.2f}x, Memory reduction: {memory_ratio:.2f}x\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define benchmark operations\n",
    "def polars_groupby_agg(file_path):\n",
    "    return (\n",
    "        pl.scan_csv(file_path)\n",
    "        .group_by([\"category\", \"region\"])\n",
    "        .agg([\n",
    "            pl.col(\"total_amount\").sum().alias(\"total_sales\"),\n",
    "            pl.col(\"total_amount\").mean().alias(\"avg_sale\"),\n",
    "            pl.col(\"quantity\").sum().alias(\"total_qty\"),\n",
    "            pl.count().alias(\"count\")\n",
    "        ])\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "def pandas_groupby_agg(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df.groupby([\"category\", \"region\"]).agg({\n",
    "        'total_amount': ['sum', 'mean'],\n",
    "        'quantity': 'sum',\n",
    "        'id': 'count'\n",
    "    })\n",
    "\n",
    "def polars_filter_sort(file_path):\n",
    "    return (\n",
    "        pl.scan_csv(file_path)\n",
    "        .filter((pl.col(\"price\") > 100) & (pl.col(\"is_premium\") == True))\n",
    "        .sort([\"total_amount\", \"order_date\"], descending=[True, False])\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "def pandas_filter_sort(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    filtered = df[(df['price'] > 100) & (df['is_premium'] == True)]\n",
    "    return filtered.sort_values(['total_amount', 'order_date'], ascending=[False, True])\n",
    "\n",
    "def polars_window_function(file_path):\n",
    "    return (\n",
    "        pl.scan_csv(file_path)\n",
    "        .with_columns([\n",
    "            pl.col(\"total_amount\").rank(method=\"ordinal\").over(\"category\").alias(\"rank_in_category\"),\n",
    "            pl.col(\"price\").mean().over(\"customer_id\").alias(\"customer_avg_price\")\n",
    "        ])\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "def pandas_window_function(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['rank_in_category'] = df.groupby('category')['total_amount'].rank(method='first')\n",
    "    df['customer_avg_price'] = df.groupby('customer_id')['price'].transform('mean')\n",
    "    return df\n",
    "\n",
    "# Run benchmarks\n",
    "benchmark_operations = [\n",
    "    ('GroupBy Aggregation', polars_groupby_agg, pandas_groupby_agg),\n",
    "    ('Filter & Sort', polars_filter_sort, pandas_filter_sort),\n",
    "    ('Window Functions', polars_window_function, pandas_window_function)\n",
    "]\n",
    "\n",
    "benchmark_results = []\n",
    "test_file = polars_test_files[100000]\n",
    "\n",
    "for op_name, polars_func, pandas_func in benchmark_operations:\n",
    "    try:\n",
    "        result = benchmark_polars_vs_pandas(test_file, op_name, polars_func, pandas_func)\n",
    "        benchmark_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {op_name}: {e}\")\n",
    "\n",
    "# Summary\n",
    "if benchmark_results:\n",
    "    results_df = pd.DataFrame(benchmark_results)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Performance Summary:\")\n",
    "    print(f\"   Average speedup: {results_df['speedup'].mean():.2f}x\")\n",
    "    print(f\"   Average memory reduction: {results_df['memory_ratio'].mean():.2f}x\")\n",
    "    print(f\"   Best speedup: {results_df['speedup'].max():.2f}x ({results_df.loc[results_df['speedup'].idxmax(), 'operation']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Aggregations and Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”§ Advanced Polars Operations Demo\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Load data\n",
    "df = pl.scan_csv(polars_test_files[100000])\n",
    "\n",
    "print(\"\\n1. Complex Aggregations with Multiple Grouping Levels\")\n",
    "complex_agg = (\n",
    "    df\n",
    "    .group_by([\"category\", \"region\", \"status\"])\n",
    "    .agg([\n",
    "        pl.col(\"total_amount\").sum().alias(\"total_sales\"),\n",
    "        pl.col(\"total_amount\").mean().alias(\"avg_sale\"),\n",
    "        pl.col(\"total_amount\").std().alias(\"sales_std\"),\n",
    "        pl.col(\"total_amount\").quantile(0.5).alias(\"median_sale\"),\n",
    "        pl.col(\"total_amount\").quantile(0.95).alias(\"p95_sale\"),\n",
    "        pl.col(\"customer_id\").n_unique().alias(\"unique_customers\"),\n",
    "        pl.count().alias(\"transaction_count\")\n",
    "    ])\n",
    "    .filter(pl.col(\"transaction_count\") > 100)\n",
    "    .sort(\"total_sales\", descending=True)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(f\"   Result shape: {complex_agg.shape}\")\n",
    "print(\"   Top 5 category-region combinations:\")\n",
    "print(complex_agg.head())\n",
    "\n",
    "print(\"\\n2. Advanced Window Functions\")\n",
    "window_analysis = (\n",
    "    df\n",
    "    .with_columns([\n",
    "        # Ranking within groups\n",
    "        pl.col(\"total_amount\").rank(method=\"ordinal\", descending=True).over(\"category\").alias(\"category_rank\"),\n",
    "        \n",
    "        # Rolling statistics\n",
    "        pl.col(\"total_amount\").mean().over(\"customer_id\").alias(\"customer_avg_spend\"),\n",
    "        pl.col(\"total_amount\").std().over(\"customer_id\").alias(\"customer_spend_volatility\"),\n",
    "        \n",
    "        # Percentage of total\n",
    "        (pl.col(\"total_amount\") / pl.col(\"total_amount\").sum().over(\"category\") * 100).alias(\"pct_of_category_sales\"),\n",
    "        \n",
    "        # Lead/lag operations (simulated with shift)\n",
    "        pl.col(\"total_amount\").shift(1).over([\"customer_id\"]).alias(\"prev_purchase_amount\"),\n",
    "        pl.col(\"total_amount\").shift(-1).over([\"customer_id\"]).alias(\"next_purchase_amount\")\n",
    "    ])\n",
    "    .filter(pl.col(\"category_rank\") <= 10)  # Top 10 in each category\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(f\"   Window analysis result shape: {window_analysis.shape}\")\n",
    "print(\"   Sample with window functions:\")\n",
    "print(window_analysis.select([\n",
    "    \"customer_id\", \"category\", \"total_amount\", \"category_rank\", \n",
    "    \"customer_avg_spend\", \"pct_of_category_sales\"\n",
    "]).head())\n",
    "\n",
    "print(\"\\n3. Time-based Analysis\")\n",
    "time_analysis = (\n",
    "    df\n",
    "    .with_columns([\n",
    "        pl.col(\"order_date\").str.to_datetime(\"%Y-%m-%d %H:%M:%S\").alias(\"order_datetime\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.col(\"order_datetime\").dt.year().alias(\"year\"),\n",
    "        pl.col(\"order_datetime\").dt.month().alias(\"month\"),\n",
    "        pl.col(\"order_datetime\").dt.weekday().alias(\"weekday\"),\n",
    "        pl.col(\"order_datetime\").dt.hour().alias(\"hour\")\n",
    "    ])\n",
    "    .group_by([\"year\", \"month\", \"category\"])\n",
    "    .agg([\n",
    "        pl.col(\"total_amount\").sum().alias(\"monthly_sales\"),\n",
    "        pl.col(\"total_amount\").count().alias(\"monthly_transactions\"),\n",
    "        pl.col(\"customer_id\").n_unique().alias(\"monthly_customers\")\n",
    "    ])\n",
    "    .sort([\"year\", \"month\", \"monthly_sales\"], descending=[True, True, True])\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(f\"   Time analysis result shape: {time_analysis.shape}\")\n",
    "print(\"   Monthly trends sample:\")\n",
    "print(time_analysis.head())\n",
    "\n",
    "print(\"\\n4. Conditional Aggregations\")\n",
    "conditional_agg = (\n",
    "    df\n",
    "    .group_by(\"category\")\n",
    "    .agg([\n",
    "        # Conditional sums\n",
    "        pl.when(pl.col(\"is_premium\") == True)\n",
    "        .then(pl.col(\"total_amount\"))\n",
    "        .otherwise(0)\n",
    "        .sum()\n",
    "        .alias(\"premium_sales\"),\n",
    "        \n",
    "        pl.when(pl.col(\"discount\") > 0.1)\n",
    "        .then(pl.col(\"total_amount\"))\n",
    "        .otherwise(0)\n",
    "        .sum()\n",
    "        .alias(\"discounted_sales\"),\n",
    "        \n",
    "        # Conditional counts\n",
    "        pl.when(pl.col(\"rating\") >= 4.0)\n",
    "        .then(1)\n",
    "        .otherwise(0)\n",
    "        .sum()\n",
    "        .alias(\"high_rating_count\"),\n",
    "        \n",
    "        pl.col(\"total_amount\").sum().alias(\"total_sales\"),\n",
    "        pl.count().alias(\"total_transactions\")\n",
    "    ])\n",
    "    .with_columns([\n",
    "        (pl.col(\"premium_sales\") / pl.col(\"total_sales\") * 100).alias(\"premium_pct\"),\n",
    "        (pl.col(\"discounted_sales\") / pl.col(\"total_sales\") * 100).alias(\"discount_pct\"),\n",
    "        (pl.col(\"high_rating_count\") / pl.col(\"total_transactions\") * 100).alias(\"high_rating_pct\")\n",
    "    ])\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(f\"   Conditional aggregation result:\")\n",
    "print(conditional_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Large Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŒŠ Polars Streaming Demo\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Use the largest test file\n",
    "large_file = polars_test_files[250000]\n",
    "\n",
    "print(f\"\\nProcessing file: {large_file}\")\n",
    "print(f\"File size: {os.path.getsize(large_file) / 1024**2:.1f} MB\")\n",
    "\n",
    "print(\"\\n1. Streaming Aggregation (processes data in chunks)\")\n",
    "\n",
    "start_time = time.time()\n",
    "start_memory = psutil.Process().memory_info().rss / 1024**2\n",
    "\n",
    "# Streaming query - processes data in chunks without loading everything\n",
    "streaming_result = (\n",
    "    pl.scan_csv(large_file)\n",
    "    .filter(pl.col(\"price\") > 50)\n",
    "    .group_by([\"category\", \"region\"])\n",
    "    .agg([\n",
    "        pl.col(\"total_amount\").sum().alias(\"total_sales\"),\n",
    "        pl.col(\"total_amount\").mean().alias(\"avg_sale\"),\n",
    "        pl.col(\"quantity\").sum().alias(\"total_quantity\"),\n",
    "        pl.count().alias(\"transaction_count\")\n",
    "    ])\n",
    "    .sort(\"total_sales\", descending=True)\n",
    "    .collect(streaming=True)  # Enable streaming\n",
    ")\n",
    "\n",
    "streaming_time = time.time() - start_time\n",
    "streaming_memory = psutil.Process().memory_info().rss / 1024**2 - start_memory\n",
    "\n",
    "print(f\"   Streaming execution time: {streaming_time:.3f}s\")\n",
    "print(f\"   Memory used: {streaming_memory:.1f} MB\")\n",
    "print(f\"   Result shape: {streaming_result.shape}\")\n",
    "print(\"   Top results:\")\n",
    "print(streaming_result.head())\n",
    "\n",
    "print(\"\\n2. Compare with Non-streaming (loads all data)\")\n",
    "\n",
    "start_time = time.time()\n",
    "start_memory = psutil.Process().memory_info().rss / 1024**2\n",
    "\n",
    "# Non-streaming query\n",
    "regular_result = (\n",
    "    pl.scan_csv(large_file)\n",
    "    .filter(pl.col(\"price\") > 50)\n",
    "    .group_by([\"category\", \"region\"])\n",
    "    .agg([\n",
    "        pl.col(\"total_amount\").sum().alias(\"total_sales\"),\n",
    "        pl.col(\"total_amount\").mean().alias(\"avg_sale\"),\n",
    "        pl.col(\"quantity\").sum().alias(\"total_quantity\"),\n",
    "        pl.count().alias(\"transaction_count\")\n",
    "    ])\n",
    "    .sort(\"total_sales\", descending=True)\n",
    "    .collect(streaming=False)  # Disable streaming\n",
    ")\n",
    "\n",
    "regular_time = time.time() - start_time\n",
    "regular_memory = psutil.Process().memory_info().rss / 1024**2 - start_memory\n",
    "\n",
    "print(f\"   Regular execution time: {regular_time:.3f}s\")\n",
    "print(f\"   Memory used: {regular_memory:.1f} MB\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Streaming vs Regular:\")\n",
    "print(f\"   Time difference: {(regular_time - streaming_time):.3f}s\")\n",
    "print(f\"   Memory savings: {regular_memory - streaming_memory:.1f} MB\")\n",
    "print(f\"   Memory reduction: {((regular_memory - streaming_memory) / regular_memory * 100):.1f}%\")\n",
    "\n",
    "print(\"\\n3. Streaming with Multiple Operations\")\n",
    "\n",
    "complex_streaming = (\n",
    "    pl.scan_csv(large_file)\n",
    "    .with_columns([\n",
    "        pl.col(\"order_date\").str.to_datetime(\"%Y-%m-%d %H:%M:%S\").alias(\"order_datetime\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.col(\"order_datetime\").dt.month().alias(\"month\"),\n",
    "        (pl.col(\"total_amount\") > pl.col(\"total_amount\").mean()).alias(\"above_avg_sale\")\n",
    "    ])\n",
    "    .group_by([\"month\", \"category\", \"above_avg_sale\"])\n",
    "    .agg([\n",
    "        pl.col(\"total_amount\").sum().alias(\"sales\"),\n",
    "        pl.count().alias(\"count\")\n",
    "    ])\n",
    "    .collect(streaming=True)\n",
    ")\n",
    "\n",
    "print(f\"   Complex streaming result shape: {complex_streaming.shape}\")\n",
    "print(\"   Sample:\")\n",
    "print(complex_streaming.head())\n",
    "\n",
    "print(\"\\nâœ… Streaming Benefits:\")\n",
    "print(\"   â€¢ Process datasets larger than memory\")\n",
    "print(\"   â€¢ Consistent memory usage regardless of data size\")\n",
    "print(\"   â€¢ Automatic query optimization\")\n",
    "print(\"   â€¢ Parallelization across chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ’¾ Memory Efficiency: Polars vs Pandas\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def analyze_memory_usage(file_path, dataset_name):\n",
    "    \"\"\"Compare memory usage between Polars and Pandas.\"\"\"\n",
    "    \n",
    "    print(f\"\\nAnalyzing {dataset_name}...\")\n",
    "    file_size = os.path.getsize(file_path) / 1024**2\n",
    "    print(f\"File size: {file_size:.1f} MB\")\n",
    "    \n",
    "    # Pandas memory usage\n",
    "    start_memory = psutil.Process().memory_info().rss / 1024**2\n",
    "    pandas_df = pd.read_csv(file_path)\n",
    "    pandas_memory = psutil.Process().memory_info().rss / 1024**2 - start_memory\n",
    "    pandas_internal_memory = pandas_df.memory_usage(deep=True).sum() / 1024**2\n",
    "    \n",
    "    print(f\"\\nPandas:\")\n",
    "    print(f\"  System memory used: {pandas_memory:.1f} MB\")\n",
    "    print(f\"  DataFrame memory: {pandas_internal_memory:.1f} MB\")\n",
    "    print(f\"  Memory overhead: {pandas_memory - pandas_internal_memory:.1f} MB\")\n",
    "    \n",
    "    # Clear pandas dataframe\n",
    "    del pandas_df\n",
    "    \n",
    "    # Polars memory usage\n",
    "    start_memory = psutil.Process().memory_info().rss / 1024**2\n",
    "    polars_df = pl.read_csv(file_path)\n",
    "    polars_memory = psutil.Process().memory_info().rss / 1024**2 - start_memory\n",
    "    \n",
    "    # Polars doesn't have direct memory_usage method, estimate from types\n",
    "    polars_estimated_memory = 0\n",
    "    for col, dtype in polars_df.schema.items():\n",
    "        if dtype == pl.Int64:\n",
    "            polars_estimated_memory += len(polars_df) * 8\n",
    "        elif dtype == pl.Float64:\n",
    "            polars_estimated_memory += len(polars_df) * 8\n",
    "        elif dtype == pl.Boolean:\n",
    "            polars_estimated_memory += len(polars_df) * 1\n",
    "        else:  # String types\n",
    "            avg_str_len = polars_df[col].str.len_chars().mean() if col in polars_df.columns else 20\n",
    "            polars_estimated_memory += len(polars_df) * avg_str_len\n",
    "    \n",
    "    polars_estimated_memory = polars_estimated_memory / 1024**2\n",
    "    \n",
    "    print(f\"\\nPolars:\")\n",
    "    print(f\"  System memory used: {polars_memory:.1f} MB\")\n",
    "    print(f\"  Estimated DataFrame memory: {polars_estimated_memory:.1f} MB\")\n",
    "    print(f\"  Memory overhead: {polars_memory - polars_estimated_memory:.1f} MB\")\n",
    "    \n",
    "    # Comparison\n",
    "    memory_improvement = (pandas_memory - polars_memory) / pandas_memory * 100\n",
    "    print(f\"\\nðŸ“Š Comparison:\")\n",
    "    print(f\"  Polars uses {memory_improvement:.1f}% less memory than Pandas\")\n",
    "    print(f\"  Memory ratio (Pandas/Polars): {pandas_memory / polars_memory:.2f}x\")\n",
    "    \n",
    "    return {\n",
    "        'dataset': dataset_name,\n",
    "        'file_size_mb': file_size,\n",
    "        'pandas_memory': pandas_memory,\n",
    "        'polars_memory': polars_memory,\n",
    "        'memory_ratio': pandas_memory / polars_memory,\n",
    "        'memory_improvement_pct': memory_improvement\n",
    "    }\n",
    "\n",
    "# Analyze different dataset sizes\n",
    "memory_comparisons = []\n",
    "\n",
    "for size, file_path in polars_test_files.items():\n",
    "    try:\n",
    "        result = analyze_memory_usage(file_path, f\"{size:,} rows\")\n",
    "        memory_comparisons.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {size} rows: {e}\")\n",
    "\n",
    "# Summary\n",
    "if memory_comparisons:\n",
    "    memory_df = pd.DataFrame(memory_comparisons)\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Memory Efficiency Summary:\")\n",
    "    print(memory_df.to_string(index=False, float_format='%.2f'))\n",
    "    \n",
    "    avg_improvement = memory_df['memory_improvement_pct'].mean()\n",
    "    avg_ratio = memory_df['memory_ratio'].mean()\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Key Insights:\")\n",
    "    print(f\"   â€¢ Average memory reduction: {avg_improvement:.1f}%\")\n",
    "    print(f\"   â€¢ Average memory ratio: {avg_ratio:.2f}x less memory\")\n",
    "    print(f\"   â€¢ Best improvement: {memory_df['memory_improvement_pct'].max():.1f}%\")\n",
    "    print(f\"   â€¢ Consistent efficiency across dataset sizes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ¤ Polars Integration with ML Pipeline\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Load data with Polars\n",
    "df = pl.scan_csv(polars_test_files[100000])\n",
    "\n",
    "print(\"\\n1. Data Preprocessing with Polars\")\n",
    "\n",
    "# Feature engineering pipeline\n",
    "preprocessed = (\n",
    "    df\n",
    "    .with_columns([\n",
    "        # Create derived features\n",
    "        (pl.col(\"price\") * pl.col(\"quantity\")).alias(\"gross_amount\"),\n",
    "        (pl.col(\"discount\") * 100).round(2).alias(\"discount_pct\"),\n",
    "        \n",
    "        # Date features\n",
    "        pl.col(\"order_date\").str.to_datetime(\"%Y-%m-%d %H:%M:%S\").alias(\"order_datetime\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.col(\"order_datetime\").dt.month().alias(\"order_month\"),\n",
    "        pl.col(\"order_datetime\").dt.weekday().alias(\"order_weekday\"),\n",
    "        pl.col(\"order_datetime\").dt.hour().alias(\"order_hour\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        # Categorical encoding (convert to numeric)\n",
    "        pl.col(\"category\").map_elements(lambda x: hash(x) % 1000, return_dtype=pl.Int32).alias(\"category_encoded\"),\n",
    "        pl.col(\"region\").map_elements(lambda x: hash(x) % 100, return_dtype=pl.Int32).alias(\"region_encoded\"),\n",
    "        pl.col(\"status\").map_elements(lambda x: hash(x) % 10, return_dtype=pl.Int32).alias(\"status_encoded\"),\n",
    "        \n",
    "        # Boolean to int\n",
    "        pl.col(\"is_premium\").cast(pl.Int8).alias(\"is_premium_int\"),\n",
    "    ])\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(f\"   Preprocessed data shape: {preprocessed.shape}\")\n",
    "print(f\"   New columns created: {set(preprocessed.columns) - set(['id', 'customer_id', 'product_id', 'quantity', 'price', 'discount', 'rating', 'category', 'region', 'status', 'is_premium', 'order_date', 'ship_date', 'total_amount', 'days_to_ship'])}\")\n",
    "\n",
    "print(\"\\n2. Converting to ML-ready format\")\n",
    "\n",
    "# Select features for ML\n",
    "feature_columns = [\n",
    "    'price', 'quantity', 'discount_pct', 'rating', 'days_to_ship',\n",
    "    'order_month', 'order_weekday', 'order_hour',\n",
    "    'category_encoded', 'region_encoded', 'status_encoded', 'is_premium_int'\n",
    "]\n",
    "\n",
    "# Create feature matrix and target\n",
    "ml_ready = (\n",
    "    preprocessed\n",
    "    .select(feature_columns + ['total_amount'])  # target variable\n",
    "    .drop_nulls()  # Remove rows with missing values\n",
    ")\n",
    "\n",
    "print(f\"   ML-ready data shape: {ml_ready.shape}\")\n",
    "print(f\"   Features: {feature_columns}\")\n",
    "print(f\"   Target: total_amount\")\n",
    "\n",
    "# Convert to numpy arrays for sklearn compatibility\n",
    "X = ml_ready.select(feature_columns).to_numpy()\n",
    "y = ml_ready.select('total_amount').to_numpy().flatten()\n",
    "\n",
    "print(f\"   Feature matrix shape: {X.shape}\")\n",
    "print(f\"   Target vector shape: {y.shape}\")\n",
    "\n",
    "print(\"\\n3. Quick ML Model Training (Demo)\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"   Model performance:\")\n",
    "print(f\"     RÂ² Score: {r2:.4f}\")\n",
    "print(f\"     RMSE: {np.sqrt(mse):.2f}\")\n",
    "\n",
    "# Feature importance (using Polars for analysis)\n",
    "importance_df = pl.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort('importance', descending=True)\n",
    "\n",
    "print(f\"\\n   Top 5 important features:\")\n",
    "print(importance_df.head())\n",
    "\n",
    "print(\"\\n4. Data Export Options\")\n",
    "\n",
    "# Export preprocessed data\n",
    "export_options = {\n",
    "    'csv': lambda df, path: df.write_csv(path),\n",
    "    'parquet': lambda df, path: df.write_parquet(path),\n",
    "    'json': lambda df, path: df.write_ndjson(path),\n",
    "    'pandas': lambda df, path: df.to_pandas()\n",
    "}\n",
    "\n",
    "print(f\"   Available export formats: {list(export_options.keys())}\")\n",
    "\n",
    "# Example: Convert to pandas for scikit-learn\n",
    "pandas_df = ml_ready.to_pandas()\n",
    "print(f\"   Converted to pandas: {pandas_df.shape}\")\n",
    "print(f\"   Memory usage: {pandas_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "print(\"\\nâœ… Integration Benefits:\")\n",
    "print(\"   â€¢ Fast data preprocessing with lazy evaluation\")\n",
    "print(\"   â€¢ Memory efficient feature engineering\")\n",
    "print(\"   â€¢ Easy conversion to pandas/numpy for ML libraries\")\n",
    "print(\"   â€¢ Multiple export formats for different use cases\")\n",
    "print(\"   â€¢ Consistent performance across data sizes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Polars vs Pandas Performance Analysis', fontsize=16)\n",
    "\n",
    "# 1. Speed comparison\n",
    "if benchmark_results:\n",
    "    ops = [r['operation'] for r in benchmark_results]\n",
    "    speedups = [r['speedup'] for r in benchmark_results]\n",
    "    \n",
    "    bars = axes[0, 0].bar(range(len(ops)), speedups, color='skyblue', alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Operation')\n",
    "    axes[0, 0].set_ylabel('Speedup (x faster)')\n",
    "    axes[0, 0].set_title('Polars Speedup vs Pandas')\n",
    "    axes[0, 0].set_xticks(range(len(ops)))\n",
    "    axes[0, 0].set_xticklabels(ops, rotation=45, ha='right')\n",
    "    axes[0, 0].axhline(y=1, color='red', linestyle='--', alpha=0.7, label='Same speed')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, speedup in zip(bars, speedups):\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                       f'{speedup:.1f}x', ha='center', va='bottom')\n",
    "else:\n",
    "    axes[0, 0].text(0.5, 0.5, 'No benchmark data available', ha='center', va='center')\n",
    "    axes[0, 0].set_title('Speed Comparison')\n",
    "\n",
    "# 2. Memory efficiency\n",
    "if memory_comparisons:\n",
    "    datasets = [comp['dataset'] for comp in memory_comparisons]\n",
    "    pandas_mem = [comp['pandas_memory'] for comp in memory_comparisons]\n",
    "    polars_mem = [comp['polars_memory'] for comp in memory_comparisons]\n",
    "    \n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 1].bar(x - width/2, pandas_mem, width, label='Pandas', color='orange', alpha=0.8)\n",
    "    axes[0, 1].bar(x + width/2, polars_mem, width, label='Polars', color='green', alpha=0.8)\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Dataset Size')\n",
    "    axes[0, 1].set_ylabel('Memory Usage (MB)')\n",
    "    axes[0, 1].set_title('Memory Usage Comparison')\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(datasets)\n",
    "    axes[0, 1].legend()\n",
    "else:\n",
    "    axes[0, 1].text(0.5, 0.5, 'No memory data available', ha='center', va='center')\n",
    "    axes[0, 1].set_title('Memory Usage Comparison')\n",
    "\n",
    "# 3. Memory efficiency ratio\n",
    "if memory_comparisons:\n",
    "    ratios = [comp['memory_ratio'] for comp in memory_comparisons]\n",
    "    \n",
    "    axes[1, 0].bar(datasets, ratios, color='purple', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Dataset Size')\n",
    "    axes[1, 0].set_ylabel('Memory Ratio (Pandas/Polars)')\n",
    "    axes[1, 0].set_title('Memory Efficiency Ratio')\n",
    "    axes[1, 0].axhline(y=1, color='red', linestyle='--', alpha=0.7, label='Same efficiency')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, ratio in enumerate(ratios):\n",
    "        axes[1, 0].text(i, ratio + 0.05, f'{ratio:.1f}x', ha='center', va='bottom')\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'No ratio data available', ha='center', va='center')\n",
    "    axes[1, 0].set_title('Memory Efficiency Ratio')\n",
    "\n",
    "# 4. Combined performance score\n",
    "if benchmark_results and memory_comparisons:\n",
    "    # Calculate combined score (speedup * memory efficiency)\n",
    "    combined_scores = []\n",
    "    labels = []\n",
    "    \n",
    "    for bench in benchmark_results:\n",
    "        # Find matching memory comparison (use average)\n",
    "        avg_memory_ratio = np.mean([comp['memory_ratio'] for comp in memory_comparisons])\n",
    "        combined_score = bench['speedup'] * avg_memory_ratio\n",
    "        combined_scores.append(combined_score)\n",
    "        labels.append(bench['operation'])\n",
    "    \n",
    "    bars = axes[1, 1].bar(range(len(labels)), combined_scores, color='teal', alpha=0.8)\n",
    "    axes[1, 1].set_xlabel('Operation')\n",
    "    axes[1, 1].set_ylabel('Combined Score (Speed Ã— Memory)')\n",
    "    axes[1, 1].set_title('Overall Performance Advantage')\n",
    "    axes[1, 1].set_xticks(range(len(labels)))\n",
    "    axes[1, 1].set_xticklabels(labels, rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, combined_scores):\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,\n",
    "                       f'{score:.1f}', ha='center', va='bottom')\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'No combined data available', ha='center', va='center')\n",
    "    axes[1, 1].set_title('Overall Performance Advantage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nðŸ“Š POLARS PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if benchmark_results:\n",
    "    avg_speedup = np.mean([r['speedup'] for r in benchmark_results])\n",
    "    best_speedup = max([r['speedup'] for r in benchmark_results])\n",
    "    print(f\"\\nâš¡ Speed Performance:\")\n",
    "    print(f\"   â€¢ Average speedup: {avg_speedup:.2f}x\")\n",
    "    print(f\"   â€¢ Best speedup: {best_speedup:.2f}x\")\n",
    "\n",
    "if memory_comparisons:\n",
    "    avg_memory_improvement = np.mean([comp['memory_improvement_pct'] for comp in memory_comparisons])\n",
    "    avg_memory_ratio = np.mean([comp['memory_ratio'] for comp in memory_comparisons])\n",
    "    print(f\"\\nðŸ’¾ Memory Efficiency:\")\n",
    "    print(f\"   â€¢ Average memory reduction: {avg_memory_improvement:.1f}%\")\n",
    "    print(f\"   â€¢ Average memory efficiency: {avg_memory_ratio:.2f}x\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Key Advantages:\")\n",
    "print(f\"   â€¢ Lazy evaluation with query optimization\")\n",
    "print(f\"   â€¢ Automatic parallelization\")\n",
    "print(f\"   â€¢ Streaming for large datasets\")\n",
    "print(f\"   â€¢ Memory efficient columnar storage\")\n",
    "print(f\"   â€¢ Easy integration with existing ML workflows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸŽ¯ POLARS BEST PRACTICES AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "print(\"\\n1. WHEN TO USE POLARS:\")\n",
    "print(\"   âœ… Large datasets (> 1GB)\")\n",
    "print(\"   âœ… Complex aggregations and transformations\")\n",
    "print(\"   âœ… Memory-constrained environments\")\n",
    "print(\"   âœ… ETL pipelines with performance requirements\")\n",
    "print(\"   âœ… Time-series analysis with window functions\")\n",
    "\n",
    "print(\"\\n2. OPTIMIZATION TECHNIQUES:\")\n",
    "print(\"   â€¢ Use lazy evaluation (scan_csv vs read_csv)\")\n",
    "print(\"   â€¢ Enable streaming for large datasets\")\n",
    "print(\"   â€¢ Filter early in the query pipeline\")\n",
    "print(\"   â€¢ Use select() to reduce columns before heavy operations\")\n",
    "print(\"   â€¢ Leverage built-in parallelization (no manual threading needed)\")\n",
    "\n",
    "print(\"\\n3. MEMORY MANAGEMENT:\")\n",
    "print(\"   â€¢ Use appropriate data types (pl.Int32 vs pl.Int64)\")\n",
    "print(\"   â€¢ Leverage categorical data when appropriate\")\n",
    "print(\"   â€¢ Process data in chunks with streaming\")\n",
    "print(\"   â€¢ Avoid collecting() until absolutely necessary\")\n",
    "\n",
    "print(\"\\n4. INTEGRATION PATTERNS:\")\n",
    "print(\"   â€¢ Use Polars for data preprocessing\")\n",
    "print(\"   â€¢ Convert to_pandas() only for ML model training\")\n",
    "print(\"   â€¢ Use to_numpy() for direct array operations\")\n",
    "print(\"   â€¢ Export to parquet for intermediate storage\")\n",
    "\n",
    "print(\"\\n5. COMMON PATTERNS:\")\n",
    "print(\"   â€¢ Chain operations for query optimization\")\n",
    "print(\"   â€¢ Use with_columns() for feature engineering\")\n",
    "print(\"   â€¢ Leverage group_by().agg() for complex aggregations\")\n",
    "print(\"   â€¢ Use window functions for time-series features\")\n",
    "\n",
    "print(\"\\n6. PERFORMANCE TIPS:\")\n",
    "print(\"   â€¢ Prefer Polars expressions over map_elements()\")\n",
    "print(\"   â€¢ Use join() instead of manual merging logic\")\n",
    "print(\"   â€¢ Batch similar operations together\")\n",
    "print(\"   â€¢ Monitor query plans with explain()\")\n",
    "\n",
    "print(\"\\n7. WHEN TO STICK WITH PANDAS:\")\n",
    "print(\"   â€¢ Small datasets (< 100MB)\")\n",
    "print(\"   â€¢ Heavy integration with pandas ecosystem\")\n",
    "print(\"   â€¢ Complex custom functions not available in Polars\")\n",
    "print(\"   â€¢ Interactive data exploration (better visualization support)\")\n",
    "\n",
    "# Performance recommendations based on our tests\n",
    "if benchmark_results and memory_comparisons:\n",
    "    print(f\"\\nðŸ“Š PERFORMANCE INSIGHTS (This System):\")\n",
    "    \n",
    "    if benchmark_results:\n",
    "        best_operation = max(benchmark_results, key=lambda x: x['speedup'])\n",
    "        print(f\"   â€¢ Best speedup achieved: {best_operation['speedup']:.2f}x ({best_operation['operation']})\")\n",
    "    \n",
    "    if memory_comparisons:\n",
    "        best_memory = max(memory_comparisons, key=lambda x: x['memory_improvement_pct'])\n",
    "        print(f\"   â€¢ Best memory reduction: {best_memory['memory_improvement_pct']:.1f}% ({best_memory['dataset']})\")\n",
    "    \n",
    "    print(f\"   â€¢ Recommended for datasets > 50MB\")\n",
    "    print(f\"   â€¢ Streaming recommended for datasets > 500MB\")\n",
    "\n",
    "print(\"\\nâœ… Polars engine analysis complete!\")\n",
    "print(\"\\nðŸš€ Ready for production use with:\")\n",
    "print(\"   â€¢ Significant performance improvements\")\n",
    "print(\"   â€¢ Reduced memory footprint\")\n",
    "print(\"   â€¢ Easy ML pipeline integration\")\n",
    "print(\"   â€¢ Scalable data processing capabilities\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}