{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engine Performance Comparison\n",
    "\n",
    "This notebook compares the performance of **pandas**, **Polars**, and **DuckDB** across identical operations to help choose the best engine for different use cases.\n",
    "\n",
    "## Test Scenarios\n",
    "1. **Data Loading**: CSV reading performance\n",
    "2. **Basic Operations**: Filtering, sorting, grouping\n",
    "3. **Aggregations**: Complex groupby operations\n",
    "4. **Joins**: Inner and left joins on large datasets\n",
    "5. **Window Functions**: Ranking and moving averages\n",
    "6. **Memory Usage**: Peak memory consumption\n",
    "7. **ML Integration**: Feature engineering pipeline\n",
    "\n",
    "## Decision Matrix\n",
    "Based on results, we'll provide recommendations for:\n",
    "- Small datasets (< 1M rows)\n",
    "- Medium datasets (1-10M rows) \n",
    "- Large datasets (> 10M rows)\n",
    "- Memory-constrained environments\n",
    "- SQL-heavy workflows\n",
    "- ML pipeline integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Memory monitoring\n",
    "import tracemalloc\n",
    "from memory_profiler import profile\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Package versions:\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"polars: {pl.__version__}\")\n",
    "print(f\"duckdb: {duckdb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Generation\n",
    "\n",
    "We'll create identical datasets for fair comparison across engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_data(n_rows: int, save_path: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Generate identical test dataset for all engines.\n",
    "    \n",
    "    Args:\n",
    "        n_rows: Number of rows to generate\n",
    "        save_path: Path to save CSV file\n",
    "    \n",
    "    Returns:\n",
    "        Dict with data for each engine\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate base data\n",
    "    data = {\n",
    "        'customer_id': np.random.randint(1, 100000, n_rows),\n",
    "        'transaction_id': [f'TXN_{i:08d}' for i in range(n_rows)],\n",
    "        'amount': np.random.exponential(50, n_rows).round(2),\n",
    "        'merchant_category': np.random.choice(['grocery', 'gas', 'restaurant', 'retail', 'online'], n_rows),\n",
    "        'transaction_date': pd.date_range('2023-01-01', periods=n_rows, freq='1min'),\n",
    "        'is_fraud': np.random.choice([0, 1], n_rows, p=[0.99, 0.01]),\n",
    "        'account_balance': np.random.normal(5000, 2000, n_rows).round(2),\n",
    "        'merchant_id': np.random.randint(1, 10000, n_rows),\n",
    "        'payment_method': np.random.choice(['credit', 'debit', 'cash'], n_rows),\n",
    "        'location_lat': np.random.uniform(25, 50, n_rows),\n",
    "        'location_lon': np.random.uniform(-125, -65, n_rows)\n",
    "    }\n",
    "    \n",
    "    # Create pandas DataFrame\n",
    "    df_pandas = pd.DataFrame(data)\n",
    "    \n",
    "    # Save to CSV for consistent loading\n",
    "    if save_path:\n",
    "        df_pandas.to_csv(save_path, index=False)\n",
    "    \n",
    "    # Create Polars DataFrame\n",
    "    df_polars = pl.DataFrame(data)\n",
    "    \n",
    "    return {\n",
    "        'pandas': df_pandas,\n",
    "        'polars': df_polars,\n",
    "        'csv_path': save_path\n",
    "    }\n",
    "\n",
    "# Generate test datasets of different sizes\n",
    "datasets = {}\n",
    "sizes = [10000, 100000, 1000000]  # 10K, 100K, 1M rows\n",
    "\n",
    "print(\"Generating test datasets...\")\n",
    "for size in sizes:\n",
    "    csv_path = f\"../data/test_data_{size}.csv\"\n",
    "    datasets[size] = generate_test_data(size, csv_path)\n",
    "    print(f\"Generated {size:,} rows dataset\")\n",
    "\n",
    "print(\"\\nDataset sizes (MB):\")\n",
    "for size in sizes:\n",
    "    df = datasets[size]['pandas']\n",
    "    memory_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"{size:,} rows: {memory_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measurement Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceTracker:\n",
    "    \"\"\"\n",
    "    Track execution time and memory usage for operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "    \n",
    "    def measure_operation(self, operation_name: str, engine: str, operation_func, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Measure time and memory for an operation.\n",
    "        \"\"\"\n",
    "        # Get initial memory\n",
    "        process = psutil.Process()\n",
    "        initial_memory = process.memory_info().rss / 1024**2  # MB\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Execute operation\n",
    "            result = operation_func(*args, **kwargs)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            execution_time = time.time() - start_time\n",
    "            peak_memory = process.memory_info().rss / 1024**2  # MB\n",
    "            memory_delta = peak_memory - initial_memory\n",
    "            \n",
    "            # Store results\n",
    "            self.results.append({\n",
    "                'operation': operation_name,\n",
    "                'engine': engine,\n",
    "                'execution_time': execution_time,\n",
    "                'initial_memory_mb': initial_memory,\n",
    "                'peak_memory_mb': peak_memory,\n",
    "                'memory_delta_mb': memory_delta,\n",
    "                'success': True,\n",
    "                'error': None\n",
    "            })\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            self.results.append({\n",
    "                'operation': operation_name,\n",
    "                'engine': engine,\n",
    "                'execution_time': execution_time,\n",
    "                'initial_memory_mb': initial_memory,\n",
    "                'peak_memory_mb': initial_memory,\n",
    "                'memory_delta_mb': 0,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            })\n",
    "            \n",
    "            print(f\"Error in {engine} {operation_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_results_df(self):\n",
    "        \"\"\"Return results as pandas DataFrame.\"\"\"\n",
    "        return pd.DataFrame(self.results)\n",
    "    \n",
    "    def clear_results(self):\n",
    "        \"\"\"Clear stored results.\"\"\"\n",
    "        self.results = []\n",
    "\n",
    "# Initialize tracker\n",
    "tracker = PerformanceTracker()\n",
    "\n",
    "print(\"Performance tracking framework initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Data Loading Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_loading(size: int):\n",
    "    \"\"\"\n",
    "    Test CSV loading performance across engines.\n",
    "    \"\"\"\n",
    "    csv_path = f\"../data/test_data_{size}.csv\"\n",
    "    \n",
    "    print(f\"\\nTesting data loading for {size:,} rows...\")\n",
    "    \n",
    "    # pandas\n",
    "    def load_pandas():\n",
    "        return pd.read_csv(csv_path)\n",
    "    \n",
    "    df_pandas = tracker.measure_operation(f'load_csv_{size}', 'pandas', load_pandas)\n",
    "    \n",
    "    # Polars\n",
    "    def load_polars():\n",
    "        return pl.read_csv(csv_path)\n",
    "    \n",
    "    df_polars = tracker.measure_operation(f'load_csv_{size}', 'polars', load_polars)\n",
    "    \n",
    "    # DuckDB\n",
    "    def load_duckdb():\n",
    "        conn = duckdb.connect()\n",
    "        return conn.execute(f\"SELECT * FROM read_csv_auto('{csv_path}')\").fetchdf()\n",
    "    \n",
    "    df_duckdb = tracker.measure_operation(f'load_csv_{size}', 'duckdb', load_duckdb)\n",
    "    \n",
    "    return df_pandas, df_polars, df_duckdb\n",
    "\n",
    "# Test loading for all dataset sizes\n",
    "loaded_data = {}\n",
    "for size in sizes:\n",
    "    loaded_data[size] = test_data_loading(size)\n",
    "\n",
    "print(\"\\nData loading tests completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_basic_operations(size: int):\n",
    "    \"\"\"\n",
    "    Test basic operations: filtering, sorting, selecting.\n",
    "    \"\"\"\n",
    "    print(f\"\\nTesting basic operations for {size:,} rows...\")\n",
    "    \n",
    "    # Get data\n",
    "    df_pandas, df_polars, df_duckdb = loaded_data[size]\n",
    "    \n",
    "    # Test 1: Filter operation\n",
    "    print(\"  Testing filtering...\")\n",
    "    \n",
    "    # pandas filter\n",
    "    def pandas_filter():\n",
    "        return df_pandas[df_pandas['amount'] > 100]\n",
    "    \n",
    "    tracker.measure_operation(f'filter_{size}', 'pandas', pandas_filter)\n",
    "    \n",
    "    # Polars filter\n",
    "    def polars_filter():\n",
    "        return df_polars.filter(pl.col('amount') > 100)\n",
    "    \n",
    "    tracker.measure_operation(f'filter_{size}', 'polars', polars_filter)\n",
    "    \n",
    "    # DuckDB filter\n",
    "    def duckdb_filter():\n",
    "        conn = duckdb.connect()\n",
    "        return conn.execute(\"SELECT * FROM df_duckdb WHERE amount > 100\").fetchdf()\n",
    "    \n",
    "    tracker.measure_operation(f'filter_{size}', 'duckdb', duckdb_filter)\n",
    "    \n",
    "    # Test 2: Sort operation\n",
    "    print(\"  Testing sorting...\")\n",
    "    \n",
    "    # pandas sort\n",
    "    def pandas_sort():\n",
    "        return df_pandas.sort_values(['amount', 'transaction_date'])\n",
    "    \n",
    "    tracker.measure_operation(f'sort_{size}', 'pandas', pandas_sort)\n",
    "    \n",
    "    # Polars sort\n",
    "    def polars_sort():\n",
    "        return df_polars.sort(['amount', 'transaction_date'])\n",
    "    \n",
    "    tracker.measure_operation(f'sort_{size}', 'polars', polars_sort)\n",
    "    \n",
    "    # DuckDB sort\n",
    "    def duckdb_sort():\n",
    "        conn = duckdb.connect()\n",
    "        return conn.execute(\"SELECT * FROM df_duckdb ORDER BY amount, transaction_date\").fetchdf()\n",
    "    \n",
    "    tracker.measure_operation(f'sort_{size}', 'duckdb', duckdb_sort)\n",
    "    \n",
    "    # Test 3: Column selection and transformation\n",
    "    print(\"  Testing column operations...\")\n",
    "    \n",
    "    # pandas select and transform\n",
    "    def pandas_select():\n",
    "        return df_pandas[['customer_id', 'amount', 'merchant_category']].assign(\n",
    "            amount_log=lambda x: np.log1p(x['amount'])\n",
    "        )\n",
    "    \n",
    "    tracker.measure_operation(f'select_transform_{size}', 'pandas', pandas_select)\n",
    "    \n",
    "    # Polars select and transform\n",
    "    def polars_select():\n",
    "        return df_polars.select([\n",
    "            pl.col('customer_id'),\n",
    "            pl.col('amount'),\n",
    "            pl.col('merchant_category'),\n",
    "            pl.col('amount').log1p().alias('amount_log')\n",
    "        ])\n",
    "    \n",
    "    tracker.measure_operation(f'select_transform_{size}', 'polars', polars_select)\n",
    "    \n",
    "    # DuckDB select and transform\n",
    "    def duckdb_select():\n",
    "        conn = duckdb.connect()\n",
    "        return conn.execute(\"\"\"\n",
    "            SELECT \n",
    "                customer_id,\n",
    "                amount,\n",
    "                merchant_category,\n",
    "                ln(1 + amount) as amount_log\n",
    "            FROM df_duckdb\n",
    "        \"\"\").fetchdf()\n",
    "    \n",
    "    tracker.measure_operation(f'select_transform_{size}', 'duckdb', duckdb_select)\n",
    "\n",
    "# Test basic operations for all sizes\n",
    "for size in sizes:\n",
    "    test_basic_operations(size)\n",
    "\n",
    "print(\"\\nBasic operations tests completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Aggregation Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_aggregations(size: int):\n",
    "    \"\"\"\n",
    "    Test aggregation operations: groupby, multiple aggregations.\n",
    "    \"\"\"\n",
    "    print(f\"\\nTesting aggregations for {size:,} rows...\")\n",
    "    \n",
    "    # Get data\n",
    "    df_pandas, df_polars, df_duckdb = loaded_data[size]\n",
    "    \n",
    "    # Test 1: Simple groupby\n",
    "    print(\"  Testing simple groupby...\")\n",
    "    \n",
    "    # pandas groupby\n",
    "    def pandas_groupby():\n",
    "        return df_pandas.groupby('merchant_category')['amount'].agg(['mean', 'sum', 'count'])\n",
    "    \n",
    "    tracker.measure_operation(f'groupby_simple_{size}', 'pandas', pandas_groupby)\n",
    "    \n",
    "    # Polars groupby\n",
    "    def polars_groupby():\n",
    "        return df_polars.group_by('merchant_category').agg([\n",
    "            pl.col('amount').mean().alias('amount_mean'),\n",
    "            pl.col('amount').sum().alias('amount_sum'),\n",
    "            pl.col('amount').count().alias('amount_count')\n",
    "        ])\n",
    "    \n",
    "    tracker.measure_operation(f'groupby_simple_{size}', 'polars', polars_groupby)\n",
    "    \n",
    "    # DuckDB groupby\n",
    "    def duckdb_groupby():\n",
    "        conn = duckdb.connect()\n",
    "        return conn.execute(\"\"\"\n",
    "            SELECT \n",
    "                merchant_category,\n",
    "                AVG(amount) as amount_mean,\n",
    "                SUM(amount) as amount_sum,\n",
    "                COUNT(amount) as amount_count\n",
    "            FROM df_duckdb \n",
    "            GROUP BY merchant_category\n",
    "        \"\"\").fetchdf()\n",
    "    \n",
    "    tracker.measure_operation(f'groupby_simple_{size}', 'duckdb', duckdb_groupby)\n",
    "    \n",
    "    # Test 2: Complex groupby with multiple columns\n",
    "    print(\"  Testing complex groupby...\")\n",
    "    \n",
    "    # pandas complex groupby\n",
    "    def pandas_complex_groupby():\n",
    "        return df_pandas.groupby(['merchant_category', 'payment_method']).agg({\n",
    "            'amount': ['mean', 'std', 'min', 'max'],\n",
    "            'is_fraud': 'sum',\n",
    "            'customer_id': 'nunique'\n",
    "        })\n",
    "    \n",
    "    tracker.measure_operation(f'groupby_complex_{size}', 'pandas', pandas_complex_groupby)\n",
    "    \n",
    "    # Polars complex groupby\n",
    "    def polars_complex_groupby():\n",
    "        return df_polars.group_by(['merchant_category', 'payment_method']).agg([\n",
    "            pl.col('amount').mean().alias('amount_mean'),\n",
    "            pl.col('amount').std().alias('amount_std'),\n",
    "            pl.col('amount').min().alias('amount_min'),\n",
    "            pl.col('amount').max().alias('amount_max'),\n",
    "            pl.col('is_fraud').sum().alias('fraud_sum'),\n",
    "            pl.col('customer_id').n_unique().alias('unique_customers')\n",
    "        ])\n",
    "    \n",
    "    tracker.measure_operation(f'groupby_complex_{size}', 'polars', polars_complex_groupby)\n",
    "    \n",
    "    # DuckDB complex groupby\n",
    "    def duckdb_complex_groupby():\n",
    "        conn = duckdb.connect()\n",
    "        return conn.execute(\"\"\"\n",
    "            SELECT \n",
    "                merchant_category,\n",
    "                payment_method,\n",
    "                AVG(amount) as amount_mean,\n",
    "                STDDEV(amount) as amount_std,\n",
    "                MIN(amount) as amount_min,\n",
    "                MAX(amount) as amount_max,\n",
    "                SUM(is_fraud) as fraud_sum,\n",
    "                COUNT(DISTINCT customer_id) as unique_customers\n",
    "            FROM df_duckdb \n",
    "            GROUP BY merchant_category, payment_method\n",
    "        \"\"\").fetchdf()\n",
    "    \n",
    "    tracker.measure_operation(f'groupby_complex_{size}', 'duckdb', duckdb_complex_groupby)\n",
    "\n",
    "# Test aggregations for all sizes\n",
    "for size in sizes:\n",
    "    test_aggregations(size)\n",
    "\n",
    "print(\"\\nAggregation tests completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Join Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_joins(size: int):\n",
    "    \"\"\"\n",
    "    Test join operations with lookup tables.\n",
    "    \"\"\"\n",
    "    print(f\"\\nTesting joins for {size:,} rows...\")\n",
    "    \n",
    "    # Get data\n",
    "    df_pandas, df_polars, df_duckdb = loaded_data[size]\n",
    "    \n",
    "    # Create lookup table\n",
    "    np.random.seed(42)\n",
    "    lookup_data = {\n",
    "        'merchant_id': range(1, 10001),\n",
    "        'merchant_name': [f'Merchant_{i}' for i in range(1, 10001)],\n",
    "        'merchant_city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], 10000),\n",
    "        'merchant_type': np.random.choice(['chain', 'local', 'franchise'], 10000)\n",
    "    }\n",
    "    \n",
    "    lookup_pandas = pd.DataFrame(lookup_data)\n",
    "    lookup_polars = pl.DataFrame(lookup_data)\n",
    "    lookup_duckdb = pd.DataFrame(lookup_data)  # For DuckDB registration\n",
    "    \n",
    "    # Test 1: Inner join\n",
    "    print(\"  Testing inner join...\")\n",
    "    \n",
    "    # pandas inner join\n",
    "    def pandas_inner_join():\n",
    "        return df_pandas.merge(lookup_pandas, on='merchant_id', how='inner')\n",
    "    \n",
    "    tracker.measure_operation(f'inner_join_{size}', 'pandas', pandas_inner_join)\n",
    "    \n",
    "    # Polars inner join\n",
    "    def polars_inner_join():\n",
    "        return df_polars.join(lookup_polars, on='merchant_id', how='inner')\n",
    "    \n",
    "    tracker.measure_operation(f'inner_join_{size}', 'polars', polars_inner_join)\n",
    "    \n",
    "    # DuckDB inner join\n",
    "    def duckdb_inner_join():\n",
    "        conn = duckdb.connect()\n",
    "        return conn.execute(\"\"\"\n",
    "            SELECT t.*, l.merchant_name, l.merchant_city, l.merchant_type\n",
    "            FROM df_duckdb t\n",
    "            INNER JOIN lookup_duckdb l ON t.merchant_id = l.merchant_id\n",
    "        \"\"\").fetchdf()\n",
    "    \n",
    "    tracker.measure_operation(f'inner_join_{size}', 'duckdb', duckdb_inner_join)\n",
    "    \n",
    "    # Test 2: Left join with aggregation\n",
    "    print(\"  Testing left join with aggregation...\")\n",
    "    \n",
    "    # pandas left join + aggregation\n",
    "    def pandas_left_join_agg():\n",
    "        joined = df_pandas.merge(lookup_pandas, on='merchant_id', how='left')\n",
    "        return joined.groupby('merchant_city')['amount'].agg(['sum', 'mean', 'count'])\n",
    "    \n",
    "    tracker.measure_operation(f'left_join_agg_{size}', 'pandas', pandas_left_join_agg)\n",
    "    \n",
    "    # Polars left join + aggregation\n",
    "    def polars_left_join_agg():\n",
    "        return (df_polars\n",
    "                .join(lookup_polars, on='merchant_id', how='left')\n",
    "                .group_by('merchant_city')\n",
    "                .agg([\n",
    "                    pl.col('amount').sum().alias('amount_sum'),\n",
    "                    pl.col('amount').mean().alias('amount_mean'),\n",
    "                    pl.col('amount').count().alias('amount_count')\n",
    "                ]))\n",
    "    \n",
    "    tracker.measure_operation(f'left_join_agg_{size}', 'polars', polars_left_join_agg)\n",
    "    \n",
    "    # DuckDB left join + aggregation\n",
    "    def duckdb_left_join_agg():\n",
    "        conn = duckdb.connect()\n",
    "        return conn.execute(\"\"\"\n",
    "            SELECT \n",
    "                l.merchant_city,\n",
    "                SUM(t.amount) as amount_sum,\n",
    "                AVG(t.amount) as amount_mean,\n",
    "                COUNT(t.amount) as amount_count\n",
    "            FROM df_duckdb t\n",
    "            LEFT JOIN lookup_duckdb l ON t.merchant_id = l.merchant_id\n",
    "            GROUP BY l.merchant_city\n",
    "        \"\"\").fetchdf()\n",
    "    \n",
    "    tracker.measure_operation(f'left_join_agg_{size}', 'duckdb', duckdb_left_join_agg)\n",
    "\n",
    "# Test joins for all sizes (skip largest for memory constraints)\n",
    "for size in sizes[:2]:  # Only test on smaller datasets for joins\n",
    "    test_joins(size)\n",
    "\n",
    "print(\"\\nJoin tests completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_window_functions(size: int):\n",
    "    \"\"\"\n",
    "    Test window functions: ranking, moving averages.\n",
    "    \"\"\"\n",
    "    print(f\"\\nTesting window functions for {size:,} rows...\")\n",
    "    \n",
    "    # Get data\n",
    "    df_pandas, df_polars, df_duckdb = loaded_data[size]\n",
    "    \n",
    "    # Test 1: Ranking within groups\n",
    "    print(\"  Testing ranking...\")\n",
    "    \n",
    "    # pandas ranking\n",
    "    def pandas_ranking():\n",
    "        return df_pandas.assign(\n",
    "            amount_rank=df_pandas.groupby('merchant_category')['amount'].rank(method='dense', ascending=False)\n",
    "        )\n",
    "    \n",
    "    tracker.measure_operation(f'ranking_{size}', 'pandas', pandas_ranking)\n",
    "    \n",
    "    # Polars ranking\n",
    "    def polars_ranking():\n",
    "        return df_polars.with_columns(\n",
    "            pl.col('amount').rank(method='dense', descending=True)\n",
    "            .over('merchant_category')\n",
    "            .alias('amount_rank')\n",
    "        )\n",
    "    \n",
    "    tracker.measure_operation(f'ranking_{size}', 'polars', polars_ranking)\n",
    "    \n",
    "    # DuckDB ranking\n",
    "    def duckdb_ranking():\n",
    "        conn = duckdb.connect()\n",
    "        return conn.execute(\"\"\"\n",
    "            SELECT *,\n",
    "                DENSE_RANK() OVER (PARTITION BY merchant_category ORDER BY amount DESC) as amount_rank\n",
    "            FROM df_duckdb\n",
    "        \"\"\").fetchdf()\n",
    "    \n",
    "    tracker.measure_operation(f'ranking_{size}', 'duckdb', duckdb_ranking)\n",
    "    \n",
    "    # Test 2: Moving averages\n",
    "    print(\"  Testing moving averages...\")\n",
    "    \n",
    "    # pandas moving average\n",
    "    def pandas_moving_avg():\n",
    "        sorted_df = df_pandas.sort_values(['customer_id', 'transaction_date'])\n",
    "        return sorted_df.assign(\n",
    "            amount_ma_7=sorted_df.groupby('customer_id')['amount'].rolling(window=7, min_periods=1).mean().reset_index(drop=True)\n",
    "        )\n",
    "    \n",
    "    tracker.measure_operation(f'moving_avg_{size}', 'pandas', pandas_moving_avg)\n",
    "    \n",
    "    # Polars moving average\n",
    "    def polars_moving_avg():\n",
    "        return (df_polars\n",
    "                .sort(['customer_id', 'transaction_date'])\n",
    "                .with_columns(\n",
    "                    pl.col('amount').rolling_mean(window_size=7, min_periods=1)\n",
    "                    .over('customer_id')\n",
    "                    .alias('amount_ma_7')\n",
    "                ))\n",
    "    \n",
    "    tracker.measure_operation(f'moving_avg_{size}', 'polars', polars_moving_avg)\n",
    "    \n",
    "    # DuckDB moving average\n",
    "    def duckdb_moving_avg():\n",
    "        conn = duckdb.connect()\n",
    "        return conn.execute(\"\"\"\n",
    "            SELECT *,\n",
    "                AVG(amount) OVER (\n",
    "                    PARTITION BY customer_id \n",
    "                    ORDER BY transaction_date \n",
    "                    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
    "                ) as amount_ma_7\n",
    "            FROM df_duckdb\n",
    "            ORDER BY customer_id, transaction_date\n",
    "        \"\"\").fetchdf()\n",
    "    \n",
    "    tracker.measure_operation(f'moving_avg_{size}', 'duckdb', duckdb_moving_avg)\n",
    "\n",
    "# Test window functions for smaller datasets\n",
    "for size in sizes[:2]:  # Only test on smaller datasets\n",
    "    test_window_functions(size)\n",
    "\n",
    "print(\"\\nWindow function tests completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: ML Pipeline Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ml_integration(size: int):\n",
    "    \"\"\"\n",
    "    Test ML pipeline integration: feature engineering, preprocessing.\n",
    "    \"\"\"\n",
    "    print(f\"\\nTesting ML integration for {size:,} rows...\")\n",
    "    \n",
    "    # Get data\n",
    "    df_pandas, df_polars, df_duckdb = loaded_data[size]\n",
    "    \n",
    "    # Feature engineering pipeline\n",
    "    print(\"  Testing feature engineering...\")\n",
    "    \n",
    "    # pandas feature engineering\n",
    "    def pandas_feature_engineering():\n",
    "        df = df_pandas.copy()\n",
    "        \n",
    "        # Create features\n",
    "        df['hour'] = df['transaction_date'].dt.hour\n",
    "        df['day_of_week'] = df['transaction_date'].dt.dayofweek\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6])\n",
    "        df['amount_log'] = np.log1p(df['amount'])\n",
    "        df['amount_normalized'] = (df['amount'] - df['amount'].mean()) / df['amount'].std()\n",
    "        \n",
    "        # Categorical encoding\n",
    "        df = pd.get_dummies(df, columns=['merchant_category', 'payment_method'], prefix=['cat', 'pay'])\n",
    "        \n",
    "        # Customer aggregations\n",
    "        customer_stats = df.groupby('customer_id')['amount'].agg(['mean', 'std', 'count']).add_prefix('customer_')\n",
    "        df = df.merge(customer_stats, left_on='customer_id', right_index=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    result_pandas = tracker.measure_operation(f'ml_features_{size}', 'pandas', pandas_feature_engineering)\n",
    "    \n",
    "    # Polars feature engineering\n",
    "    def polars_feature_engineering():\n",
    "        # Create base features\n",
    "        df = df_polars.with_columns([\n",
    "            pl.col('transaction_date').dt.hour().alias('hour'),\n",
    "            pl.col('transaction_date').dt.weekday().alias('day_of_week'),\n",
    "            pl.col('amount').log1p().alias('amount_log'),\n",
    "            ((pl.col('amount') - pl.col('amount').mean()) / pl.col('amount').std()).alias('amount_normalized')\n",
    "        ]).with_columns(\n",
    "            pl.col('day_of_week').is_in([6, 7]).alias('is_weekend')\n",
    "        )\n",
    "        \n",
    "        # One-hot encoding\n",
    "        df = df.to_dummies(['merchant_category', 'payment_method'])\n",
    "        \n",
    "        # Customer aggregations\n",
    "        customer_stats = df.group_by('customer_id').agg([\n",
    "            pl.col('amount').mean().alias('customer_mean'),\n",
    "            pl.col('amount').std().alias('customer_std'),\n",
    "            pl.col('amount').count().alias('customer_count')\n",
    "        ])\n",
    "        \n",
    "        df = df.join(customer_stats, on='customer_id')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    result_polars = tracker.measure_operation(f'ml_features_{size}', 'polars', polars_feature_engineering)\n",
    "    \n",
    "    # DuckDB feature engineering\n",
    "    def duckdb_feature_engineering():\n",
    "        conn = duckdb.connect()\n",
    "        \n",
    "        # Create customer stats first\n",
    "        conn.execute(\"\"\"\n",
    "            CREATE TEMP TABLE customer_stats AS\n",
    "            SELECT \n",
    "                customer_id,\n",
    "                AVG(amount) as customer_mean,\n",
    "                STDDEV(amount) as customer_std,\n",
    "                COUNT(amount) as customer_count\n",
    "            FROM df_duckdb\n",
    "            GROUP BY customer_id\n",
    "        \"\"\")\n",
    "        \n",
    "        # Main feature engineering query\n",
    "        return conn.execute(\"\"\"\n",
    "            SELECT \n",
    "                t.*,\n",
    "                EXTRACT(hour FROM transaction_date) as hour,\n",
    "                EXTRACT(dow FROM transaction_date) as day_of_week,\n",
    "                CASE WHEN EXTRACT(dow FROM transaction_date) IN (6, 0) THEN true ELSE false END as is_weekend,\n",
    "                LN(1 + amount) as amount_log,\n",
    "                (amount - (SELECT AVG(amount) FROM df_duckdb)) / (SELECT STDDEV(amount) FROM df_duckdb) as amount_normalized,\n",
    "                CASE WHEN merchant_category = 'grocery' THEN 1 ELSE 0 END as cat_grocery,\n",
    "                CASE WHEN merchant_category = 'gas' THEN 1 ELSE 0 END as cat_gas,\n",
    "                CASE WHEN merchant_category = 'restaurant' THEN 1 ELSE 0 END as cat_restaurant,\n",
    "                CASE WHEN merchant_category = 'retail' THEN 1 ELSE 0 END as cat_retail,\n",
    "                CASE WHEN merchant_category = 'online' THEN 1 ELSE 0 END as cat_online,\n",
    "                CASE WHEN payment_method = 'credit' THEN 1 ELSE 0 END as pay_credit,\n",
    "                CASE WHEN payment_method = 'debit' THEN 1 ELSE 0 END as pay_debit,\n",
    "                CASE WHEN payment_method = 'cash' THEN 1 ELSE 0 END as pay_cash,\n",
    "                cs.customer_mean,\n",
    "                cs.customer_std,\n",
    "                cs.customer_count\n",
    "            FROM df_duckdb t\n",
    "            LEFT JOIN customer_stats cs ON t.customer_id = cs.customer_id\n",
    "        \"\"\").fetchdf()\n",
    "    \n",
    "    result_duckdb = tracker.measure_operation(f'ml_features_{size}', 'duckdb', duckdb_feature_engineering)\n",
    "    \n",
    "    return result_pandas, result_polars, result_duckdb\n",
    "\n",
    "# Test ML integration for smaller datasets\n",
    "ml_results = {}\n",
    "for size in sizes[:2]:  # Only test on smaller datasets\n",
    "    ml_results[size] = test_ml_integration(size)\n",
    "\n",
    "print(\"\\nML integration tests completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results as DataFrame\n",
    "results_df = tracker.get_results_df()\n",
    "\n",
    "# Filter only successful operations\n",
    "successful_results = results_df[results_df['success'] == True].copy()\n",
    "\n",
    "print(f\"Total operations tested: {len(results_df)}\")\n",
    "print(f\"Successful operations: {len(successful_results)}\")\n",
    "print(f\"Failed operations: {len(results_df) - len(successful_results)}\")\n",
    "\n",
    "# Display first few results\n",
    "print(\"\\nSample results:\")\n",
    "print(successful_results[['operation', 'engine', 'execution_time', 'memory_delta_mb']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "fig.suptitle('Data Engine Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Execution Time Comparison\n",
    "ax1 = axes[0, 0]\n",
    "execution_pivot = successful_results.pivot_table(\n",
    "    index='operation', \n",
    "    columns='engine', \n",
    "    values='execution_time', \n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "execution_pivot.plot(kind='bar', ax=ax1, width=0.8)\n",
    "ax1.set_title('Average Execution Time by Operation', fontweight='bold')\n",
    "ax1.set_ylabel('Time (seconds)')\n",
    "ax1.set_xlabel('Operation')\n",
    "ax1.legend(title='Engine')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Memory Usage Comparison\n",
    "ax2 = axes[0, 1]\n",
    "memory_pivot = successful_results.pivot_table(\n",
    "    index='operation', \n",
    "    columns='engine', \n",
    "    values='memory_delta_mb', \n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "memory_pivot.plot(kind='bar', ax=ax2, width=0.8)\n",
    "ax2.set_title('Average Memory Usage by Operation', fontweight='bold')\n",
    "ax2.set_ylabel('Memory Delta (MB)')\n",
    "ax2.set_xlabel('Operation')\n",
    "ax2.legend(title='Engine')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Performance by Dataset Size\n",
    "ax3 = axes[1, 0]\n",
    "# Extract dataset size from operation name\n",
    "successful_results['dataset_size'] = successful_results['operation'].str.extract(r'_(\\d+)$')[0]\n",
    "size_perf = successful_results.groupby(['dataset_size', 'engine'])['execution_time'].mean().unstack()\n",
    "\n",
    "size_perf.plot(kind='line', ax=ax3, marker='o', linewidth=2, markersize=6)\n",
    "ax3.set_title('Execution Time vs Dataset Size', fontweight='bold')\n",
    "ax3.set_ylabel('Average Time (seconds)')\n",
    "ax3.set_xlabel('Dataset Size (rows)')\n",
    "ax3.legend(title='Engine')\n",
    "ax3.set_yscale('log')\n",
    "\n",
    "# 4. Engine Performance Summary (Radar Chart Data)\n",
    "ax4 = axes[1, 1]\n",
    "engine_summary = successful_results.groupby('engine').agg({\n",
    "    'execution_time': 'mean',\n",
    "    'memory_delta_mb': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "# Normalize scores (lower is better, so invert)\n",
    "engine_summary['speed_score'] = 1 / engine_summary['execution_time']\n",
    "engine_summary['memory_score'] = 1 / (engine_summary['memory_delta_mb'] + 0.1)  # Add small constant to avoid division by zero\n",
    "\n",
    "# Plot normalized scores\n",
    "engines = engine_summary.index\n",
    "x_pos = range(len(engines))\n",
    "\n",
    "width = 0.35\n",
    "ax4.bar([x - width/2 for x in x_pos], engine_summary['speed_score'], width, label='Speed Score', alpha=0.8)\n",
    "ax4.bar([x + width/2 for x in x_pos], engine_summary['memory_score'], width, label='Memory Score', alpha=0.8)\n",
    "\n",
    "ax4.set_title('Engine Performance Scores\\n(Higher is Better)', fontweight='bold')\n",
    "ax4.set_ylabel('Normalized Score')\n",
    "ax4.set_xlabel('Engine')\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(engines)\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../artifacts/reports/engine_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEngine Performance Summary:\")\n",
    "print(engine_summary[['execution_time', 'memory_delta_mb']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed performance breakdown by operation type\n",
    "operation_types = {\n",
    "    'load_csv': 'Data Loading',\n",
    "    'filter': 'Filtering',\n",
    "    'sort': 'Sorting',\n",
    "    'select_transform': 'Selection/Transform',\n",
    "    'groupby_simple': 'Simple Aggregation',\n",
    "    'groupby_complex': 'Complex Aggregation',\n",
    "    'inner_join': 'Inner Join',\n",
    "    'left_join_agg': 'Join + Aggregation',\n",
    "    'ranking': 'Window Functions',\n",
    "    'moving_avg': 'Moving Average',\n",
    "    'ml_features': 'ML Feature Engineering'\n",
    "}\n",
    "\n",
    "# Extract operation type\n",
    "successful_results['operation_type'] = successful_results['operation'].str.extract(r'^([^_]+(?:_[^_\\d]+)?)')\n",
    "successful_results['operation_type'] = successful_results['operation_type'].map(operation_types).fillna('Other')\n",
    "\n",
    "# Create detailed comparison table\n",
    "comparison_table = successful_results.groupby(['operation_type', 'engine']).agg({\n",
    "    'execution_time': ['mean', 'std'],\n",
    "    'memory_delta_mb': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "print(\"Detailed Performance Comparison by Operation Type:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engine Recommendations and Decision Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(results_df):\n",
    "    \"\"\"\n",
    "    Generate recommendations based on performance results.\n",
    "    \"\"\"\n",
    "    recommendations = {\n",
    "        'Small Datasets (< 100K rows)': {},\n",
    "        'Medium Datasets (100K - 1M rows)': {},\n",
    "        'Large Datasets (> 1M rows)': {},\n",
    "        'Use Case Specific': {}\n",
    "    }\n",
    "    \n",
    "    # Analyze performance by dataset size\n",
    "    for operation_type in operation_types.values():\n",
    "        op_data = results_df[results_df['operation_type'] == operation_type]\n",
    "        \n",
    "        if len(op_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Find best performer for each size category\n",
    "        for size_category in ['10000', '100000', '1000000']:\n",
    "            size_data = op_data[op_data['dataset_size'] == size_category]\n",
    "            \n",
    "            if len(size_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Best time performer\n",
    "            best_time = size_data.loc[size_data['execution_time'].idxmin()]\n",
    "            \n",
    "            # Best memory performer\n",
    "            best_memory = size_data.loc[size_data['memory_delta_mb'].idxmin()]\n",
    "            \n",
    "            if size_category == '10000':\n",
    "                category = 'Small Datasets (< 100K rows)'\n",
    "            elif size_category == '100000':\n",
    "                category = 'Medium Datasets (100K - 1M rows)'\n",
    "            else:\n",
    "                category = 'Large Datasets (> 1M rows)'\n",
    "            \n",
    "            if operation_type not in recommendations[category]:\n",
    "                recommendations[category][operation_type] = {}\n",
    "            \n",
    "            recommendations[category][operation_type] = {\n",
    "                'best_speed': best_time['engine'],\n",
    "                'best_memory': best_memory['engine'],\n",
    "                'speed_time': f\"{best_time['execution_time']:.4f}s\",\n",
    "                'memory_usage': f\"{best_memory['memory_delta_mb']:.2f}MB\"\n",
    "            }\n",
    "    \n",
    "    # Use case specific recommendations\n",
    "    recommendations['Use Case Specific'] = {\n",
    "        'SQL-Heavy Analytics': {\n",
    "            'recommended': 'duckdb',\n",
    "            'reason': 'Native SQL support, optimized for analytical queries'\n",
    "        },\n",
    "        'Memory-Constrained Environments': {\n",
    "            'recommended': 'polars',\n",
    "            'reason': 'Efficient memory usage and lazy evaluation'\n",
    "        },\n",
    "        'Existing Pandas Workflows': {\n",
    "            'recommended': 'pandas',\n",
    "            'reason': 'Drop-in replacement, extensive ecosystem'\n",
    "        },\n",
    "        'Large-Scale Data Processing': {\n",
    "            'recommended': 'polars',\n",
    "            'reason': 'Parallel processing and query optimization'\n",
    "        },\n",
    "        'Complex Window Functions': {\n",
    "            'recommended': 'duckdb',\n",
    "            'reason': 'Advanced SQL window function support'\n",
    "        },\n",
    "        'ML Feature Engineering': {\n",
    "            'recommended': 'polars',\n",
    "            'reason': 'Fast transformations and memory efficiency'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = generate_recommendations(successful_results)\n",
    "\n",
    "print(\"ENGINE SELECTION DECISION MATRIX\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "\n",
    "for category, ops in recommendations.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    print(\"-\" * len(category))\n",
    "    \n",
    "    if category == 'Use Case Specific':\n",
    "        for use_case, rec in ops.items():\n",
    "            print(f\"\\n  {use_case}:\")\n",
    "            print(f\"    Recommended: {rec['recommended'].upper()}\")\n",
    "            print(f\"    Reason: {rec['reason']}\")\n",
    "    else:\n",
    "        for operation, rec in ops.items():\n",
    "            print(f\"\\n  {operation}:\")\n",
    "            print(f\"    Best Speed: {rec['best_speed'].upper()} ({rec['speed_time']})\")\n",
    "            print(f\"    Best Memory: {rec['best_memory'].upper()} ({rec['memory_usage']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics and Winner Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate winner statistics\n",
    "def analyze_winners(results_df):\n",
    "    \"\"\"\n",
    "    Analyze which engine wins most often across different metrics.\n",
    "    \"\"\"\n",
    "    winners = {'speed': {}, 'memory': {}}\n",
    "    \n",
    "    # Group by operation and find winners\n",
    "    for operation in results_df['operation'].unique():\n",
    "        op_data = results_df[results_df['operation'] == operation]\n",
    "        \n",
    "        if len(op_data) >= 2:  # Need at least 2 engines to compare\n",
    "            # Speed winner (lowest time)\n",
    "            speed_winner = op_data.loc[op_data['execution_time'].idxmin(), 'engine']\n",
    "            if speed_winner not in winners['speed']:\n",
    "                winners['speed'][speed_winner] = 0\n",
    "            winners['speed'][speed_winner] += 1\n",
    "            \n",
    "            # Memory winner (lowest memory delta)\n",
    "            memory_winner = op_data.loc[op_data['memory_delta_mb'].idxmin(), 'engine']\n",
    "            if memory_winner not in winners['memory']:\n",
    "                winners['memory'][memory_winner] = 0\n",
    "            winners['memory'][memory_winner] += 1\n",
    "    \n",
    "    return winners\n",
    "\n",
    "winners = analyze_winners(successful_results)\n",
    "\n",
    "print(\"WINNER ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "print()\n",
    "\n",
    "print(\"Speed Champions (Number of operations won):\")\n",
    "for engine, wins in sorted(winners['speed'].items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {engine.upper()}: {wins} operations\")\n",
    "\n",
    "print(\"\\nMemory Champions (Number of operations won):\")\n",
    "for engine, wins in sorted(winners['memory'].items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {engine.upper()}: {wins} operations\")\n",
    "\n",
    "# Overall performance scores\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"OVERALL PERFORMANCE SCORES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "engine_scores = successful_results.groupby('engine').agg({\n",
    "    'execution_time': ['mean', 'median', 'std'],\n",
    "    'memory_delta_mb': ['mean', 'median', 'std']\n",
    "}).round(4)\n",
    "\n",
    "print(\"\\nExecution Time Statistics:\")\n",
    "print(engine_scores['execution_time'])\n",
    "\n",
    "print(\"\\nMemory Usage Statistics:\")\n",
    "print(engine_scores['memory_delta_mb'])\n",
    "\n",
    "# Calculate relative performance\n",
    "baseline_engine = 'pandas'\n",
    "if baseline_engine in engine_scores.index:\n",
    "    print(f\"\\nRelative Performance (vs {baseline_engine.upper()}):\")\n",
    "    baseline_time = engine_scores.loc[baseline_engine, ('execution_time', 'mean')]\n",
    "    baseline_memory = engine_scores.loc[baseline_engine, ('memory_delta_mb', 'mean')]\n",
    "    \n",
    "    for engine in engine_scores.index:\n",
    "        time_ratio = engine_scores.loc[engine, ('execution_time', 'mean')] / baseline_time\n",
    "        memory_ratio = engine_scores.loc[engine, ('memory_delta_mb', 'mean')] / baseline_memory if baseline_memory > 0 else 1\n",
    "        \n",
    "        print(f\"  {engine.upper()}:\")\n",
    "        print(f\"    Speed: {time_ratio:.2f}x ({'faster' if time_ratio < 1 else 'slower'})\")\n",
    "        print(f\"    Memory: {memory_ratio:.2f}x ({'less' if memory_ratio < 1 else 'more'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results and Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "output_dir = Path('../artifacts/reports')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save raw results\n",
    "results_df.to_csv(output_dir / 'engine_comparison_results.csv', index=False)\n",
    "\n",
    "# Save performance summary\n",
    "summary_report = {\n",
    "    'test_summary': {\n",
    "        'total_operations': len(results_df),\n",
    "        'successful_operations': len(successful_results),\n",
    "        'engines_tested': list(results_df['engine'].unique()),\n",
    "        'dataset_sizes': sizes,\n",
    "        'operation_types': list(operation_types.values())\n",
    "    },\n",
    "    'winners': winners,\n",
    "    'recommendations': recommendations,\n",
    "    'performance_summary': engine_scores.to_dict()\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(output_dir / 'engine_comparison_summary.json', 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved to {output_dir}\")\n",
    "print(\"\\nFiles generated:\")\n",
    "print(\"  - engine_comparison_results.csv: Detailed performance data\")\n",
    "print(\"  - engine_comparison_summary.json: Summary and recommendations\")\n",
    "print(\"  - engine_comparison.png: Performance visualization\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENGINE COMPARISON COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. Each engine has its strengths for different use cases\")\n",
    "print(\"2. Consider dataset size, operation type, and constraints\")\n",
    "print(\"3. Use the decision matrix above for engine selection\")\n",
    "print(\"4. Test with your specific data and workload for best results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}